
% Note for any github stalkers. I am currently in the process
% of learning LaTeX. I don't know what I'm doing yet. Sorry
% if my code absolutely sucks.
\documentclass{book}

\usepackage{fontspec} % used to import Calibri
\usepackage{anyfontsize} % used to adjust font size

% needed for inch and other length measurements
% to be recognized
\usepackage{calc}

% for colors and text effects as is hopefully obvious
\usepackage[dvipsnames]{xcolor}
\usepackage{soul}

% control over margins
\usepackage[margin=1in]{geometry}
\usepackage[strict]{changepage}

\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb} % originally imported to get the proof square
\usepackage[overcommands]{overarrows} % Get my preferred vector arrows...
\usepackage{relsize}

% Just am using this to get a dashed line in a table...
% Also you apparently want this to be inactive if you aren't
% using it because it slows compilation.
\usepackage{arydshln} \ADLinactivate 
\newenvironment{allowTableDashes}{\ADLactivate}{\ADLinactivate}

\usepackage{graphicx}
%\graphicspath{{./140A_images/}}

\usepackage{tikz}
   \usetikzlibrary{arrows.meta}

\newfontfamily{\calibri}{Calibri}
\setlength{\parindent}{0pt}
\definecolor{RawerSienna}{HTML}{945D27}
\setul{0.14em}{0.07em}

\newcommand{\hOne}{%
   \color{Black}%
   \fontsize{14}{16}\selectfont%
}
\newcommand{\hTwo}{%
   \color{MidnightBlue}%
   \fontsize{13}{15}\selectfont%
}
\newcommand{\hThree}{%
   \color{PineGreen}
   \fontsize{13}{15}\selectfont%
}
\newcommand{\hFour}{%
   \color{Cerulean}
   \fontsize{12}{14}\selectfont%
}
\newcommand{\myComment}{%
   \color{RawerSienna}%
   \fontsize{12}{14}\selectfont%
}
\newcommand{\teachComment}{
   \color{Orange}%
   \fontsize{12}{14}\selectfont%
}
\newcommand{\exOne}{%
   \color{Purple}%
   \fontsize{14}{16}\selectfont%
}
\newcommand{\exTwo}{%
   \color{RedViolet}%
   \fontsize{13}{15}\selectfont%
}
\newcommand{\exP}{%
   \color{VioletRed}%
   \fontsize{12}{14}\selectfont%
}

\newenvironment{myIndent}{%
   \begin{adjustwidth}{2.5em}{0em}%
}{%
   \end{adjustwidth}%
}
\newenvironment{myDindent}{%
   \begin{adjustwidth}{5.0em}{0em}%
}{%
   \end{adjustwidth}%
}
\newenvironment{myTindent}{%
   \begin{adjustwidth}{7.5em}{0em}%
}{%
   \end{adjustwidth}%
}

\newenvironment{myConstrict}{%
   \begin{adjustwidth}{2.5em}{2.5em}%
}{%
   \end{adjustwidth}%
}

\newcommand{\udefine}[1]{%
   {\setulcolor{Red}%
   \setul{0.14em}{0.07em}%
   \ul{#1}}%
}

\newcommand{\uuline}[2][.]{%
{\vphantom{a}\color{#1}%
\rlap{\rule[-0.18em]{\widthof{#2}}{0.06em}}%
\rlap{\rule[-0.32em]{\widthof{#2}}{0.06em}}}%
#2}

\newcounter{LectureNumber}
\newcommand*{\markLecture}[1]{%
   \stepcounter{LectureNumber}%
   {\huge \color{Black} \textbf{Lecture \theLectureNumber: #1} \newline}%
}

\newcommand{\pprime}{{\prime\prime}}

\newcounter{PropNumber}
\newcommand{\propCount}{%
   \stepcounter{PropNumber}%
   \thePropNumber%
}

\newcommand{\mySepOne}[1][.]{%
   {\noindent\color{#1}{\rule{6.5in}{1mm}}}\\%
}
\newcommand{\mySepTwo}[1][.]{%
   {\noindent\color{#1}{\rule{6.5in}{0.5mm}}}\\%
}

\newenvironment{myClosureOne}[2][.]{%
   \color{#1}%
   \begin{tabular}{|p{#2in}|} \hline \\%
}{%
   \\ \\ \hline \end{tabular}%
}

\newcommand{\retTwo}{\hfill\bigbreak}


% Hopefully this will be good enough
\NewOverArrowCommand{myVector}{%
   start = {{\smallermathstyle\relbar}},
   middle = {{\smallermathstyle\relbar}},
   end={{\rightharpoonup}}, space before arrow=0.15em,
   space after arrow=-0.045em,
}
\newcommand{\mVec}[1]{\myVector{#1}}
\newcommand{\mVecAst}[1]{\myVector*{#1}}
\newcommand{\mMat}[1]{\mathbf{#1}}


% Programming stuff:
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\usepackage{listings} % for writing code...
\lstset{
   basicstyle=\ttfamily
}

\colorlet{currentIdentifierFromLst}{Black}

\lstdefinestyle{170test}{
   basicstyle=\normalsize\ttfamily\color{Black},
   language=MATLAB,
   backgroundcolor = \color{Dandelion!5},
   commentstyle=\color{OliveGreen},
   keywordstyle=\color{Cerulean},
   numbersep=5pt,
   numbers=left,
   numberstyle=\color{Gray},
   tabsize=3,
   identifierstyle=\color{RedViolet},
   stringstyle=\color{BrickRed},
   showstringspaces=false,
   %morestring=[b]"
}
\newcommand{\lstSetTest}{%
   \lstset{style=170test}%
   \colorlet{currentIdentifierFromLst}{RedViolet}%
}

\newcommand{\identifier}[1]{%
   {\color{currentIdentifierFromLst}\texttt{#1}}%
}
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



\title{Math 170A Lecture Notes (Professor: Lei Huang)}
\author{Isabelle Mills}

\begin{document}
   \lstSetTest
   \maketitle
   \calibri

   {\huge \color{Black} \textbf{Week 1 Notes (1/8 - 1/12/2024)}%
   \stepcounter{LectureNumber}%
   \stepcounter{LectureNumber}%
   \stepcounter{LectureNumber} \retTwo}
   
   \hOne
   For this class, we shall define the number of \udefine{flops} an
   algorithm takes as the number of individual $+$, $-$, $\times$, $/$,
   and $\sqrt{\phantom{x}}$ operations on \uuline{real numbers}
   used in the algorithm. \retTwo
   
   \begin{myIndent} \hTwo
      For example: taking the inner product of two vectors
      $\mVec{u} = (u_1, u_2, \ldots, u_n)$ and
      $\mVec{v} = (v_1, v_2, \ldots, v_n)$ requires
      $n$ multiplications and $(n-1)$ additions. So we'll say
      it has a flops count of $2n-1$.
   \end{myIndent}
   \hOne
   \retTwo

   Technically, the word "flop" stands for 
   \textul{floating (point) operation}. Based on that knowledge,
   hopefully it is easier to guess what is and is not a flop. For
   instance, observe the code written below for taking an inner
   product of two $n$-vectors.

   \begin{tabular}{ p{3.0in} p{3.0in}}
      \retTwo
      \begin{lstlisting}
   P = 0
   for i = 1:n
      P = P + v(i) * w(i)
   end
      \end{lstlisting}
      &
      \begin{myIndent} 
         {\hTwo Neither incrementing \identifier{i} nor \newline
         initializing any other variables are counted towards the 
         flop number. Because the code does \identifier{n} additions
         and multiplications between \newline floating point
         numbers, we say this function has $2n$ flops.}
      \end{myIndent}
   \end{tabular}


   \mySepOne

   Here is how we formally define \udefine{Big-$\mathcal{O}$ Notation}:
   \retTwo
   For a sequence $a_n$, we define $a_n = \mathcal{O}(b_n)$ if there exists
   real constants\\ $C, N \geq 0$ such that for $n \geq N$, \hspace{0.25em}
   $a_n \leq Cb_n$.
   \retTwo

   \exOne
   \uuline{Example problem}:

   \begin{tabular}{ p{3.7in} p{2.4in} }
      \begin{lstlisting}
      function x = lowertriangsolve(L, b)
         N = size(L);
         n=size(b,1);
         x=b;
   
         for i=1:N(1):
            for j=1:i-1
               x(i) = x(i) - L(i,j)*x(j);
            end
   
            if L(i, j) == 0
               error('matrix is singular')
            end
            x(i) = x(i)/L(i,i);
         end
      end
      \end{lstlisting}
      &
      \exP
      Inside the main for loop (lines 6-15):
      \begin{myIndent}
         Line 8 has $2(\texttt{i}-1)$ flops.\newline
         Line 14 has an additional flop.\newline
      \end{myIndent}

      Thus, the total number of flops is:
      \[\sum_{\texttt{i}=1}^{n}(2\texttt{i}-1)
      = 2\left(\sum_{\texttt{i}=1}^{n}\texttt{i}\right) - n\]

      This then simplifies to:
      \[ 2\left( \dfrac{n(n+1)}{2} \right) - n
      = 1n^2 \]

      So \identifier{x} has $\mathcal{O}(n^2)$ flops as \identifier{n} goes
      to \newline infinity.
   \end{tabular}

   \newpage \hOne
   
   \markLecture{1/17/2024}

   Row operations used in Gaussian elemination can be represented via matrix \\multiplication as demonstrated below.
   
   {\hTwo
   \begin{allowTableDashes}
      \begin{tabular}{ p{4.4in};{10pt/3pt}p{1.6in} }
         {\center \phantom{ Matrix representation}\newline Action \par} & {\center Matrix representation \newline of the operation \newline \par} \\
         \hline {\hFour
         \[
         \begin{bmatrix}
            a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\
            a_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} \\
            a_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} \\
            a_{4,1} & a_{4,2} & a_{4,3} & a_{4,4}
         \end{bmatrix} \xlongrightarrow{R_2 \leftrightarrow R_3}
         \begin{bmatrix}
            a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\
            a_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} \\
            a_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} \\
            a_{4,1} & a_{4,2} & a_{4,3} & a_{4,4}
         \end{bmatrix} \]} & {\hFour
         \[
         \begin{bmatrix}
            1 & 0 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 1
         \end{bmatrix}\]}
         \\
         \hline {\hFour
         \[
         \begin{bmatrix}
            a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\
            a_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} \\
            a_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} \\
            a_{4,1} & a_{4,2} & a_{4,3} & a_{4,4}
         \end{bmatrix} \xlongrightarrow{\frac{2}{3}R_2 \rightarrow R_2}
         \begin{bmatrix}
            a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\
            \frac{2}{3}a_{2,1} & \frac{2}{3}a_{2,2} & \frac{2}{3}a_{2,3} & \frac{2}{3}a_{2,4} \\
            a_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} \\
            a_{4,1} & a_{4,2} & a_{4,3} & a_{4,4}
         \end{bmatrix} \]} & {\hFour
         \[
         \begin{bmatrix}
            1 & 0 & 0 & 0 \\
            0 & \frac{2}{3} & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 1
         \end{bmatrix}\]}
         \\
         \hline {\hFour
         \[
         \begin{matrix}
            \begin{bmatrix}
               a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\
               a_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} \\
               a_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} \\
               a_{4,1} & a_{4,2} & a_{4,3} & a_{4,4}
            \end{bmatrix} \\
            \phantom{
               \text{\footnotesize $R_3 + \frac{2}{3}R_2 \rightarrow R_3$ }}
            {\left\downarrow \vphantom{{\int_A^\frac{\frac{\frac{A}{1}}{1}}{1}}} \right.}  \text{\footnotesize $R_3 + \frac{2}{3}R_2 \rightarrow R_3$ } \\
            \begin{bmatrix}
               a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\
               a_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} \\
               a_{3,1} {+ \frac{2}{3}a_{2,1}}  & a_{3,2} {+ \frac{2}{3}a_{2,2}} & a_{3,3} {+ \frac{2}{3}a_{2,3}} & a_{3,4} {+ \frac{2}{3}a_{2,4}} \\
               a_{4,1} & a_{4,2} & a_{4,3} & a_{4,4}
            \end{bmatrix}
         \end{matrix}
         \]} & { \hFour \retTwo \retTwo 
         \[
         \begin{bmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & \frac{2}{3} & 1 & 0 \\
            0 & 0 & 0 & 1
         \end{bmatrix}\]} 
      \end{tabular}
   \end{allowTableDashes}
   } \retTwo \retTwo

   The importance of representing elementary row operations as matrices is that we can multiply these representations together to compose rows operations. Thus, these representations are central to many matrix decompositions.

   
   \begin{myIndent}
      {\exTwo
         For example, we can represent turning a matrix into row echelon form as follows:
         \[
         \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & -\frac{2}{9} & 1
         \end{bmatrix}\begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            -2 & 0 & 1
         \end{bmatrix}\begin{bmatrix}
            1 & 0 & 0 \\
            -3 & 1 & 0 \\
            0 & 0 & 1
         \end{bmatrix}\begin{bmatrix}
            2 & 4 & 5 \\
            6 & 3 & 4 \\
            4 & 6 & 2
         \end{bmatrix} = \begin{bmatrix}
            2 & 4 & 5 \\
            0 & -9 & -11 \\
            0 & 0 & \frac{-50}{9}
         \end{bmatrix}\]
      }
   \end{myIndent}
   \newpage

   Here are some more observations about row operation matrices:
   
   \begin{itemize}
      \item Row scaling operations are represented by diagonal matrices and thus are also both lower and upper triangular.
      
      \item For $i<j$, adding a multiple of the $i$th row to the $j$th row is represented by a lower triangular matrix.
      
      \item For $i>j$, adding a multiple of the $i$th row to the $j$th row is represented by an upper triangular matrix.
      
      \item Row swaps are not represented as triangular matrices. However, they are\\ \uuline{permutation} matrices.
   \end{itemize} \retTwo

   Now note that in the normal algorithm for Gaussian elimination, assuming we never need to swap rows, all row operations will be such that their matrix representation is lower triangular. 
   
   \begin{myIndent} \hTwo
      Thus, given an invertible square matrix $\mMat{A}$, we can represent doing \\Gaussian elimination on $\mMat{A}$ by the equation: $\mMat{L}_n\mMat{L}_{n-1}\dotsb\mMat{L}_{2}\mMat{L}_{1}\mMat{A} = \mMat{U}$ where $\mMat{L}_i$ is a lower triangular matrix and $\mMat{U}$ is an upper triangular matrix. Then, because the product of two lower triangular matrices is also lower triangular, we can multiply all the lower triangular matries together to get an equation of the form $\mMat{L}\mMat{A}=\mMat{U}$. Finally, we multiply both sides of the equation on the left by $\mMat{L}^{-1}$ to get that\\ $\mMat{A}=\mMat{L}^{-1}\mMat{U}$. And as the inverse of a lower triangular matrix is also lower triangular, we know that we have decomposed $\mMat{A}$ into the product of a lower triangular matrix and an upper triangular matrix. This algorithm is called \udefine{LU decomposition}. \retTwo
   \end{myIndent}

   As for why we would want to use LU decomposition, we can look at the number of flops different matrix operations require.
   \hTwo
   \begin{myIndent}
      Assume we are given an invertible $n\times n$ matrix $\mMat{A}$ and two vectors $\mVec{x}, \mVec{b} \in \mathbb{R}^n$. \retTwo
      Firstly note that row reduction takes approximately $\frac{2}{3}n^3$ flops while the back\\ substitution algorithm takes approximately $n^2$ flops. Thus, for larger values of $n$, solving the matrix equation $\mMat{A}\mVec{x} = \mVec{b}$ takes around $\frac{2}{3}n^3$ flops. \retTwo

      Now lets compare this to the number of flops the best algorithm we can make to do LU decomposition takes.
      
      {\begin{myIndent} \hThree
         Assuming we don't need to do any row swaps, than the only elementary row operations we need to do are adding scaled rows to other rows. This is where our first optimization comes into play. The inverse of a row addition is merely a row subtraction. For example:
            {\begin{myTindent} \hThree
               $\begin{bmatrix}
                  1 & 0 & 0 \\
                  a & 1 & 0 \\
                  0 & 0 & 1
               \end{bmatrix}^{-1} = \begin{bmatrix}
                  1 & 0 & 0 \\
                  -a & 1 & 0 \\
                  0 & 0 & 1
               \end{bmatrix}$ \retTwo
            \end{myTindent}}

         \newpage
         Thus, we can fairly straightforwardly get an expression of the form \\$ \mMat{A} = \mMat{L}_1 \mMat{L}_2 \cdots \mMat{L}_k \mMat{U}$ by multiplying the inverse of each elementary row \\operation matrix to both sides of the equation representing the actions we took to reduce $\mMat{A}$ to $\mMat{U}$. \retTwo

         Now, we apply another two shortcuts to speed up our calculations. \retTwo Firstly, observe that:
         $
         \left[\begin{smallmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & a & 1 & 0 \\
            0 & 0 & 0 & 1
         \end{smallmatrix}\right]
         \left[\begin{smallmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & b & 0 & 1
         \end{smallmatrix}\right] = \left[\begin{smallmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & b & 0 & 1
         \end{smallmatrix}\right]
         \left[\begin{smallmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & a & 1 & 0 \\
            0 & 0 & 0 & 1
         \end{smallmatrix}\right] = \left[\begin{smallmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & a & 1 & 0 \\
            0 & b & 0 & 1
         \end{smallmatrix}\right]$
         
         {\begin{myTindent}\begin{myIndent} \hFour
            There's nothing special about that column or that \\particular matrix size. In fact, this should make sense when you consider that those two elementary row \\operations don't effect each other.
         \end{myIndent}\end{myTindent}} \retTwo

         Next, observe that: $\left[\begin{smallmatrix}
            1 & 0 & 0 & 0 \\
            a & 1 & 0 & 0 \\
            b & 0 & 1 & 0 \\
            c & 0 & 0 & 1
         \end{smallmatrix}\right]
         \left[\begin{smallmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & d & 1 & 0 \\
            0 & e & 0 & 1
         \end{smallmatrix}\right]
         \left[\begin{smallmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & f & 1
         \end{smallmatrix}\right] = \left[\begin{smallmatrix}
            1 & 0 & 0 & 0 \\
            a & 1 & 0 & 0 \\
            b & d & 1 & 0 \\
            c & e & f & 1
         \end{smallmatrix}\right]$

         {\begin{myTindent}\begin{myIndent} \hFour
            To intuit why this works, think about how the matrix \\product has every row affect the rows underneath it \\before it itself is affected by any rows above it. So, it is specifically the initial state of every row that is effecting every other row in the matrix product.
         \end{myIndent}\end{myTindent}} \retTwo

         The end result of these observations is that the $(i,j)$th element of $\mMat{L}$ where $i>j$ is just going to be the negative of whatever coefficient one multiplied a copy of row $i$ by before then adding that to row $j$ as part of doing the row reduction algorithm. But now note that means that every element in $\mMat{L}$ can be directly extracted from the calculations done to find $\mMat{U}$. So finding $\mMat{U}$ takes approximately $\frac{2}{3}n^3$ flops and finding $\mMat{L}$ takes no additional flops. \retTwo
      \end{myIndent}}

      Once, we've found $\mMat{L}\mMat{U}$, we can then use back substitution twice to solve any matrix vector equation $\mMat{A}\mVec{x} = \mMat{L}\mMat{U}\mVec{x} = \mVec{b}$. This will take $2n^2$ flops. \retTwo

      Technically, this means that if you are trying to solve a single matrix vector equation $\mMat{A}\mVec{x} = \mVec{b}$ by first decomposing $\mMat{A}$, then it will take longer than if you just solved it directly. However, if you have many matrix vector equations involving $\mMat{A}$, then you can work way faster by decomposing $\mMat{A}$. This is because once you decompose $\mMat{A}$ once, you can just store $\mMat{L}$ and $\mMat{U}$ for later use. Thus, every subsequent matrix vector equation involving $\mMat{A}$ can be done in approximately $2n^2$ flops instead of taking approximately $\frac{2}{3}n^3$ flops. \retTwo
   \end{myIndent}
   \newpage

   \hOne
   \markLecture{1/19/2024}

   A crucial assumption we made in the last lecture was that we never would have to do row swaps when doing row reduction. In this lecture, we'll now allow ourselves to do row swaps. \retTwo

   Firstly, here are two general reasons to want to do row swaps.
   
   \begin{itemize}
      \item Firstly, sometimes we have no choice.
      {\begin{myTindent}\begin{myDindent} \exOne
      
         \begin{tabular}{p{0.9in} p{2.5in}}
            \raisebox{-1.5em}{$\begin{bmatrix}
               0 & 4 & 1 \\
               1 & 3 & 4 \\
               2 & 2 & 5
            \end{bmatrix}$} &
            For this matrix, if we were to \newline divide the second and third row by $a_{1,1}$, we'd be dividing them by $0$. Thus, we clearly can't do that.
         \end{tabular}
      \end{myDindent}\end{myTindent}}

      \item Secondly, dividing by small numbers causes more roundoff erros. So, we can slow the accumulation of roundoff erros by swapping rows in order to prioritize dividing by larger elements.
   \end{itemize}

   So here's an algorithm called \udefine{parial pivoting} for taking a \udefine{PLU decomposition} of an invertible $n\times n$ matrix $\mMat{A}$.
   
   \hTwo
   \begin{myIndent}
      When doing Guassian elimination, for every new pivot of $\mMat{A}$, first perform a row swap with the top non-reduced row and the non-reduced row with the largest would-be pivot element. Only, after that do you then add a scaled version of the pivot row to the other rows like you were doing before. \retTwo

      Now note that performing the same row swap twice in a row is equivalent to doing nothing. Thus, every matrix representing a row swap is its own inverse. \\ Additionally, as we already covered, the inverse of a row addition operation is just a row subtraction operation. Because of this, we can easily get an equation for $A$ as the product of many elementary triangular matrices and permutation matrices. \retTwo

      Let's denote $\mMat{L}_i$ to be the lower triangular matrix representing all row additions of row $i$ to the rest of the matrix. In other words, every element below the main \\diagonal in $\mMat{L}_i$ will be nonzero except for the elements in column $i$. Additionally, let's denote $\mMat{P}_{i, j}$ to be the permutation matrix swapping row $i$ and row $j$. Then, we can say that:
      \[\mMat{A} = \mMat{P}_{1, k_{1}}\mMat{L}_{1}\mMat{P}_{2, k_{2}}\mMat{L}_{2}\cdots\mMat{P}_{n-1, k_{n-1}}\mMat{L}_{n-1}\mMat{U}\]
      {\begin{myTindent}\begin{myTindent} \hFour
         (Note that $i < k_i \leq n$ since it doesn't make sense to swap a row that has already been dealt with.) \retTwo
      \end{myTindent}\end{myTindent}}

      We can rewrite this as $\mMat{P}_{1, k_{1}}\mMat{A} = \mMat{L}_{1}\mMat{P}_{2, k_{2}}\mMat{L}_{2}\cdots\mMat{P}_{n-1, k_{n-1}}\mMat{L}_{n-1}\mMat{U}$. And now here is where the magic starts. If we multiply both sides of that equation on the left by $\mMat{P}_{2, k_{2}}$, then we can say that $(\mMat{P}_{2, k_{2}}\mMat{L}_{1}\mMat{P}_{2, k_{2}})=\mMat{L}^\prime_{1}$ where $\mMat{L}^\prime_{1}$ is the matrix which would have resulted if we had only applied the permutation $\mMat{P}_{2, k_{2}}$ to the elements off the main diagonal.
      \newpage

      Now we can do the same process again to $(\mMat{L}^\prime_1\mMat{L}_2)\mMat{P}_{3, k_{3}}$, multiplying both sides of our equation by $\mMat{P}_{3, k_{3}}$ and then saying that $\mMat{L}^\prime_{2}=(\mMat{P}_{3, k_{3}}\mMat{L}^\prime_1\mMat{L}_2\mMat{P}_{3, k_{3}})$ is a lower triangular matrix whose only nonzero elements below the diagonal are in columns 1 and 2. Doing this for all remaining permutation matrices on the right side of the equation, we will eventually get an equation of the form:
      \[\mMat{P}_{n-1, k_{n-1}}\cdots\mMat{P}_{2, k_{2}}\mMat{P}_{1, k_{1}}\mMat{A} = \mMat{L}\mMat{U}\] \retTwo

      Finally, by multiplying those permutation matrices together, we get an equation: \[\mMat{P}\mMat{A} = \mMat{L}\mMat{U}\]

      {\hThree \center
      \begin{myClosureOne}{4.5}
         Here is why the product $\mMat{P}_{i+1, k_{i+1}}\mMat{L}^\prime_{i-1}\mMat{L}_{i}\mMat{P}_{i{+1}, k_{i+1}}$ was \\equivalent to only applying the row permutation underneath the diagonal of $\mMat{L}^\prime_{i-1}\mMat{L}_{i}$. \\ \\

         Observe the following diagram of where the permutation\\ matrices send the elements in row $i{+1}$, row $k_{i{+1}}$, column $i{+1}$, and column $k_{i{+1}}$ of the matrix $\mMat{L}^\prime_{i-1}\mMat{L}_{i}$: \\
         \[
         \begin{matrix}
            \begin{bmatrix}
               &     &     {\color{BrickRed}0}&    &    {\color{Orange}0}& & \\
               &     &     {\color{BrickRed}0}&    &    {\color{Orange}0}& & \\
               {\color{Plum}a}&     {\color{Plum}b}&     {\color{Plum}1}&    {\color{Plum}0}&    {\color{Plum}0}& {\color{Plum}0}& {\color{Plum}0}\\
               &     &     {\color{BrickRed}0}&    &    {\color{Orange}0}& & \\
               &     &     {\color{BrickRed}0}&    &    {\color{Orange}0}& & \\
               {\color{Magenta}c}&     {\color{Magenta}d}&     {\color{Magenta}0}&    {\color{Magenta}0}&    {\color{Magenta}1}& {\color{Magenta}0}& {\color{Magenta}0}\\
               &     &     {\color{BrickRed}0}&    &    {\color{Orange}0}& & \\
               &     &     {\color{BrickRed}0}&    &    {\color{Orange}0}& & \\
            \end{bmatrix}
            \\
            {\left\downarrow \vphantom{{\int_A^\frac{\frac{\frac{A}{1}}{1}}{1}}} \right.}
            \\
            \begin{bmatrix}
               &     &     {\color{Orange}0}&    &    {\color{BrickRed}0}& & \\
               &     &     {\color{Orange}0}&    &    {\color{BrickRed}0}& & \\
               {\color{Magenta}c}&     {\color{Magenta}d}&     {\color{Magenta}1}&    {\color{Magenta}0}&    {\color{Magenta}0}& {\color{Magenta}0}& {\color{Magenta}0}\\
               &     &     {\color{Orange}0}&    &    {\color{BrickRed}0}& & \\
               &     &     {\color{Orange}0}&    &    {\color{BrickRed}0}& & \\
               {\color{Plum}a}&     {\color{Plum}b}&     {\color{Plum}0}&    {\color{Plum}0}&    {\color{Plum}1}& {\color{Plum}0}& {\color{Plum}0}\\
               &     &     {\color{Orange}0}&    &    {\color{BrickRed}0}& & \\
               &     &     {\color{Orange}0}&    &    {\color{BrickRed}0}& & \\
            \end{bmatrix}
         \end{matrix} \]
      \end{myClosureOne}
      \par}
   \end{myIndent}
   \hOne \retTwo \retTwo
   \uuline{Theorem}: Every invertible matrix has a PLU-decomposition.
   \newpage
   \markLecture{1/22/2024}

   A matrix $\mMat{A} \in \mathbb{R}^{n\times n}$ is said to be \udefine{positive definite} if:
   \begin{enumerate}
      \item $\mMat{A}^T=\mMat{A} \quad$ ($\mMat{A}$ is symmetric)
      \item $\mVec{x}^T\mMat{A}\mVec{x} = \langle\mVec{x}, \mMat{A}\mVec{x}\rangle  > 0$ for all vectors $\mVec{x} \in \mathbb{R}^n$ with $\mVec{x} \neq 0$.
      {\begin{myTindent}\begin{myTindent} \hFour
         ${\displaystyle \langle \mVec{x}, \mMat{A}\mVec{x} \rangle = \sum_{i=1}^n{\sum_{j=1}^n{x_ia_{i,j}x_j}}}$
      \end{myTindent}\end{myTindent}}
   \end{enumerate} \retTwo

   {\hTwo
   \begin{myIndent}
      \uuline{Lemma}: If $\mMat{A}$ is positive definite, then $\mMat{A}$ is invertible.
      {\hThree %Aww this class gonna finally make me be rigorous...
      \begin{myIndent}
         Proof: (we proceed towards a contradiction)\retTwo
         Assume there exists a vector $\mVec{y} \in \mathbb{R}^n$ not equal to $0$ such that $\mMat{A}\mVec{y} = 0$. Then $\langle \mVec{y}, \mMat{A}\mVec{x} \rangle = \langle \mVec{y}, 0 \rangle = 0$. So, $\mMat{A}$ cannot be positive definite.
      \end{myIndent}}\retTwo

      \uuline{Theorem}: Let $\mMat{M} \in \mathbb{R}^{n\times n}$ be invertible. Then $\mMat{A} = \mMat{M}^T\mMat{M}$ is positive definite.

      {\begin{myIndent} \hThree
         Proof:
         \begin{enumerate}
            \item $\mMat{A}^T = \left(\mMat{M}^T\mMat{M}\right)^T = \mMat{M}^T\left(\mMat{M}^T\right)^T = \mMat{M}^T\mMat{M} = \mMat{A}$. So $\mMat{A}$ is symmetric.
            \item Note that {$\mVec{x}^T\mMat{A}\mVec{x} = \mVec{x}^T\mMat{M}^T\mMat{M}\mVec{x} =(\mMat{M}\mVec{x})^T\mMat{M}\mVec{x} = \langle \mMat{M}\mVec{x}, \mMat{M}\mVec{x} \rangle = \lvert \mMat{M}\mVec{x} \rvert$.} Now, $\lvert \mMat{M}\mVec{x} \rvert > 0$ for all $\mMat{M}\mVec{x} \neq 0$. And because $\mMat{M}$ is invertible, we know the only $\mVec{x}$ such that $\mMat{M}\mVec{x} = 0$ is $\mVec{x} = 0$. So, $\mVec{x}^T\mMat{A}\mVec{x} > 0$ for all $\mVec{x} \neq 0$.
            {\begin{myIndent} \hFour
               If $\mMat{M}$ is not invertible, then $\mMat{A}$ is positive semidefnite as $\lvert \mMat{M}\mVec{x} \rvert$ cannot be less than $0$ but we can find a nonzero $\mVec{x}$ such that $\lvert \mMat{M}\mVec{x} \rvert = 0$. 
            \end{myIndent}}
         \end{enumerate} \retTwo
      \end{myIndent}}

      \uuline{Theorem} (\udefine{Cholesky decomposition}):
      \begin{myIndent}
         Let $\mMat{A}$ be positive definite. Then there exists an upper triangular matrix $\mMat{R}$ such that $\mMat{A}=\mMat{R}^T\mMat{R}$. We call $\mMat{R}$ the \udefine{Cholesky factor} and can calculate it as follows:\retTwo
   
         \hThree
         If such an $\mMat{R}$ exists, then we can arrive at the following formula for each\\ element of $\mMat{A} = \mMat{R}^T\mMat{R}$:
            \[a_{i,j}=\mathlarger{\mathlarger{\sum}}_{k=1}^{\min{(i,j)}}{r_{k,i}r_{k,j}}\]

         Now as $\mMat{A}$ must be symmetric based on our formula, we can safely ignore the elements of $\mMat{A}$ below the main diagonal. So, assuming that $j \geq i$, we can rearrange terms to get the following equation including the element $a_{i,j}$:
         \[r_{i,i}r_{i,j} = a_{i,j} - \sum_{k=1}^{i-1}{r_{k,i}r_{k,j}}\]

         \newpage
         
         And with that, we now have a way of expressing any element $r_{i,j}$ in terms of \\elements of $\mMat{R}$ which are in previous rows or previous columns of $\mMat{R}$. So, we can inductively solve for the coefficients of $\mMat{R}$ as follows: \retTwo
         {\begin{myIndent} \hFour
            for $i = 1, \ldots, n$:\\
            \begin{myIndent}
               $r_{i,i} = \sqrt{a_{i,i} - {\displaystyle\sum_{k=1}^{i-1}{r_{k,i}^2}}}$\\

               for $j = (i+1), \ldots, n$:\\
               \begin{myIndent}
               $r_{i,j} = \dfrac{1}{r_{i,i}}\left(a_{i,j} - {\displaystyle\sum_{k=1}^{i-1}{r_{k,i}r_{k,j}}}\right)$
               \end{myIndent}
               end
            \end{myIndent}
            end \\
         \end{myIndent}}

         Now it's worth noting what can go wrong in the above algorithm. 
         \begin{myIndent}
            Firstly, if $a_{i,i} - {\displaystyle\sum_{k=1}^{i-1}{r_{k,i}^2}} < 0$, then $r_{i,i} \notin \mathbb{R}$. So, $\mMat{R}$ does not exist \retTwo
            
            Secondly, if $r_{i,i} = 0$, then you can't divide by it to isolate $r_{i,j}$. Thus, one of two possibilities will arise:
            \begin{itemize}
               \item If $\left(a_{i,j} - {\displaystyle\sum_{k=1}^{i-1}{r_{k,i}r_{k,j}}}\right) = 0$, then you can set $r_{i,j}$ to anything and maintain equality. Thus, if a Cholesky factorization exists, it is not unique. \\
               \item If $\left(a_{i,j} - {\displaystyle\sum_{k=1}^{i-1}{r_{k,i}r_{k,j}}}\right) \neq 0$, then there is nothing you can set $r_{i,j}$ to. So, no Cholesky factor of $\mMat{A}$ exists.
            \end{itemize}
         \end{myIndent}\retTwo
      
         Now unfortunately, this class is not interested in proving when and when not each of the above problems will arise. So, for now just know that:
         \begin{enumerate}
            \item $\mMat{A}$ is positive definite if and only if $\mMat{R}$ exists and is unique.
            \item $\mMat{A}$ is positive \uuline{semidefinite} if and only if $\mMat{R}$ exists but is not invertible and thus also not unique.
            \item $\mMat{A}$ is not positive definite or semidefinite if and only if $\mMat{R}$ does not exist.
         \end{enumerate}
         {\begin{myDindent} \hFour
            The second theorem covered in lecture today let's us easily prove one direction in each of the above implications. So, the challenging task would be to prove the other direction, and to do that you would need to show that for any positive definite or semidefinite matrix $\mMat{A}$, you can always use the above algorithm to find a matrix $\mMat{R}$.
            \newpage
            \teachComment
            It takes approximately $\frac{1}{3}n^3$ flops to do Cholesky decomposition as $n$ gets larger. This is notably half of what LU decomposition takes.
         \end{myDindent}}
      \end{myIndent}
   \end{myIndent}} \retTwo

   \markLecture{1/24/2024}

   A \udefine{norm} of a vector $\mVec{x} \in \mathbb{R}^n$ is a real number $\|\mVec{x}\|$ that is assigned to $\mVec{x}$ satisfying:
   \begin{itemize}
      \item $\mVec{x} \neq 0 \Longrightarrow \|\mVec{x}\| > 0$ whereas $\| \mVec{0} \| = 0$.
      \item $\| c\mVec{x} \| = \lvert c \rvert \| \mVec{x} \|$ for all $c \in \mathbb{R}$.
      \item $\|\mVec{x}+\mVec{y}\| \leq \|\mVec{x}\| + \|\mVec{y}\|$ for all $\mVec{x}, \mVec{y} \in \mathbb{R}^n$
   \end{itemize} \retTwo

   Some important vector norms:
   \begin{enumerate}
      \item \udefine{Vector $p$-norm}: For an integer $p \geq 1$, we define $\|\mVec{x}\|_p = \left({\displaystyle \sum_{i=1}^n{\lvert x_i \rvert^p}}\right)^\frac{1}{p}$
      
      \item \udefine{Infinity Norm}: $\|\mVec{x}\|_\infty = \max{\{\lvert x_i\rvert \mid 1\leq i \leq n\}}$
   \end{enumerate}

   \mySepTwo

   A \udefine{matrix norm} assigns a real value $\|\mMat{A}\|$ to a matrix $\mMat{A}$ satisfying:
   \begin{itemize}
      \item $\mMat{A} \neq \mMat{0} \Longrightarrow \|\mMat{A}\| > 0$ whereas $\| \mMat{0} \| = 0$.
      \item $\| c\mMat{A} \| = \lvert c \rvert \| \mMat{A} \|$ for all $c \in \mathbb{R}$.
      \item $\|\mMat{A}+\mMat{B}\| \leq \|\mMat{A}\| + \|\mMat{B}\|$ for all $\mMat{A}, \mMat{B} \in \mathbb{R}^{n\times n}$
   \end{itemize} \retTwo

   Some important matrix norms:
   \begin{enumerate}
      \item \udefine{Frobenius Norm}: $\| \mMat{A} \| = \left({\displaystyle\sum_{i,j=1}^n{a_{i,j}^2}}\right)^\frac{1}{2}$
      {\begin{myTindent} \teachComment
         Assuming $\mMat{A}$ is an $m\times n$ matrix, this norm is equivalent to stringing out $\mMat{A}$'s rows or columns to form an $mn$ element vector and then taking the $2$-norm of the resulting vector.
         \[
         \left\|\begin{bmatrix}
            a & b & c \\ d & e & f \\ g & h & i
         \end{bmatrix}\right\| = \left\|(a, b, c, d, e, f, g, h, i)\right\|\]
      \end{myTindent}}
      \item \udefine{Induced Norm}: Let $\|\cdot\|$ be a vector norm on $\mathbb{R}^m$ and $\mathbb{R}^n$. Then the matrix norm induced by $\|\cdot \|$ is:  \[\|\mMat{A}\| = \max\limits_{\mVec{x} \neq 0}{\frac{\|\mMat{A}\mVec{x}\|}{\|\mVec{x}\|}}\]
   \end{enumerate}
   \newpage
   
   {\begin{myIndent} \hTwo
      The induced norm can be thought of as measuring the maximum stretch which a linear function $L(\mVec{x}) = \mMat{A}\mVec{x}$ applies to $\mVec{x}$ relative to $\mVec{x}$'s starting length. \retTwo

      Importantly by the properties of a norm:
      \[\frac{\|\mMat{A}\mVec{x}\|}{\|\mVec{x}\|} = \left\|\frac{1}{\|\mVec{x}\|}\mMat{A}\mVec{x}\right\| = \left\|\mMat{A}\frac{\mVec{x}}{\|\mVec{x}\|}\right\| = \left\|\mMat{A}\hat{x}\right\|\]
      Thus, it's also common to see induced norms defined as: $\|\mMat{A}\| = \max\limits_{\|\mVec{x}\| = 1}{\|\mMat{A}\mVec{x}\|}$
      \retTwo

      Some important formulas: (let $\mMat{A}$ be an $m \times n$ matrix)
      \begin{itemize}
         \item $\|\mMat{A}\|_1 = \max\limits_{1\leq j \leq n}{{\displaystyle \sum_{k=1}^m{\lvert a_{k,j} \rvert}}}$ \retTwo
         
         {\begin{myIndent} \hFour
            To prove this, first note that: \[\|\mMat{A}\mVec{x}\|_1 = \sum_{i=1}^m{\left|\sum_{j=1}^n{a_{i,j}x_j}\right|} \leq \sum_{i=1}^m{\sum_{j=1}^n{\left|a_{i,j}\right|\left|x_j\right|}} = \sum_{j=1}^n{\left(\left|x_j\right|\sum_{i=1}^m{\left|a_{i,j}\right|}\right)}\]
            Now lets replace each $a_{i,j}$ with the max element $a_{i,k}$ in the $i$th row. Thus: \[\sum_{j=1}^n{\left(\left|x_j\right|\sum_{i=1}^m{\left|a_{i,j}\right|}\right)} \leq \sum_{j=1}^n{\left(\left|x_j\right|\max\limits_{1\leq k \leq m}{
            \sum_{i=1}^m{\left|a_{i,k}\right|}}\right)}\]
            And now we can seperate the summands to get that: \[\|\mMat{A}\mVec{x}\|_1 \leq \|\mVec{x}\|_1\max\limits_{1\leq k \leq m}{\left(\sum_{i=1}^m{\left|a_{i,k}\right|}\right)}\]
            Now $\|\mMat{A}\|_1$ is the max value of $\|\mMat{A}\mVec{x}\|_1$ when $\|\mVec{x}\|_1 = 1$. So clearly: \[\|\mMat{A}\|_1 \leq{\displaystyle\max\limits_{1\leq k \leq m}{\left(\sum_{i=1}^m{\left|a_{i,k}\right|}\right)}}\]
            Finally, to show equality note that the $k$th. standard unit basis vector $e_k$ satisfies both that $\|e_k\|_1 = 1$ and that:
            \[\|\mMat{A}e_k\| = \max\limits_{1\leq k \leq m}{\left(\sum_{i=1}^m{\left|a_{i,k}\right|}\right)}\]. \retTwo
         \end{myIndent}}

         \item $\|\mMat{A}\|_\infty = \max\limits_{1\leq i \leq m}{{\displaystyle \sum_{k=1}^n{\lvert a_{i,k} \rvert}}}$ \retTwo
         
         {\begin{myIndent} \hFour
            This one is more obvious. The $\mVec{x}$ which maximizes $\|\mMat{A}\mVec{x}\|_\infty$ while $\|\mVec{x}\|_\infty = 1$ is the vector whose elemnts are $1$ and $-1$ such that the $k$th element of $\mVec{x}$ has the same sign as the $k$th element of the row of $\mMat{A}$ whose elements have the largest sum of absolute values.
         \end{myIndent}}
         \newpage

         \item $\| \mMat{A} \|_2 = \sqrt{\lambda_{\max}(\mMat{A}^T\mMat{A})}$
         {\begin{myTindent}\begin{myIndent} \teachComment
            ($\lambda_{\max}(\mMat{M})$ refers to the largest eigenvalue of $\mMat{M}$.)
         \end{myIndent}\end{myTindent}}
         {\begin{myIndent} \hFour
            Neither the lecture notes or the author of our textbook BÃ¶rgers at this point gives a proof of this. However, at the very least we know from the proposition on page 8 that $\mMat{A}^T\mMat{A}$ is positive definite or positive semidefinite. So, all of its eigenvalues are greater than or equal to zero, meaning that our distance \\formula will always give a real value. \retTwo
         \end{myIndent}}
      \end{itemize}
   \end{myIndent}}
   
   \markLecture{1/26/2024}

   Here is proof that induced norms are matrix norms. \hTwo
   \begin{enumerate}
      \item Positivity:
      {\begin{myIndent} \hThree
         If $\|\mMat{A}\| = 0$, then $\|\mMat{A}\mVec{x}\| = 0$ for all nonzero $\mVec{x}$. This means that $\mMat{A}\mVec{x} = \mVec{0}$ for all nonzero $\mVec{x}$. However, note that $\mMat{A}e_k$ equals the $k$th column of $\mMat{A}$. So, all columns of $\mMat{A}$ must be zero. Hence, $\|\mMat{A}\| = 0 \Longrightarrow \mMat{A} = \mMat{0}$
      \end{myIndent}}

      \item Multiplication by a constant:
      {\begin{myIndent} \hThree
         $\|c\mMat{A}\| = \max\limits_{\mVec{x} \neq 0}{\dfrac{\|c\mMat{A}\mVec{x}\|}{\|\mVec{x}\|}} = \max\limits_{\mVec{x} \neq 0}{\dfrac{\left|c\right|\|\mMat{A}\mVec{x}\|}{\|\mVec{x}\|}} = \left|c\right|\left(\max\limits_{\mVec{x} \neq 0}{\dfrac{\|\mMat{A}\mVec{x}\|}{\|\mVec{x}\|}}\right) = \left|c\right|\|\mMat{A}\|$
      \end{myIndent}}

      \item Triangle inequality:
      {\begin{myIndent} \hThree
         {\center $\|\mMat{A}+\mMat{B}\| = \max\limits_{\mVec{x} \neq 0}{\dfrac{\|(\mMat{A}+\mMat{B})\mVec{x}\|}{\|\mVec{x}\|}} = \max\limits_{\mVec{x} \neq 0}{\dfrac{\|\mMat{A}\mVec{x} + \mMat{B}\mVec{x}\|}{\|\mVec{x}\|}}$ \par} Now, by the triangle inequality of the vector norm, we have that:\\
         {\center $\dfrac{\|\mMat{A}\mVec{x} + \mMat{B}\mVec{x}\|}{\|\mVec{x}\|} \leq \dfrac{\|\mMat{A}\mVec{x}\| + \|\mMat{B}\mVec{x}\|}{\|\mVec{x}\|} = \dfrac{\|\mMat{A}\mVec{x}\|}{\|\mVec{x}\|} + \dfrac{\|\mMat{B}\mVec{x}\|}{\|\mVec{x}\|}$ \par} So, $\|\mMat{A}+\mMat{B}\| \leq \|\mMat{A}\| + \|\mMat{B}\|$.
      \end{myIndent}}
   \end{enumerate}

   \hOne
   Here are two more properties of induced norms:
   {\hTwo
   \begin{enumerate}
      \item[4.] $\|\mMat{A}\mMat{B}\| \leq \|\mMat{A}\|\|\mMat{B}\| \quad \quad$ {\teachComment This is called \uuline{submultiplicativity}.}
      {\begin{myIndent} \hThree
         $\|\mMat{A}\mMat{B}\| = \max\limits_{\mVec{x} \neq 0}{\dfrac{\|(\mMat{A}\mMat{B})\mVec{x}\|}{\|\mVec{x}\|}} = \max\limits_{\mVec{x} \neq 0}{\left(\dfrac{\|\mMat{A}\mMat{B}\mVec{x}\|}{\|\mMat{B}\mVec{x}\|}\dfrac{\|\mMat{B}\mVec{x}\|}{\|\mVec{x}\|}\right)}$ if $\mMat{B}\mVec{x} \neq 0$. \retTwo
         Now this is obviously less than or equal to $\max\limits_{\mMat{B}\mVec{x} \neq 0}\left({\dfrac{\|\mMat{A}\mMat{B}\mVec{x}\|}{\|\mMat{B}\mVec{x}\|}}\right)\max\limits_{\mVec{x} \neq 0}\left({\dfrac{\|\mMat{B}\mVec{x}\|}{\|\mVec{x}\|}}\right)$ \retTwo
         Finally defining $\mVec{y} = \mMat{B}\mVec{x}$, we get that:
         \[\|\mMat{A}\mMat{B}\| \leq \max\limits_{\mVec{y} \neq 0}\left({\dfrac{\|\mMat{A}\mVec{y}\|}{\|\mVec{y}\|}}\right)\max\limits_{\mVec{x} \neq 0}\left({\dfrac{\|\mMat{B}\mVec{x}\|}{\|\mVec{x}\|}}\right) = \|\mMat{A}\|\|\mMat{B}\|\]
      \end{myIndent}}
      \newpage
      \item[5.] $\|\mMat{A}\mVec{x}\| \leq \|\mMat{A}\|\|\mVec{x}\|$ for all $\mVec{x} \in \mathbb{R}^n$.
      {\begin{myIndent} \hThree
         To explain this, remember that $\|\mMat{A}\|$ is the maximum value that $\frac{\|\mMat{A}\mVec{x}\|}{\|\mVec{x}\|}$ could be.
      \end{myIndent}}
   \end{enumerate}}

   \mySepTwo

   {\fontsize{18}{18}\selectfont \uuline{Sensitivity of $\mMat{A}\mVec{x}=\mVec{b}$ with respect to perturbation}}
   \retTwo

   Due to rounding errors and limited precision, we know that $\mVec{b}$ and $\mMat{A}$ will \\typically not be perfectly accurate when doing calculations. We refer to introducing this inaccuracy as perturbing $\mVec{b}$ and $\mMat{A}$. Now as seen in the demonstration below, this will effect the accuracy of our calculations. So, it would be nice to be able to predict and quantify that loss in accuracy.\retTwo

   {\center \exOne
   \begin{myClosureOne}{5.5}
      Demonstration of perturbing $\mVec{b}$: \\
      \begin{myIndent} \exTwo
         $
         \begin{bmatrix}
            1 & 1 \\ 0 & 1
         \end{bmatrix}
         \begin{bmatrix}
            x_1 \\ x_2
         \end{bmatrix} = 
         \begin{bmatrix}
            2 \\ 0
         \end{bmatrix}$ gives a solution of $\mVec{x} = 
         \begin{bmatrix}
            2 \\ 0
         \end{bmatrix}$. Meanwhile, \newline \newline
            $
            \begin{bmatrix}
               1 & 1 \\ 0 & 1
            \end{bmatrix}
            \begin{bmatrix}
               x_1 \\ x_2
            \end{bmatrix} = 
            \begin{bmatrix}
               2 \\ 0.001
            \end{bmatrix}$ gives a solution of $\mVec{x}^\prime = 
            \begin{bmatrix}
               1.999 \\ 0.001
            \end{bmatrix}$. \retTwo

            The error in $\mVec{b}$ is $\|\mVec{b} - \mVec{b}^\prime\|_2 = 0.001$, whereas the \newline corresponding error in $\mVec{x}$ is $\|\mVec{x} - \mVec{x}^\prime\|_2 = \sqrt{2}(0.001) \approx 0.0014$
      \end{myIndent}
   \end{myClosureOne}
   \par} \retTwo

   For now, let's address perturbing $\mVec{b}$\dots
   {\hTwo\begin{myIndent}
      Assume $\mMat{A}$ is an invertible matrix.\\
      Formally, we shall write:
         
         \begin{myIndent} $
            \begin{matrix}
               \text{\color{BrickRed}Original equation: } & \mMat{A}\mVec{x} = \mVec{b} \\
               \text{\color{BrickRed}New equation: } & \mMat{A}\mVec{x}^\prime = \mVec{b}^\prime
            \end{matrix}\quad \quad$ {\hFour where $\mVec{b}^\prime = \mVec{b} + \mVec{\delta b}$ and $\mVec{x}^\prime = \mVec{x} + \mVec{\delta x}$}
         \end{myIndent}
      
      Then for any vector norm and its induced matrix norm: $\dfrac{\|\mVec{\delta x}\|}{\|\mVec{x}\|} \leq \|\mMat{A}\|\|\mMat{A}^{-1}\|\dfrac{\|\mVec{\delta b}\|}{\|\mVec{b}\|}$.\retTwo

      {\begin{myIndent} \hThree
         Proof: \\
         Firstly, $\mMat{A}\mVec{x}^\prime = \mVec{b}^\prime \Longrightarrow \mMat{A}\mVec{x} + \mMat{A}\mVec{\delta x} = \mVec{b} + \mVec{\delta b}$. Now since $\mMat{A}\mVec{x} = \mVec{b}$, it must be the case that $\mMat{A}\mVec{\delta x} = \mVec{\delta b}$. So, $\mVec{\delta x} = \mMat{A}^{-1}\mVec{\delta b}$. From this, we now get that for any vector norm and corresponding induced matrix norm, we have that\\ $\|\mVec{\delta x}\| = \|\mMat{A}^{-1}\mVec{\delta b}\| \leq \|\mMat{A}^{-1}\|\|\mVec{\delta b}\|$.
         \newpage
         Secondly, using the same norms as before, we have that:\\ $\mVec{b} = \mMat{A}\mVec{x} \Longrightarrow \|\mVec{b}\| = \|\mMat{A}\mVec{x}\| \leq \|\mMat{A}\|\|\mVec{x}\|$. Thus dividing both sides by\\ $\|\mVec{b}\|\|\mVec{x}\|$, we get that:
         \[\frac{1}{\|\mVec{x}\|} \leq \|\mMat{A}\|\frac{1}{\|\mVec{b}\|}\]
         \retTwo
         Thus in conclusion: $\dfrac{\|\mVec{\delta x}\|}{\|\mVec{x}\|} \leq \|\mMat{A}\|\|\mMat{A^{-1}}\|\dfrac{\|\mVec{\delta b}\|}{\|\mVec{b}\|}$
      \end{myIndent}} \retTwo

      We call $K(\mMat{A}) = \|\mMat{A}\|\|\mMat{A^{-1}}\|$ the \udefine{condition number} of $\mMat{A}$ under some induced norm (indicated as a subscript to $K$). Clearly, it is useful as it gives us an upper bound to the relative error of our solution $\mVec{x}$ given the relative error in $\mVec{b}$.
      
      \begin{itemize}
         \item If $K(\mMat{A})$ is small (close to $1$), then $\mMat{A}$ is called \udefine{well conditioned}.
         \item If $K(\mMat{A})$ is large, then $\mMat{A}$ is called \udefine{ill conditioned}. \retTwo
      \end{itemize}
   \end{myIndent}}

   \markLecture{1/29/2024}

   Now let's consider perturbing $\mMat{A}$. As before, we shall assume that $\mMat{A}$ is invertible.
   {\begin{myIndent} \hTwo
      Formally, we shall write:
      \begin{myIndent} $
         \begin{matrix}
            \text{\color{BrickRed}Original equation: } & \mMat{A}\mVec{x} = \mVec{b} \\
            \text{\color{BrickRed}New equation: } & \mMat{A}^\prime\mVec{x}^\prime = \mVec{b}
         \end{matrix}\quad \quad$ {\hFour where $\mMat{A}^\prime = \mMat{A} + \mMat{\delta A}$ and $\mVec{x}^\prime = \mVec{x} + \mVec{\delta x}$}
      \end{myIndent}

      Then for any vector norm and its induced matrix norm: $\dfrac{\|\mVec{\delta x}\|}{\|\mVec{x}^\prime\|} \leq K(A)\dfrac{\|\mMat{\delta A}\|}{\|\mMat{A}\|}$.\retTwo

      {\begin{myIndent} \hThree
         Proof:\\
         We can rewrite $\mMat{A}^\prime\mVec{x}^\prime = \mVec{b}$ as $\mMat{A}\mVec{x} + \mMat{A}\mVec{\delta x} + \mMat{\delta A}(\mVec{x} + \mVec{\delta x}) = \mVec{b}$. Then, by canceling out $\mVec{b}$ and $\mMat{A}\mVec{x}$, we get the formula: $\mMat{A}\mVec{\delta x} + \mMat{\delta A}\mVec{x}^\prime = 0$. And, further rearranging of this yields:

         {\centering $\mVec{\delta x} = -\mMat{A}^{-1}(\mMat{\delta A})\mVec{x}^\prime$ \retTwo \par}

         So by the submultiplicativity property of matrix norms:

         {\centering $\|\mVec{\delta x}\| = \|\mMat{A}^{-1}(\mMat{\delta A})\mVec{x}^\prime\| \leq \|\mMat{A}^{-1}\|\|(\mMat{\delta A})\mVec{x}^\prime\| \leq \|\mMat{A}^{-1}\|\|(\mMat{\delta A})\|\|\mVec{x}^\prime\|$ \retTwo \par}

         Dividing both sides by our above ineqality by $\|\mVec{x}^\prime\|$ and multiplying the greater side by $1 = \frac{\|\mMat{A}\|}{\|\mMat{A}\|}$, we finally get that:
         \[\dfrac{\|\mVec{\delta x}\|}{\|\mVec{x}^\prime\|} \leq \|\mMat{A}^{-1}\|\|\mMat{A}\|\dfrac{\|\mMat{\delta A}\|}{\|\mMat{A}\|} =  K(A)\dfrac{\|\mMat{\delta A}\|}{\|\mMat{A}\|}\]
      \end{myIndent}}
   \end{myIndent}}

   \newpage

   Finally, let's consider perturbing $\mMat{A}$ and $\mVec{b}$. Once again, we will assume $\mMat{A}^{-1}$ exists.
   {\begin{myIndent} \hTwo
      Formally, we shall write:
      \begin{myIndent} $
         \begin{matrix}
            \text{\color{BrickRed}Original equation: } & \mMat{A}\mVec{x} = \mVec{b} \\
            \text{\color{BrickRed}New equation: } & \mMat{A}^\prime\mVec{x}^\prime = \mVec{b}^\prime
         \end{matrix}\quad \quad$ {\hFour $
         \begin{matrix}
            \text{where } \mMat{A}^\prime = \mMat{A} + \mMat{\delta A}, \mVec{b}^\prime = \mVec{b} + \mVec{\delta b},\\ \text{ and } \mVec{x}^\prime = \mVec{x} + \mVec{\delta x} 
         \end{matrix}$
         \retTwo}
      \end{myIndent}
      
      Then for any vector norm and its induced matrix norm:\\ \[\dfrac{\|\mVec{\delta x}\|}{\|\mVec{x}^\prime\|} \leq K(A)\left(\dfrac{\|\mMat{\delta A}\|}{\|\mMat{A}\|} + \dfrac{\|\mVec{\delta b}\|}{\|\mVec{b}^\prime\|} + \dfrac{\|\mMat{\delta A}\|}{\|\mMat{A}\mVec{x}\|}\dfrac{\|\mVec{\delta b}\|}{\|\mVec{b}^\prime\|} \right)\].\retTwo

      
      {\begin{myIndent} \hThree
         Proof:\retTwo
         Step 1:\\ $\mMat{A}^\prime \mVec{x}^\prime = \mVec{b}^\prime$ can be rewritten as: $\mMat{A}\mVec{x} + \mMat{A}\mVec{\delta x} + \mMat{\delta A}(\mVec{x} + \mVec{\delta x}) = \mVec{b} + \mVec{\delta b}$. Then as $\mMat{A}\mVec{x} = \mVec{b}$, we cancel those two terms to ge that $\mMat{A}\mVec{\delta x} + \mMat{\delta A}(\mVec{x}^\prime) = \mVec{\delta b}$. And so, a little rearranging yields:

         {\centering $\mVec{\delta x} = \mMat{A}^{-1}\left(\mVec{\delta b} - \mMat{\delta A}\mVec{x}^\prime\right)$ \retTwo \par}

         Next, using properties of the induced matrix norm, we get that:

         {\centering $
         \begin{matrix}
            \|\mVec{\delta x}\| = \|\mMat{A}^{-1}\left(\mVec{\delta b} - \mMat{\delta A}\mVec{x}^\prime\right)\| \leq \|\mMat{A}^{-1}\|\|\mVec{\delta b} - \mMat{\delta A}\mVec{x}^\prime\| \\

            \phantom{aaaaaaaaaaaaaaaaaaaaaaaaaaaa %first try
            } \leq \|\mMat{A}^{-1}\|\left(\|\mVec{\delta b}\| + \|\mMat{\delta A}\mVec{x}^\prime\|\right) \\

            \phantom{aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa %second try
            } \leq \|\mMat{A}^{-1}\|\left(\|\mVec{\delta b}\| + \|\mMat{\delta A}\|\|\mVec{x}^\prime\|\right)
         \end{matrix}
         $ \retTwo \par}

         Therefore, by dividing both sides of our inequality by $\|\mVec{x}^\prime\|$ and multiplying the greater side by $1 = \frac{\|\mMat{A}\|}{\|\mMat{A}\|}$, we finally get that:
         \[\dfrac{\|\mVec{\delta x}\|}{\|\mVec{x}^\prime\|} \leq K(A)\left(\dfrac{\|\mVec{\delta b}\|}{\|\mMat{A}\|\|\mVec{x}^\prime\|}+  \dfrac{\|\mMat{\delta A}\|}{\|\mMat{A}\|}\right)\] \\

         Step 2:\\ $\mVec{b}^\prime = \mMat{A}^\prime\mVec{x}^\prime \Longrightarrow \|\mVec{b}^\prime\| = \|(\mMat{A}^\prime)\mVec{x}^\prime\| \leq \|\mMat{A} + \mMat{\delta A}\|\|\mVec{x}^\prime\|$ \\
         So dividing by $\|\mVec{b}^\prime\|\|\mVec{x}^\prime\|$, we get that: \[
            \frac{1}{\|\mVec{x}^\prime\|} \leq \frac{\|\mMat{A} + \mMat{\delta A}\|}{\|\mVec{b}^\prime\|} \leq \frac{\|\mMat{A}\| + \|\mMat{\delta A}\|}{\|\mVec{b}^\prime\|}\]
         \newpage
         Step 3:\\
         By combining the inequalities in steps 1 and 2, we know:
         \[\dfrac{\|\mVec{\delta x}\|}{\|\mVec{x}^\prime\|} \leq K(A)\left(\dfrac{\|\mVec{\delta b}\|}{\|\mMat{A}\|}\left(\dfrac{\|\mMat{A}\| + \|\mMat{\delta A}\|}{\|\mVec{b}^\prime\|}\right)  +  \dfrac{\|\mMat{\delta A}\|}{\|\mMat{A}\|}\right)\]
         We can then manipulate that into the inequality claimed above. \retTwo
      \end{myIndent}}

      
      \begin{myTindent}\begin{myIndent}
         \myComment
         If you are wondering why the upper bounds we derived in this lecture are for $\frac{\|\mVec{\delta x}\|}{\|\mVec{x}^\prime\|}$ instead of $\frac{\|\mVec{\delta x}\|}{\|\mVec{x}\|}$, know that I am too. I asked the professor and he said its because the upper bounds for $\frac{\|\mVec{\delta x}\|}{\|\mVec{x}\|}$ are more difficult. That said, our textbook does derive upper bounds for $\frac{\|\mVec{\delta x}\|}{\|\mVec{x}\|}$.
      \end{myIndent}\end{myTindent}
   \end{myIndent}}

\end{document}
