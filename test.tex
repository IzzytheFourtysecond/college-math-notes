\documentclass{book}

\usepackage{fontspec} % used to import Calibri
\usepackage{anyfontsize} % used to adjust font size

% needed for inch and other length measurements
% to be recognized
\usepackage{calc}

% for colors and text effects as is hopefully obvious
\usepackage[dvipsnames]{xcolor}
\usepackage{soul}

% control over margins
\usepackage[margin=1in]{geometry}
\usepackage[strict]{changepage}

\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{bm}

\usepackage[scr=rsfso, scrscaled=.96]{mathalpha}

% This is how I'm getting the nice caligraphy font :(
\DeclareMathAlphabet{\eulerscr}{U}{eus}{m}{n}
\newcommand{\mathcalli}[1]{\text{\scalebox{1.11}{$\eulerscr{#1}$}}}


\usepackage{amssymb} % originally imported to get the proof square
\usepackage{xfrac}
\usepackage[overcommands]{overarrows} % Get my preferred vector arrows...
\usepackage{relsize}

% Just am using this to get a dashed line in a table...
% Also you apparently want this to be inactive if you aren't
% using it because it slows compilation.
\usepackage{arydshln} \ADLinactivate 
\newenvironment{allowTableDashes}{\ADLactivate}{\ADLinactivate}

\usepackage{graphicx}
\graphicspath{{./158_Images/}}

\usepackage{tikz}
   \usetikzlibrary{arrows.meta}
   \usetikzlibrary{graphs, graphs.standard}

\usepackage{quiver} %commutative diagrams






\usepackage[hidelinks]{hyperref}
\newcommand{\inLinkRap}[2]{{\color{blue}\hyperlink{#1}{\textit{#2}}}}







\newfontfamily{\calibri}{Calibri}
\setlength{\parindent}{0pt}
\definecolor{RawerSienna}{HTML}{945D27}

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%Arrow Commands:

% Thank you Bernard, gernot, and Sigur who I copied this from:
% https://tex.stackexchange.com/questions/364096/command-for-longhookrightarrow
\renewcommand{\hookrightarrow}{\lhook\joinrel\rightarrow}
\renewcommand{\hookleftarrow}{\leftarrow\joinrel\rhook}
\newcommand{\hooklongrightarrow}{\lhook\joinrel\longrightarrow}
\newcommand{\hooklongleftarrow}{\longleftarrow\joinrel\rhook}
\newcommand{\hookxlongrightarrow}[2][]{\lhook\joinrel\xrightarrow[#1]{#2}}
\newcommand{\hookxlongleftarrow}[2][]{\xleftarrow[#1]{#2}\joinrel\rhook}

% Thank you egreg who I copied from:
% https://tex.stackexchange.com/questions/260554/two-headed-version-of-xrightarrow
\newcommand{\longrightarrowdbl}{\longrightarrow\mathrel{\mkern-14mu}\rightarrow}
\newcommand{\rightarrowdbl}{\rightarrow\mathrel{\mkern-14mu}\rightarrow}
\newcommand{\longleftarrowdbl}{\leftarrow\mathrel{\mkern-14mu}\longleftarrow}

\newcommand{\xrightarrowdbl}[2][]{%
  \xrightarrow[#1]{#2}\mathrel{\mkern-14mu}\rightarrow
}
\newcommand{\xleftarrowdbl}[2][]{%
  \leftarrow\mathrel{\mkern-14mu}\xleftarrow[#1]{#2}
}

\newcommand{\mRoman}[1]{%
   \textrm{\MakeUppercase{\romannumeral #1}}%
}

\newcommand{\locUConverges}{%
	\xrightarrow{\ell.u.}
}
\newcommand{\UConverges}{%
	\xrightarrow{u.}
}





% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newcommand{\hOne}{%
   \color{Black}%
   \fontsize{14}{16}\selectfont%
}
\newcommand{\hTwo}{%
\color{Black}%
   \fontsize{13}{15}\selectfont%
}
% \newcommand{\scratchWork}{%
%    \color{PineGreen!85!Orange}
%    \fontsize{12}{14}\selectfont%
% }
\newcommand{\hThree}{%
   \color{Black}%
   \fontsize{12}{14}\selectfont%
}
\newcommand{\myComment}{%
   \color{RawerSienna}%
   \fontsize{12}{14}\selectfont%
}
\newcommand{\pracOne}{
   \color{BrickRed}%
   \fontsize{13}{15}\selectfont%
}
\newcommand{\pracTwo}{
   \color{Orange}%
   \fontsize{12}{14}\selectfont%
}
\newcommand{\why}{%
   \color{Orange}%
   \fontsize{12}{14}\selectfont%
	Why:
}
\newcommand{\exOne}{%
   \color{Purple}%
   \fontsize{14}{16}\selectfont%
}
\newcommand{\exTwo}{%
   \color{Purple}%
   \fontsize{13}{15}\selectfont%
}
\newcommand{\exThree}{%
   \color{Purple}%
   \fontsize{12}{14}\selectfont%
}
\newcommand{\exP}{%
   \color{Purple}%
   \fontsize{12}{14}\selectfont%
}
\newcommand{\exTwoP}{%
   \color{RedViolet}%
   \fontsize{13}{15}\selectfont%
}
\newcommand{\exThreeP}{%
   \color{RedViolet}%
   \fontsize{12}{14}\selectfont%
}
\newcommand{\exFourP}{%
   \color{RedViolet}%
   \fontsize{11}{13}\selectfont%
}
\newcommand{\exPP}{%
   \color{RedViolet}%
   \fontsize{12}{14}\selectfont%
}
\newcommand{\exPPP}{%
   \color{VioletRed}%
   \fontsize{12}{14}\selectfont%
}

% Homework standard below (God the bloat in the header is absurd...)
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\newcommand{\Hstatement}{%
   \color{MidnightBlue!90!Black}%
   \fontsize{12}{13}\selectfont%
}
\newcommand{\HexOne}{%
   \color{Purple}%
   \fontsize{12}{13}\selectfont%
}
\newcommand{\HexTwoP}{%
   \color{RedViolet}%
   \fontsize{12}{13}\selectfont%
}
\newcommand{\HexPPP}{%
   \color{VioletRed}%
   \fontsize{11}{12}\selectfont%
}

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newcommand{\cyPen}[1]{{\vphantom{.}\color{Cerulean}#1}}
\newcommand{\redPen}[1]{{\vphantom{.}\color{Red}#1}}

\newenvironment{myIndent}{%
   \begin{adjustwidth}{2.5em}{0em}%
}{%
   \end{adjustwidth}%
}

\newenvironment{myDindent}{%
   \begin{adjustwidth}{5em}{0em}%
}{%
   \end{adjustwidth}%
}

\newenvironment{myTindent}{%
   \begin{adjustwidth}{7.5em}{0em}%
}{%
   \end{adjustwidth}%
}

\newenvironment{myConstrict}{%
   \begin{adjustwidth}{2.5em}{2.5em}%
}{%
   \end{adjustwidth}%
}

\newcommand{\udefine}[1]{{%
   \setulcolor{Red}%
   \setul{0.14em}{0.07em}%
   \ul{#1}%
}}

\newcommand{\uprop}[1]{{%
   \setulcolor{Purple}%
   \setul{0.14em}{0.07em}%
   \ul{#1} 
}}

\newcommand{\blab}[1]{\textbf{#1}}
\newcommand{\blect}[1]{{\color{MidnightBlue}\textbf{#1}}}

\newcommand{\uuline}[2][.]{%
{\vphantom{a}\color{#1}%
\rlap{\rule[-0.18em]{\widthof{#2}}{0.06em}}%
\rlap{\rule[-0.32em]{\widthof{#2}}{0.06em}}}%
#2}

\newcommand{\pprime}{{\prime\prime}}
\newcommand{\suchthat}{ \hspace{0.3em}s.t.\hspace{0.3em}}
\newcommand{\rea}[1]{\mathrm{Re}(#1)}
\newcommand{\ima}[1]{\mathrm{Im}(#1)}
\newcommand{\comp}{\mathsf{C}}
\newcommand{\trans}{\mathsf{T}}
\newcommand{\myHS}{ \hspace{0.5em}}
\newcommand{\gap}{\phantom{2}}

\newcommand{\GenLin}{\ensuremath{\mathrm{GL}}}
\newcommand{\Cay}{\ensuremath{\mathrm{Cay}}}

\newcommand{\myId}{\mathrm{Id}}
\newcommand{\myIm}{\mathrm{im}}
\newcommand{\Obj}{\mathrm{Obj}}
\newcommand{\Hom}{\mathrm{Hom}}
\newcommand{\End}{\mathrm{End}}
\newcommand{\Aut}{\mathrm{Aut}}

\newcommand{\df}{\mathrm{d}}
\newcommand{\Df}{\mathrm{D}}

\newcommand{\mcateg}[1]{{\bm{\mathsf{#1}}}}

\newcommand{\mdeg}{\mathrm{mdeg}\phantom{.}}

\newcommand{\dividesDeprecated}{\mathop{\mid}}
\newcommand{\divides}{\mathrel{\mid}}

\newcommand{\card}{\mathrm{card}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\diam}{\mathrm{diam}}
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\opnorm}{\mathrm{op}}
\newcommand{\loc}{\mathrm{loc}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\acc}{\mathrm{acc}}

\newcommand{\mSpan}{\mathrm{span}}
\newcommand{\Interior}{\mathop{\mathrm{Int}}}

\newcommand{\mMat}[1]{\mathbf{#1}}

\newcommand{\NBV}{\ensuremath{\mathrm{NBV}}}
\newcommand{\Acc}{\mathrm{Acc}}
\newcommand{\BV}{\ensuremath{\mathrm{BV}}}
\newcommand{\Var}{\ensuremath{\mathrm{Var}}}

\newcommand{\Alt}{\mathrm{Alt}}
\newcommand{\Sym}{\mathrm{Sym}}

\newcommand{\weakst}{weak$^*$ }

\newcommand{\radtimes}{\mathop{\widehat{\times}}}

\newcommand{\mMod}[1]{\phantom{a}(\mathrel{\mathrm{mod}} #1)}
\newcommand{\Fun}{\mathrm{Fun}}
\newcommand{\act}{\mathrm{act}}
\newcommand{\Fix}{\mathrm{Fix}}
\newcommand{\Sub}{\mathrm{Sub}}
\newcommand{\Cl}{\mathrm{Cl}}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\SL}{\mathrm{SL}}
\newcommand{\PSL}{\mathrm{PSL}}
\newcommand{\core}{\mathrm{core}}
\newcommand{\Syl}{\mathrm{Syl}}
\newcommand{\Iso}{\mathrm{Iso}}
\newcommand{\Homeo}{\mathrm{Homeo}}
\newcommand{\Inn}{\mathrm{Inn}}
\newcommand{\Out}{\mathrm{Out}}
\newcommand{\ab}{\mathrm{ab}}
\newcommand{\Max}{\mathrm{Max}}
\newcommand{\lt}{\mathrm{lt}}
\newcommand{\Nil}{\mathrm{Nil}}
\newcommand{\Ideal}{\mathrm{Ideal}}
\newcommand{\Spec}{\mathrm{Spec}}
\newcommand{\Res}{\mathrm{Res}}
\newcommand{\prim}{\mathrm{prim}}
\newcommand{\exConv}{\mathrm{ex}}
\newcommand{\mRank}{\mathrm{rank}}
\newcommand{\DivGroup}{\mathrm{Div}}
\newcommand{\divGroup}{\mathrm{div}}
\newcommand{\PrinGroup}{\mathrm{Prin}}
\newcommand{\Ann}{\mathrm{Ann}}
\newcommand{\Tor}{\mathrm{Tor}}
\newcommand{\diag}{\mathrm{diag}}

\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\Log}{Log}
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\symdif}{\triangle}
\DeclareMathOperator{\Average}{Average}
\DeclareMathOperator*{\AverageAst}{Average}

% Thank you Gonzalo Medina and Moriambar who wrote this on stack exchange:
%https://tex.stackexchange.com/questions/74125/how-do-i-put-text-over-symbols%
\newcommand{\myequiv}[1]{\stackrel{\mathclap{\mbox{\footnotesize{$#1$}}}}{\equiv}}

% Thank you chs who wrote this on stack exchange:
%https://tex.stackexchange.com/questions/89821/how-to-draw-a-solid-colored-circle%
\newcommand{\filledcirc}[1][.]{\ensuremath{\hspace{0.05em}{\color{#1}\bullet}\mathllap{\circ}\hspace{0.05em}}}

%Thank you blerbl who wrote this on stack exchange:
%https://tex.stackexchange.com/questions/25348/latex-symbol-for-does-not-divide
\newcommand{\ndiv}{\hspace{-0.3em}\not|\hspace{0.35em}}

\newcommand{\mySepOne}[1][.]{%
   {\noindent\color{#1}{\rule{6.5in}{1mm}}}\\%
}
\newcommand{\mySepTwo}[1][.]{%
   {\noindent\color{#1}{\rule{6.5in}{0.5mm}}}\\%
}
\newcommand{\mySepThree}[1][.]{%
   {\noindent\color{#1}{\rule{6in}{0.25mm}}}\\%
}

\newenvironment{myClosureOne}[2][.]{%
   \color{#1}%
   \begin{tabular}{|p{#2in}|} \hline \\%
}{%
   \\ \hline \end{tabular}%
}

\newcommand{\retTwo}{\hfill\bigbreak}

\newcommand{\dispDate}[1]{{
   \color{Black}%
   \fontsize{20}{18}\selectfont%
   #1\retTwo
}}


\begin{document}
\setul{0.14em}{0.07em}
\calibri

\hTwo Given a P.I.D. $D$, let $M(r; a_1, \ldots, a_m) \coloneqq D^r \oplus \bigoplus_{i=1}^m \frac{D}{\langle a_i\rangle}$ where $r \in \mathbb{Z}_{\geq 0}$ and\\ $a_i \in D - (D^\times \cup \{0\})$ satisfies that $a_1 \divides a_2 \divides \cdots \divides a_m$.\retTwo

\pracOne\mySepTwo
If $M$ is an $A$-module, then the \udefine{annihilator} of $M$ is:

{\centering$\Ann_A(M) \coloneqq \{a \in A : am = 0 \text{ for all }m \in M\}$.\retTwo\par}

\begin{myIndent}\pracTwo
	Note that $\Ann_A(M)$ is always an ideal of $A$. After all:
	
	{\centering$\Ann_A(M) = \bigcap_{m \in M} \ker(a \mapsto am)$.\retTwo\par}
\end{myIndent}

Similarly, if $m \in M$ then we define $\Ann_A(m) \coloneqq \{a \in A : am = 0\}$. This is also an ideal\\ of $A$.

\mySepTwo

\exTwo\ul{Lemma:} $\mRank(M(r; a_1, \ldots, a_m)) = r$.
\begin{myIndent}\exThreeP
	Since there is an injection $D^r \hookrightarrow M(r; a_1, \ldots, a_m) \eqqcolon M$, we know that:
	
	{\centering$r = \mRank(D^r) \leq \mRank(M)$.\retTwo\par}
	
	Meanwhile, suppose $v_1, \ldots, v_{r+1} \in M$. Then note that $a_m \in \Ann_D(\bigoplus_{i=1}^m \frac{D}{\langle a_i\rangle}) - \{0\}$. Thus we can get that $a_m v_1, \ldots, a_m v_{r+1}$ are in $D^r \oplus \{0\}$. And since $\mRank(D^r) = r$, there exists $c_1, \ldots, c_{r + 1}$ not all zero such that $c_1 a_m v_1 + \ldots + c_{r+1} a_m v_r = 0$. Thus, the $v_i$ are $D$-linearly dependent. And this proves that $\mRank(M) < r + 1$. $\blacksquare$\retTwo
\end{myIndent}

\hTwo Suppose $D$ is an integral domain and $M$ is a $D$-module. Then the \udefine{torsion} of $M$ is defined to be:

{\centering$\Tor(M) \coloneqq \{m \in M : \exists a \in D - \{0\} \suchthat am = 0\}$ \retTwo\par}

\begin{myIndent}\pracTwo
	In other words, $m \in \Tor(M)$ if $\Ann_D(m) \neq \{0\}$.\retTwo
\end{myIndent}

\exTwo\ul{Lemma:} $\Tor(M)$ is a submodule of $M$. (note this requires $D$ to be a domain\dots)
\begin{myIndent}\exThreeP
	Proof:\\
	Suppose $m_1, m_2 \in \Tor(M)$ and $a_1, a_2 \in D$. We know there exists $c_1, c_2 \in D - \{0\}$ such that $c_1 m_1 = c_2 m_2 = 0$. Also as $D$ is a domain, we know that $c_1 c_2 \neq 0$. That said:

	{\centering $c_1c_2(a_1m_1 + a_2m_2) = c_2a_1(c_1m_1) + c_1a_2(c_2m_2) = 0$ \retTwo\par}

	Therefore $a_1m_1 + a_2m_2 \in \Tor(M)$. $\blacksquare$\retTwo
\end{myIndent}

\hTwo\ul{Remark:} If $\theta: M_1 \to M_2$ is a $D$-module isomorphism, then $\theta(\Tor(M_1)) = \Tor(M_2)$.
\begin{myIndent}\pracTwo
	Why:\\
	Suppose $\exists c \in D - \{0\}$ such that $cm = 0$. Then $c\theta(m) = \theta(cm) = \theta(0) = 0$. Hence, $\theta(m) \subseteq \Tor(M_2)$\retTwo

	This proves that $\theta(\Tor(M_1)) \subseteq \Tor(M_2)$. By symmetric reasoning, we can see that $\theta^{-1}(\Tor(M_2)) \subseteq \Tor(M_1)$. By applying $\theta$ to both sides, we get $\Tor(M_2) \subseteq \theta(\Tor(M_1))$. So, we can conclude that $\theta(\Tor(M_1)) = \Tor(M_2)$.\retTwo
\end{myIndent}

Consequently, if $\theta: M_1 \to M_2$ is a $D$-module isomorphism then $\Tor(M_1) \cong \Tor(M_2)$.\newpage

\exTwo\ul{Lemma:} $\Tor(M(r; a_1, \ldots, a_m)) = \frac{D}{\langle a_1\rangle} \oplus \cdots \oplus \frac{D}{\langle a_m\rangle}$ (where the latter is an internal direct sum).
\begin{myIndent}\exThreeP
	\begin{myIndent}\myComment
		Note that we also write $\frac{D}{\langle a_1\rangle} \oplus \cdots \oplus \frac{D}{\langle a_m\rangle} = M(0, a_1, \ldots, a_m)$.\retTwo
	\end{myIndent}

	Proof:\\
	Note that $a_1\cdots a_m \cdot x = 0$ for any $x \in M(0, a_1, \ldots, a_m)$. Hence, the $\supseteq$ inclusion is clear.\retTwo

	On the other hand, suppose that $(v, x) \in \Tor(M)$ where $v \in D^r$ and ${x \in M(0, a_1, \ldots, a_m)}$. Then there exists $c \in D - \{0\}$ such that $(cv, cx) = 0$. But $cv = 0 \Longrightarrow v = 0$ as $c \neq 0$. So, $(v, x) \in M(0, a_1, \ldots, a_m)$. $\blacksquare$\retTwo
\end{myIndent}

\hTwo\mySepTwo 

If $A$ is a unital commutative ring, we say $\mathfrak{a}_1, \ldots, \mathfrak{a}_m \lhd A$ are \udefine{coprime} if $\mathfrak{a}_i + \mathfrak{a}_j = A$\\ whenever $i \neq j$.
\begin{myIndent}\pracTwo
	As a side note, suppose $D$ is a P.I.D and let $a, b \in D$. Also let $d = \gcd(a, b)$ (see \inLinkRap{Math 200b Definition of GCD}{page 519}). Then we claim that $\langle a\rangle + \langle b\rangle = \langle a, b\rangle$ is precisely equal to $\langle d\rangle$.\retTwo

	To see why, note that because $d \divides a$ and $d \divides b$, we know that $\langle a, b\rangle \subseteq \langle d\rangle$. On the other hand, as $D$ is a P.I.D. we know that $\langle a, b\rangle = \langle c \rangle$ for some $c \in D$. But now as $c \divides a$ and $c \divides b$, we must have that $c \divides d$. Therefore, $\langle d \rangle \subseteq \langle c \rangle = \langle a, b\rangle$.\retTwo

	As a consequence, $\langle a \rangle + \langle b \rangle = A$ iff $\gcd(a, b) \in A^\times$. So this new definition of coprimeness purely generalizes the original definition.\retTwo
\end{myIndent}

\exTwo\ul{Generalized Chinese Remainder Theorem:} Suppose $A$ is a unital commutative ring and $\mathfrak{a}_1, \ldots, \mathfrak{a}_m \lhd A$ are coprime. Then $\frac{A}{\mathfrak{a}_1 \cap \cdots \cap \mathfrak{a}_m} \cong \frac{A}{\mathfrak{a}_1} \oplus \cdots \oplus \frac{A}{\mathfrak{a}_m}$ via the $A$-algebra isomorphism $x + (\mathfrak{a}_1 \cap \cdots \cap \mathfrak{a}_m) \mapsto (x + \mathfrak{a}_1, \ldots, x + \mathfrak{a_m})$

\begin{myIndent}\exThreeP
	Proof:\\
	\ul{Step 1:} For all $i$, we have that $\mathfrak{a}_i + \bigcap_{j \neq i}\mathfrak{a}_j = A$.
	\begin{myIndent}\exPPP
		Proof:\\
		For all $j \neq i$, there exists $x_j \in \mathfrak{a}_j$ such that $x_j \equiv 1 \mMod{\mathfrak{a}_i}$. Then, $y \coloneqq \prod_{j \neq i} x_j$ is in $\bigcap_{j \neq i}\mathfrak{a}_j$ and satisfies that $y \equiv 1 \mMod{\mathfrak{a}_i}$. It follows that there exists $x_i \in \mathfrak{a}_i$ with $y + x = 1$. And that shows that $\mathfrak{a}_i + \bigcap_{j \neq i}\mathfrak{a}_j = A$.\retTwo
	\end{myIndent}

	\ul{Step 2:} $\theta : A \to \bigoplus_{i=1}^m \frac{A}{\mathfrak{a}_i}$ given by $\theta(x) = (x + \mathfrak{a}_1, \ldots, x + \mathfrak{a}_m)$ is a surjective \udefine{$A$-algebra homomorphism} (meaning it is an $A$-module homomorphism and a ring homomorphism). 

	\begin{myIndent}\myComment
		As a side note, we can view $\bigoplus_{i = 1}^m\frac{A}{\mathfrak{a}_i}$ as a ring by identifying it with the product ring $\prod_{i=1}^m \frac{A}{\mathfrak{a}_i}$. In general, an \udefine{$A$-algebra} $M$ is an $A$-module such that $M$ is also a ring and $a(m_1m_2) = (am_1)m_2 = m_1(am_2)$ for all $a \in A$ and $m_1, m_2 \in M$.\retTwo
	\end{myIndent}

	\begin{myIndent}\exPPP
		Proof:\\
		The fact that $\theta$ is a homomorphism is obvious. What is less obvious is that $\theta$ is surjective. To prove this, it is necessary and sufficient to show that:
		
		{\centering$(0, \ldots, 0, 1 + \mathfrak{a}_i, 0, \ldots, 0) \in \myIm(\theta)$ for all $i$.\retTwo\par}

		Fortunately by step 1, we know there exists $y_i \in \bigcap_{j \neq i} \mathfrak{a_j}$ such that $y_i \equiv 1 \mMod{\mathfrak{a}_i}$. In turn $\theta(y_i) = (0, \ldots, 0, 1 + \mathfrak{a}_i, 0, \ldots, 0) \in \myIm(\theta)$.\newpage
	\end{myIndent}

	\ul{Step 3:} $\ker(\theta) = \bigcap_{i=1}^m \mathfrak{a}_i$.
	\begin{myIndent}\exPPP
		Hopefully this is obvious.\retTwo
	\end{myIndent}

	To finish, just use the first isomorphism theorem for rings as well as the first isomorphism theorem for $A$-modules. $\blacksquare$\retTwo
\end{myIndent}

\hTwo\mySepTwo

\exTwo\ul{Corollary:} If $M$ is a finitely generated $D$-module and $D$ is a PID, then $M$ is torsion-free (meaning $\Tor(M) = \{0\}$) if and only if $M$ is free.

\begin{myIndent}\exThree
	$(\Longleftarrow)$\\
	If $M$ is free then we know that $M \cong D^r$ for some positive integer $r$. But $\Tor(D^r) = \{0\}$ since $D$ is a domain. Therefore, $\Tor(M) = \{0\}$ as well.\retTwo

	$(\Longrightarrow)$\\
	Using the fundamental theorem of finitely generated modules over a PID, write\\ $M \cong D^r \oplus M(0, a_1, \ldots, a_m)$. Then $\{0\} = \Tor(M) = M(0, a_1 \ldots, a_m)$ implies that $M \cong D^r$. So, $M$ is free. $\blacksquare$\retTwo
\end{myIndent}

\hTwo Other remarks:
\begin{itemize}
	\item I used multiple times throughout the proof on \inLinkRap{Math 200b page 587-591 theorem}{pages 588-591} that if $D$ is an integral domain, then every ideal of $D$ is torsion free. 
	
	\item Note that if $D$ is not a P.I.D then there exists an ideal $\mathfrak{a} \lhd D$ that is not principle. In turn, $\mathfrak{a}$ is not free.
	\begin{myIndent}\pracTwo
		To see why, first note that $\mRank(\mathfrak{a}) = 1$. After all, we clearly have that $\{a\}$ is $D$-linearly independent for any $a \in \mathfrak{a} - \{0\}$. Meanwhile suppose $b$ is another element in $\mathfrak{a}$. Then, $b(a) + (-a)b = 0$ but $-a \neq 0$. So, $\{a, b\}$ is not $D$-linearly independent.\retTwo

		As isomorphisms preserve the rank of modules and $\mRank(D^n) = n$ for all integers $n$, we must have that if $\mathfrak{a}$ were free then we'd have that $\mathfrak{a} \cong D$ via a $D$-module homomorphism $\theta : D \to \mathfrak{a}$. But now we'd have a contradiction as $\mathfrak{a} = \langle \theta(1)\rangle$ is principal.\retTwo
	\end{myIndent}

	\item By combining these remarks, we have shown that the $(\Longrightarrow)$ implication of the above corollary is false if $D$ is merely a domain instead of a P.I.D.
\end{itemize}

\mySepTwo 

Let $D$ be a P.I.D. and let $\mathcal{P}_0 \subseteq D$ contain precisely one element from each equivalence class of companions containing irreducible elements (see \inLinkRap{Math 200b Page 518 reference}{page 518} and recall that all P.I.Ds are U.F.Ds).\retTwo

Note that because $D$ is a U.F.D., $\gcd(a, b) = 1$, then $\langle ab\rangle = \langle a \rangle \cap \langle b\rangle$.
\begin{myIndent}\pracTwo
	After all, the $\subseteq$ inclusion is trivial. Meanwhile, if $a \divides x$ and $b \divides x$, then because\\ $\gcd(a, b) = 1$, the only way to not violate the unique factorization of $x$ is if $ab \divides x$.\\ This shows that $\langle a \rangle \cap \langle b\rangle \subseteq \langle ab\rangle$.\newpage
\end{myIndent}

By induction we can conclude that $\langle a \rangle = \bigcap_{p \in \mathcal{P}_0}\langle p^{\nu_p(a)}\rangle$. And since $D$ is a P.I.D., we know from two pages ago that all the ideals $\langle p^{\nu_p(a)}\rangle$ are coprime. Hence by the generalized\\ [2pt] Chinese remainder theorem, we have that:

{\centering $\frac{D}{\langle a\rangle} = \frac{D}{\bigcap_{p \in \mathcal{P}_0}\langle p^{\nu_p(a)}\rangle} \cong \bigoplus_{p \in \mathcal{P}_0} \frac{D}{\langle p^{\nu_p(a)}\rangle}$.\retTwo\par}


Now at last we are going to prove the uniqueness part of the theorem on \inLinkRap{Fundamental Theorem of finitely generated modules over a P.I.D.}{page 591}.\retTwo

\exTwo\ul{Theorem:} If $M(r; a_1, \ldots, a_m) \cong M(r^\prime, b_1, \ldots, b_\ell)$, then $r = r^\prime$, $m = \ell$, and $\langle a_i \rangle = \langle b_i \rangle$  for all $i$.

\begin{myIndent}\exThreeP
	Proof:\\
	To start out, we know that $r = \mRank(M(r; a_1, \ldots, a_m)) = \mRank(M(r^\prime, b_1, \ldots, b_\ell)) = r^\prime$. Also, we know that:

	{\centering\begin{tabular}{l}
		$M(0; a_1, \ldots, a_m) = \Tor(M(r; a_1, \ldots, a_m))$\\ [4pt]
		$\phantom{M(0; a_1, \ldots, a_m)}\cong \Tor(M(r^\prime, b_1, \ldots, b_\ell)) = M(0; b_1, \ldots, b_\ell)$.
	\end{tabular}\retTwo\par}

	Hence, it now suffices to show that if $M(0; a_1, \ldots, a_m) \cong M(0; b_1, \ldots, b_\ell)$ then $m = \ell$ and $\langle a_i \rangle = \langle b_i \rangle$  for all $i$.\retTwo

	\ul{Part 1: Building a Counting Machine}\\
	Consider that:

	{\centering\begin{tabular}{l}
		$M \coloneqq M(0; c_1, \ldots, c_m) = \bigoplus\limits_{i=1}^m \frac{D}{\langle c_i\rangle} \cong \bigoplus\limits_{i=1}^m \left(\bigoplus\limits_{p \in \mathcal{P}_0}\frac{D}{\langle p^{\nu_p(c_i)}\rangle}\right) \cong \bigoplus\limits_{p \in \mathcal{P}_0} \left(\bigoplus\limits_{i=1}^m\frac{D}{\langle p^{\nu_p(c_i)}\rangle}\right)$
	\end{tabular}\retTwo\par}

	We shall denote $M(c_1, \ldots, c_m; p) \coloneqq \bigoplus_{i=1}^m\frac{D}{\langle p^{\nu_p(c_i)}\rangle}$.\retTwo
	
	Importantly note that as $c_1 \divides c_2 \divides \cdots \divides c_m$, we know that $\nu_p(c_1) \leq \cdots \leq \nu_p(c_m)$ for all $p \in \mathcal{P}_0$. Since $M(c_1, \ldots, c_m ; p)$ has that additional structure, we'll try studying it.
	\begin{itemize}\exPPP
		\item Note that if $A$ is a commutative ring, then for any $A$-module $N$ and $b \in A$ we have that $\ell_b(m) \coloneqq bm$ is an $A$-module homomorphism from $N$ to itself. It follows that $b \cdot M = \myIm(\ell_b)$ is an $A$-module.\retTwo
		
		\item If $\{M_i\}_{i \in I}$ is a family of $A$-modules and $b \in A$, then $b \cdot \bigoplus\limits_{i \in I} M_i = \bigoplus\limits_{i \in I}(b \cdot M_i)$.\retTwo
		
		\item Suppose $\theta: M_1 \to M_2$ is an $A$-module isomorphism. Then $\theta(b \cdot M_1) = b \cdot M_2$. 
		\begin{myIndent}\color{Orange}
			Hopefully this is easy to see.\retTwo
		\end{myIndent}
		
		\item In particular, if $D$ is a P.I.D. and $a \in D$, then we have that
		
		{\centering $ b \cdot \frac{D}{\langle a \rangle} = \frac{bD + \langle a \rangle}{\langle a\rangle} = \frac{\langle b, a \rangle}{\langle a\rangle} = \frac{\langle \gcd(a, b)\rangle}{\langle a\rangle} $\retTwo\par}

		Consequently, if $p_0, p$ are prime elements and $r, k$ are nonnegative integers, then:

		{\centering $p_0^{r} \cdot \frac{D}{\langle p^k\rangle} = \left\{ \begin{matrix}
		\frac{D}{\langle p^k\rangle} & \text{ if } p_0 \neq p \\ \\ \frac{\langle p^{\min(r, k)}\rangle}{\langle p^{k}\rangle} & \text{ if } p_0 = p
		\end{matrix}\right.$ \newpage\par}

		\item $\frac{\langle\gcd(a, b)\rangle}{\langle a\rangle} \cong \frac{D}{\langle \frac{a}{\gcd(a, b)}\rangle}$ as $D$-modules.
		\begin{myIndent}\color{Orange}
			Proof:\\
			Let $d = \gcd(a, b)$ and define $\widehat{\theta} : D \to \frac{\langle d\rangle}{\langle a\rangle}$ by $\widehat{\theta}(x) \coloneqq dx + \langle a \rangle$. Then $\widehat{\theta}$ is a surjective $D$-module homomorphism.\\ [-9pt]
			
			Also, $x \in \ker(\widehat{\theta})$ iff $dx \in \langle a \rangle$, and that happens iff $dx = ay$ for some $y \in D$. But now as $d \divides a$ and $D$ is an integral domain, we can say that $x = \frac{a}{d} y$ for some $y \in D$.\\ [-9pt]
			
			Hence, $\ker(\widehat{\theta}) = \langle \frac{a}{d}\rangle$ and we finish by invoking the first isomorphism theorem. $\blacksquare$\retTwo
		\end{myIndent}
	\end{itemize}

	Now for all integers $r \geq 0$ and $p_0 \in \mathcal{P}_0$, we have that:

	{\center\begin{tabular}{l}
		$p_0^r \cdot M \cong \bigoplus_{p \in \mathcal{P}_0} p_0^r \cdot M(c_1, \ldots, c_m ; p)$\\ [8pt]
		$\phantom{p_0^r \cdot M} = \bigoplus_{p \in \mathcal{P}_0}  \left(\bigoplus_{i=1}^m p_0^r \cdot \frac{D}{\langle p^{\nu_p(c_i)}\rangle}\right)$\\ [8pt]
		$\phantom{p_0^r \cdot M} \cong \left(\bigoplus_{p \in \mathcal{P}_0 - \{p_0\}}  \left(\bigoplus_{i=1}^m \frac{D}{\langle p^{\nu_p(c_i)}\rangle}\right)\right) \oplus \bigoplus_{i=1}^m \frac{\langle {p_0}^{\min(r, \nu_{p_0}(c_i))}\rangle}{\langle {p_0}^{\nu_{p_0}(c_i)}\rangle}$
	\end{tabular}\retTwo\par}

	In order to cancel out the first ugly direct sum, we do an algebraic trick (which is referred to as taking a \udefine{graded filtration}). Consider the submodules:

	{\centering $M \supseteq p_0 \cdot M \supseteq p_0^2 \cdot M \supseteq \cdots$ \retTwo\par}

	Now by recalling the lemma on \inLinkRap{Fundamental Theorem of finitely generated modules over a P.I.D.}{page 591}, we get that:

	{\centering$\frac{p_0^r \cdot M}{p_0^{r+1} \cdot M} \cong \frac{\bigoplus_{i=1}^m \frac{\langle {p_0}^{\min(r, \nu_{p_0}(c_i))}\rangle}{\langle {p_0}^{\nu_{p_0}(c_i)}\rangle}}{\bigoplus_{i=1}^m \frac{\langle {p_0}^{\min(r + 1, \nu_{p_0}(c_i))}\rangle}{\langle p_0^{\nu_{p_0}(c_i)}\rangle}} \cong \bigoplus_{i=1}^m \frac{\left(\frac{\langle p_0^{\min(r, \nu_{p_0}(c_i))}\rangle}{\langle {p_0}^{\nu_{p_0}(c_i)}\rangle}\right)}{\left(\frac{\langle p_0^{\min(r + 1, \nu_{p_0}(c_i))}\rangle}{\langle p_0^{\nu_{p_0}(c_i)}\rangle}\right)}$.\retTwo\par}

	\ul{Lemma:} Suppose $M \supseteq N \supseteq L$ are $D$-modules. Then $(M/L)/(N/L) \cong (M/N)$
	\begin{myIndent}\exPPP
		Proof:\\
		We already know from the third isomorphism theorem for groups that\\ $(x + L) + N/L \mapsto  x + N$ is a group isomorphism. Then it's trivial to further\\ check that this group isomorphism is also $A$-module homomorphism.\retTwo
	\end{myIndent}

	So, $\frac{p_0^r \cdot M}{p_0^{r+1} \cdot M} \cong \bigoplus_{i=1}^m \frac{\langle {p_0}^{\min(r, \nu_{p_0}(c_i))}\rangle}{\langle {p_0}^{\min(r + 1, \nu_{p_0}(c_i))}\rangle}$.\retTwo

	But now consider that $\frac{\langle p_0^{\min(r, \nu_{p_0}(c_i))}\rangle}{\langle {p_0}^{\min(r + 1, \nu_{p_0}(c_i))}\rangle} \cong \left\{\begin{matrix}
	\{0\} & \text{ if } \nu_{p_0}(c_i) \leq r \\ \\ \frac{D}{\langle p_0 \rangle} & \text{ if } \nu_{p_0}(c_i) > r
	\end{matrix}\right.$\retTwo

	Therefore, we have that $\frac{p_0^r \cdot 
	M}{p_0^{r+1} \cdot M} \cong \left(\frac{D}{\langle p_0\rangle}\right)^{\#\{i\phantom{.} :\phantom{.} \nu_{p_0}(c_i) > r\}}$ {\exPPP(where $\#S$ denotes the\\ [-5pt]\phantom{aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa} cardinality of the set $S$)}.\retTwo

	\ul{Lemma:} If $M$ is an $A$-module, $\mathfrak{b} \lhd A$, and $\theta: M \to (A/\mathfrak{b})^k$ is an $A$-module isomorphism, then we can view $M$ as an $(A/\mathfrak{b})$-module by defining $(a + \mathfrak{b})m = am$. Furthermore, $\theta$\\ is then an $(A/\mathfrak{b})$-module isomorphism when $M$ is equipped with this multiplication\\ operation.\newpage
	\begin{myIndent}\exPPP
		Proof:\\
		Suppose $a_1 \equiv a_2 \mMod{\mathfrak{b}}$. Then $a_1 - a_2 \equiv 0 \mMod{\mathfrak{b}}$. In turn, for any $m \in M$ we must have that
		
		{\centering$\theta((a_1 - a_2)m) = (a_1 - a_2)\theta(m) = 0$\retTwo\par}

		And because $\theta$ is an isomorphism, that implies that $(a_1 - a_2)m = 0$. So, $a_1m = a_2m$ and we've proven that our scalar multiplication operation is well-defined. All the needed properties of this scalar multiplication are now easily seen as being inherited from the old scalar multiplication.\retTwo
	\end{myIndent}

	At last we get to the key observation. Note that as $D$ is a P.I.D. and $p_0$ is irreducible, we\\ [1pt] know that $\frac{D}{\langle p_0\rangle}$ is a field. Hence, our prior lemma says that $\frac{p_0^r \cdot M}{p_0^{r+1} \cdot M}$ is a $\frac{D}{\langle p_0\rangle}$-vector space\\ [-0pt] whose dimension is precisely the number of $c_i$ for which $p_0^r \divides c_i$.\retTwo

	\mySepThree 

	\ul{Part 2: Counting the irreducible factors of $a_1, \ldots, a_m$ and $b_1, \ldots, b_\ell$}\\
	Let $N_1 \coloneqq M(0; a_1, \ldots, a_m)$ and $N_2 \coloneqq M(0; b_1, \ldots, b_\ell)$, and suppose $N_1 \cong N_2$.\retTwo
	
	\ul{Remark:} If $M_1, M_2$ are $A$-modules, $N \subseteq M_1$ is a submodule, and $\theta: M_1 \to M_2$ is an $A$-module isomorphism, then $\frac{M_1}{N} \cong \frac{M_2}{\theta(N)}$.
	\begin{myIndent}\exPPP
		Proof:\\
		Let $\pi: M_2 \to M_2/\theta(N)$ be the $A$-module homomorphism $m \mapsto m + \theta(N)$. Then, consider the map $\phi \coloneqq \pi \circ \theta$. Clearly $\phi$ is surjective since both $\theta$ and $\pi$ are. Also, $x \in \ker(\phi)$ iff $\theta(x) \in \theta(N)$. And since $\theta$ is injective, that happens iff $x \in N$. To finish off, we invoke the first isomorphism theorem.\retTwo
	\end{myIndent}

	As a result of the above remark plus an earlier remark that if $\theta: M_1 \to M_2$ is an $A$-module isomorphism then $\theta(b \cdot M_1) = b \cdot M_2$, we can now conclude for all for all $p \in \mathcal{P}_0$ and $r \in \mathbb{Z}_{\geq 0}$, we have that:

	{\centering $\left(\frac{D}{\langle p\rangle}\right)^{\#\{i\phantom{.} :\phantom{.} \nu_{p}(a_i) > r\}} \cong \frac{p^r \cdot N_1}{p^{r+1} \cdot N_1} \cong \frac{p^r \cdot N_2}{p^{r+1} \cdot N_2} \cong \left(\frac{D}{\langle p\rangle}\right)^{\#\{i\phantom{.} :\phantom{.} \nu_{p}(b_i) > r\}}$ \retTwo\par}

	Since vector space isomorphisms preserve dimension, we can conclude that:

	{\centering $\#\{i\phantom{.} :\phantom{.} \nu_{p}(a_i) > r\} = \#\{i\phantom{.} :\phantom{.} \nu_{p}(b_i) > r\}$ for all $p \in \mathcal{P}_0$ and $r \in \mathbb{Z}_{\geq 0}$ \retTwo\par}

	This let's us show that $m = \ell$ and that $\langle a_i\rangle = \langle b_i\rangle$ for all $i$.
	\begin{myIndent}\exPPP
		To start off, let $M = \max\{ \#\{i\phantom{.} :\phantom{.} \nu_{p}(a_i) > 0\} : p \in \mathcal{P}_0\}$.\retTwo
		
		It's clear that $M \leq m$. Meanwhile, because $a_1 \notin A^\times$, there must exists some irreducible element $p_0$ such that $p_0 \divides a_1$. In turn, as $a_1 \divides a_2 \divides \cdots \divides a_m$, we have that $\#\{i\phantom{.} :\phantom{.} \nu_{p_0}(a_i) > 0\} = m$. Hence, we've proven that $M = m$.\retTwo

		By similar reasoning with the quantity $\max\{ \#\{i\phantom{.} :\phantom{.} \nu_{p}(b_i) > 0\} : p \in \mathcal{P}_0\}$, we can thus conclude that:
		
		{\centering\fontsize{11}{11} $m = \max\{ \#\{i\phantom{.} :\phantom{.} \nu_{p}(a_i) > 0\} : p \in \mathcal{P}_0\} = \max\{ \#\{i\phantom{.} :\phantom{.} \nu_{p}(b_i) > 0\} : p \in \mathcal{P}_0\} = \ell$ \retTwo\par}

		Next, for any fixed $p \in \mathcal{P}_0$ consider the Young diagram such that the number of boxes in the $(r + 1)$th column is equal to $\#\{i\phantom{.} :\phantom{.} \nu_{p}(a_i) > r\} = \#\{i\phantom{.} :\phantom{.} \nu_{p}(b_i) > r\}$.\newpage Then the number of boxes in the $k$th row is precisely equal to $\nu_p(a_{-k + m + 1})$ and $\nu_p(b_{-k + m + 1})$. Below I've attached a sample diagram to show what I mean.

		{\centering\includegraphics[scale=0.9]{math 200b awful obnoxious uniqueness proof.png}\retTwo\par}

		This proves that $\nu_p(a_i) = \nu_p(b_i)$ for all $p \in \mathcal{P}_0$ and $i \in \{1, \ldots, m\}$. The only way this is possible is if $a_i, b_i$ are companions for each $i$. \exThreeP $\blacksquare$\retTwo
	\end{myIndent}
\end{myIndent}

\hTwo\mySepTwo

Suppose $M$ is an $F[x]$-module where $F$ is a field. Then $\ell_x : M \to M$ defined by\\ $\ell_x(m) = xm$ is an $F$-linear map. Also if $f(x) = c_nx^n + \ldots + c_1x + c_0 \in F[x]$,\\ then $f(x) \cdot m = \sum_{i=0}^n c_i \ell_x^i(m)$. Hence, the $F$-linear map $\ell_x$ uniquely determines\\ the $F[x]$-module structure of $M$.\retTwo

In a similar vein, if $M_1, M_2$ are $F[x]$-modules then $\theta: M_1 \to M_2$ is an $F[x]$-module\\ homomorphism if and only if $\theta$ is an $F$-linear map, and $\theta(x \cdot m_1) = x \cdot \theta(m_1)$. To put another $\theta$ is an $F[x]$-module homomorphism iff if the following commutative diagram of $F$-module homomorphisms holds:
% https://q.uiver.app/#q=WzAsNCxbMCwwLCJNXzEiXSxbMSwwLCJNXzIiXSxbMSwxLCJNXzIiXSxbMCwxLCJNXzEiXSxbMCwxLCJcXHRoZXRhIl0sWzMsMiwiXFx0aGV0YSJdLFswLDMsIlxcZWxsX3giLDJdLFsxLDIsIlxcZWxsX3giXV0=
\[\begin{tikzcd}
	{M_1} & {M_2} \\
	{M_1} & {M_2}
	\arrow["\theta", from=1-1, to=1-2]
	\arrow["{\ell_x}"', from=1-1, to=2-1]
	\arrow["{\ell_x}", from=1-2, to=2-2]
	\arrow["\theta", from=2-1, to=2-2]
\end{tikzcd}\]

\begin{myIndent}\pracTwo
	$(\Longrightarrow)$\\
	This is obvious.\retTwo

	$(\Longleftarrow)$\\
	Suppose $f(x) = c_nx^n + \ldots + c_1x + c_0$. Then:
	
	{\centering$\theta(f(x) \cdot m) = \theta(\sum\limits_{i=0}^n c_i \ell_x^i(m)) = \sum\limits_{i=0}^n c_i \theta(\ell_x^i(m)) = \sum\limits_{i=0}^n c_i \ell_x^i(\theta(m)) = f(x) \cdot \theta(m)$.\retTwo\par}
\end{myIndent}

We're going to use this to study linear algebra. Suppose $F$ is a field and $a \in M_n(F)$. Then we are interested in the map $a : F^n \to F^n$ given by $| v {\rangle} \mapsto a\phantom{.} | v {\rangle}$. So, we define the $F[x]$-\newpage module $V_a$ by taking the $F$-vector space $F^n$ and defining $\ell_x(v) = a\phantom{.} |v {\rangle}$. Hence:

{\centering$(c_nx^n + \cdots + c_1x + c_0) \cdot v = (c_n a^n + \cdots + c_1 a + c_0 I) | v {\rangle}$\retTwo\par}

\exTwo\ul{Lemma:} $V_a \cong V_b$ as $F[x]$-modules if and only if $a$ and $b$ are similar matrices.
\begin{myIndent}\exThreeP
	Proof:\\
	$V_a \cong V_b$ as $F[x]$-modules iff there exists an $F[x]$ module isomorphism $\theta: V_a \to V_b$. But that happens iff there exists a bijective $F$-linear map $\theta: V_a \to V_b$ such that the below diagram commutes:

	% https://q.uiver.app/#q=WzAsOCxbMSwwLCJWX2EiXSxbMCwwLCJGXm4iXSxbMCwxLCJGXm4iXSxbMSwxLCJWX2EiXSxbMiwwLCJWX2IiXSxbMiwxLCJWX2IiXSxbMywwLCJGXm4iXSxbMywxLCJGXm4iXSxbMCwxLCIiLDAseyJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzAsMSwiIiwyLHsib2Zmc2V0IjoxLCJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzMsMiwiIiwyLHsib2Zmc2V0IjoxLCJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzMsMiwiIiwwLHsic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFswLDMsIlxcZWxsX3giXSxbMSwyLCJhIl0sWzAsNCwiXFx0aGV0YSJdLFszLDUsIlxcdGhldGEiXSxbNCw1LCJcXGVsbF94Il0sWzYsNywiYiJdLFs0LDYsIiIsMCx7Im9mZnNldCI6LTEsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbNCw2LCIiLDAseyJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzUsNywiIiwwLHsic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFs1LDcsIiIsMCx7Im9mZnNldCI6LTEsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XV0=
	\[\begin{tikzcd}
	{F^n} & {V_a} & {V_b} & {F^n} \\
	{F^n} & {V_a} & {V_b} & {F^n}
	\arrow["a", from=1-1, to=2-1]
	\arrow[no head, from=1-2, to=1-1]
	\arrow[shift right, no head, from=1-2, to=1-1]
	\arrow["\theta", from=1-2, to=1-3]
	\arrow["{\ell_x}", from=1-2, to=2-2]
	\arrow[shift left, no head, from=1-3, to=1-4]
	\arrow[no head, from=1-3, to=1-4]
	\arrow["{\ell_x}", from=1-3, to=2-3]
	\arrow["b", from=1-4, to=2-4]
	\arrow[shift right, no head, from=2-2, to=2-1]
	\arrow[no head, from=2-2, to=2-1]
	\arrow["\theta", from=2-2, to=2-3]
	\arrow[no head, from=2-3, to=2-4]
	\arrow[shift left, no head, from=2-3, to=2-4]
	\end{tikzcd}\]

	\begin{myIndent}\exPPP
		As a side note, if $a \in M_{n_1}(F)$, $b \in M_{n_2}(F)$, and $n_1 \neq n_2$, then there doesn't exists an $F$-linear bijection $\theta: F^{n_1} \to F^{n_2}$. Hence, we lose nothing by assuming both $V_a$ and $V_b$ are equal as sets to $F^n$.\retTwo
	\end{myIndent}

	Equivalently, we can say there is some $g \in \GL_n(F)$ with $\theta(v) = g\phantom{.} | v {\rangle}$ such that the below diagram commutes:

	% https://q.uiver.app/#q=WzAsNCxbMCwwLCJGXm4iXSxbMSwwLCJGXm4iXSxbMSwxLCJGXm4iXSxbMCwxLCJGXm4iXSxbMCwxLCJnIl0sWzMsMiwiZyJdLFswLDMsImEiXSxbMSwyLCJiIl1d
	\[\begin{tikzcd}
		{F^n} & {F^n} \\
		{F^n} & {F^n}
		\arrow["g", from=1-1, to=1-2]
		\arrow["a", from=1-1, to=2-1]
		\arrow["b", from=1-2, to=2-2]
		\arrow["g", from=2-1, to=2-2]
	\end{tikzcd}\]

	In other words, there exists an invertible matrix $g$ such that $ga = bg$. And finally, we get that $b = gag^{-1}$ for some $g \in \GL_n(F)$. Hence, $a$ and $b$ are similar matrices. $\blacksquare$\retTwo
\end{myIndent}

\hTwo Suppose $a \in M_{n_1}(F)$ and $b \in M_{n_2}(F)$. Then if we consider the direct sum $V_a \oplus V_b$ note that for any $(v, w) \in V_a \oplus V_b$ we have that:

{\centering$x \cdot (v, w) = (a\phantom{.} | v{\rangle}, b\phantom{.} | v{\rangle}) = \begin{bmatrix}a & \bm{0} \\ \bm{0} & b\end{bmatrix}\begin{bmatrix}v \\ w\end{bmatrix}  $\retTwo\par}

In particular, if we denote the block matrix $\begin{bmatrix}a & \bm{0} \\ \bm{0} & b\end{bmatrix}$ as $\diag(a, b)$, then we've shown that\\ [-4pt] $V_a \oplus V_b \cong V_{\diag(a, b)}$.\retTwo

Hopefully it is clear how this generalizes to larger finite direct sums, and how one would define a block matrix $\diag(a_1, \ldots, a_n)$.

\mySepTwo

Given a field $F$, we know that $F[x]$ is a P.I.D. Also if $a \in M_n(F)$ then we trivially have that $V_a$ is finitely generated (because the standard basis for $F^n$ will always generate all of $V_a$).
Therefore, by the \inLinkRap{Fundamental Theorem of finitely generated modules over a P.I.D.}{fundamental theorem of finitely generated modules over a P.I.D}, we know that there exists a unique $r \in \mathbb{Z}_{\geq 0}$ as well as unique ideals $\langle f_1 \rangle \supseteq \langle f_2\rangle \supseteq \cdots \supseteq \langle f_m\rangle$ in $F[x]$ such that:

{\centering $V_a \cong (F[x])^r \oplus \frac{F[x]}{\langle f_1\rangle} \oplus \cdots \oplus \frac{F[x]}{\langle f_m\rangle}$ as $F[x]$-modules. \newpage\par}

Equivalently, there exists unique monic polynomials $f_1 \divides f_2 \divides \cdots \divides f_m$ in $F[x]$ such that:

{\centering $V_a \cong (F[x])^r \oplus \frac{F[x]}{\langle f_1\rangle} \oplus \cdots \oplus \frac{F[x]}{\langle f_m\rangle}$ as $F[x]$-modules.\retTwo\par}

But now we can immediately deduce that $r = 0$.
\begin{myIndent}\pracTwo
	To see why compare the dimensions of both sides of the above equation as $F$-vector spaces. $V_a$ would be an $n$-dimensional $F$-vector but $F[x]^r$ would be an infinite-dimensional $F$-vector space for any $r > 0$.\retTwo
\end{myIndent}

So, we can further refine our conclusion to the following:
\begin{myIndent}\exTwo
	\ul{Corollary:} Suppose $F$ is a field and $a \in M_n(F)$. There exists unique monic positive degree polynomials $f_1 \divides f_2 \divides \cdots \divides f_m$ such that $V_a \cong \frac{F[x]}{\langle f_1\rangle} \oplus \cdots \oplus \frac{F[x]}{\langle f_m\rangle}$ as $F[x]$-modules.\retTwo
\end{myIndent}

We call the polynomials $f_1, \ldots, f_m$ above the \udefine{invariant factors} of the matrix $a$.\retTwo

\exTwo\ul{Corollary:} If $F$ is a field and $a, b \in M_n(F)$, then $a$ is similar to $b$ iff $a$ and $b$ have the same invariant factors.
\begin{myIndent}\exThreeP
	Proof:\\
	$a$ is similar to $b$ iff $V_a \cong V_b$ as $F[x]$-modules. But by the \inLinkRap{Fundamental Theorem of finitely generated modules over a P.I.D.}{fundamental theorem of finitely generated modules over a P.I.D}, the latter statement is equivalent to both $a$ and $b$ having the same invariant factors. $\blacksquare$\retTwo
\end{myIndent}

\hTwo\mySepTwo 

Question? Given any $f \in F[x]$, can we find a matrix $a \in M_n(F)$ such that $V_a \cong \frac{F[x]}{\langle f\rangle}$.
\begin{myIndent}\myComment
	A reason to ask this question is that after finding matrices $a_1, \ldots, a_m$ such that $V_{a_i} \cong \frac{F[x]}{\langle f_i\rangle}$ for each $i \in \{1, \ldots, m\}$, we could then conclude that:

	{\centering $V_a \cong \frac{F[x]}{\langle f_1\rangle} \oplus \cdots \oplus \frac{F[x]}{\langle f_m\rangle} \cong V_{a_1} \oplus \cdots \oplus V_{a_m} \cong V_{\diag(a_1, \ldots, a_m)}$ \retTwo\par}
\end{myIndent}

The answer to the above question is yes. To show this, first recall from math 100b and 100c that if $f(x) = c_n x^n + \cdots + c_1 x + c_0$ (where $c_n \neq 0$) then $\frac{F[x]}{\langle f\rangle}$ has the following $F$-basis (where $\overline{x^i}$ is just the equivalence class of $x^i$\hspace{-0.3em} $\mMod{\langle f\rangle}$):\\ [-18pt]

{\center $\mathcalli{B} = (\overline{1}, \overline{x}, \ldots, \overline{x^{n-1}})$\par}

\begin{myIndent}\pracTwo
	To see why, first note that by the long division theorem we can conclude that every\\ equivalence class in $F[x] / \langle f\rangle$ contains a unique polynomial with degree less than $n$. It easily follows that $\mathcalli{B}$ spans all of $F[x] / \langle f\rangle$. Also, as the unique polynomial of degree less than $n$ equivalent to $0 \mMod{\langle f\rangle}$ is $0$, we know that:
	
	{\centering$c_{n-1}\overline{x^{n-1}} + \cdots + c_{1}\overline{x^{1}} + c_0\overline{1} \equiv 0 \mMod{\langle f\rangle}$ iff all $c_i = 0$.\retTwo\par}
\end{myIndent}

Thus, we identify $\frac{F[x]}{\langle f\rangle} \to F^n$ as $F$-vector spaces via the mapping $\overline{g} \mapsto | \overline{g} {\rangle}_{\mathcalli{B}}$ (where $| \overline{g} {\rangle}_{\mathcalli{B}}$ is just the column vector for $\overline{g}$ with respect to the basis $\mathcalli{B}$).

\begin{myIndent}\myComment
	In other words, $a_{m-1}x^{m-1} + \cdots + a_1x + a_0 + \langle f\rangle \mapsto | (a_0, a_1, \ldots, a_{m-1}) {\rangle}$.\newpage
\end{myIndent}

We want to find a matrix $[\ell_x]_{\mathcalli{B}} \in M_n(F)$ such that $[\ell_x]_{\mathcalli{B}} | \overline{g} {\rangle}_{\mathcalli{B}} = |\ell_x(\overline{g}) {\rangle}_{\mathcalli{B}}$. In other words, we want the following diagram to commute:

% https://q.uiver.app/#q=WzAsNCxbMCwwLCJcXGZyYWN7Rlt4XX17XFxsYW5nbGUgZlxccmFuZ2xlfSJdLFsxLDAsIkZebiJdLFsxLDEsIkZebiJdLFswLDEsIlxcZnJhY3tGW3hdfXtcXGxhbmdsZSBmXFxyYW5nbGV9Il0sWzAsMywiXFxlbGxfeCIsMl0sWzEsMiwiW1xcZWxsX3hdX1xcbWF0aGNhbGxpe0J9Il0sWzAsMSwifFxcY2RvdFxccmFuZ2xlX3tcXG1hdGhjYWxsaXtCfX0iXSxbMywyLCJ8XFxjZG90XFxyYW5nbGVfe1xcbWF0aGNhbGxpe0J9fSIsMl1d
\[\begin{tikzcd}
	{\frac{F[x]}{\langle f\rangle}} & {F^n} \\
	{\frac{F[x]}{\langle f\rangle}} & {F^n}
	\arrow["{|\cdot\rangle_{\mathcalli{B}}}", from=1-1, to=1-2]
	\arrow["{\ell_x}"', from=1-1, to=2-1]
	\arrow["{[\ell_x]_\mathcalli{B}}", from=1-2, to=2-2]
	\arrow["{|\cdot\rangle_{\mathcalli{B}}}"', from=2-1, to=2-2]
\end{tikzcd}\]

But this is easy. After all, given any linear map $T: V \to V$ and basis vectors $v_1, \ldots, v_n$,\\ we can always write the matrix of $T$ with respect to that basis as $\begin{bmatrix}T(v_1) & \cdots & T(v_n) \end{bmatrix}$.\\ Hence we must have that:

{\center\begin{tabular}{l}
	$[\ell_x]_{\mathcalli{B}} = \begin{bmatrix} |\ell_x(\overline{1}){\rangle}_{\mathcalli{B}} & |\ell_x(\overline{x}){\rangle}_{\mathcalli{B}} & \cdots & |\ell_x(\overline{x^{n-2}}){\rangle}_{\mathcalli{B}} & |\ell_x(\overline{x^{n-1}}){\rangle}_{\mathcalli{B}} \end{bmatrix}$\\ [10pt]

	$\phantom{[\ell_x]_{\mathcalli{B}}} = \begin{bmatrix} |\overline{x}{\rangle}_{\mathcalli{B}} & |\overline{x^2}{\rangle}_{\mathcalli{B}} & \cdots & |\overline{x^{n-1}}{\rangle}_{\mathcalli{B}} & |\overline{x^{n}}{\rangle}_{\mathcalli{B}} \end{bmatrix}$\\ [10pt]

	$\phantom{[\ell_x]_{\mathcalli{B}}} = \begin{bmatrix} 0 & 0 & \cdots & 0 & -c_0/c_n \\ 1 & 0 & \cdots & 0 & -c_1/c_n \\ 0 & 1 & \cdots & 0 & -c_2/c_n \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \cdots & 1 & -c_{n-1} / c_n\end{bmatrix}$\\ [10pt]
\end{tabular}\retTwo\par}

\begin{myDindent}\myComment
	This matrix is called the \udefine{companion matrix} of $f(x)$ and we denote it by $c_f$. (Note that if $f$ is monic then $c_n = 1$\dots)\retTwo
\end{myDindent}

Thus we can conclude that $\frac{F[x]}{\langle f\rangle} \cong V_{c_f}$. Also, this leads to the following theorem.\retTwo

\exTwo\ul{Theorem (Rational Canonical Form):} If $a \in M_n(F)$ and $f_1, \ldots, f_m$ are the invariant factors of $a$ then $a$ is similar to $\diag(c_{f_1}, \ldots, c_{f_m})$.\retTwo

\hTwo\mySepTwo

\color{red}(This is the beginning of the final lecture I need to get through to fully catch up to the class. Unfortunately I didn't have time to do the third problem set.)\retTwo

\hTwo Firstly, we prove a uniqueness result related to the prior theorem. Note that given a matrix of the form:\\ [-34pt]

$$a  = \begin{bmatrix} 0 & 0 & \cdots & 0 & -c_0 \\ 1 & 0 & \cdots & 0 & -c_1 \\ 0 & 1 & \cdots & 0 & -c_2 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \cdots & 1 & -c_{n-1}\end{bmatrix} \in M_n(F)\text{,}$$

we can find a unique monic polynomial $f \in F[x]$ such that $a = c_f$.\retTwo

\exTwo\ul{Theorem (Uniqueness of the Rational Canonical Form):} Suppose $f_1, \ldots, f_m \in F[x]$\\ are monic polynomials satisfing that $f_1 \divides f_2 \divides \cdots \divides f_m$ and the matrix $a$ is similar to\\ $\diag(c_{f_1}, \ldots, c_{f_m})$. Then $f_1, \ldots, f_m$ are the invariant factors of the matrix $a$.
\begin{myIndent}\exThreeP
	Proof:\\
	$V_a \cong V_\diag(c_{f_1}, \ldots, c_{f_m}) \cong \bigoplus_{i=1}^m V_{c_{f_i}} \cong \bigoplus_{i=1}^m\frac{F[x]}{\langle f_i\rangle}$. Then as $f_1 \divides f_2 \divides \cdots \divides f_m$, we\\ can conclude by the uniqueness part of the fundamental theorem of finitely generated\\ modules over a P.I.D. that $f_1, \ldots, f_m$ are the invariant polynomials. $\blacksquare$\retTwo
\end{myIndent}

\hTwo\mySepTwo

\hypertarget{page 602 reference math 200b}{On} \inLinkRap{Definition of F-algebra page 518}{page 518} I wrote what it means for a unital commutative ring to be an $F$-algebra. However, for this next section it will be necessary to define what it means for a non-\\commutative unit ring to be an $F$-algebra. Also in general I want to prove some things\\ I didn't think of proving back on page 518.\retTwo

If $F$ is a field and $A$ is a noncommutative unital ring, we say that $A$ is an \udefine{$F$-algebra} if there exists a ring homomorphism $f : F \to A$ such that $f(1_f) = f(1_A)$ and $f(c)a = af(c)$ for all $a \in A$ and $c \in F$.
\begin{itemize}
	\item Note that if $A \neq \{0\}$ then $f$ is necessarily injective. This is because $\ker(f)$ is an ideal of $F$. That said, the only ideals of $F$ are $F$ and $\{0\}$, and we know that $\ker(f) \neq F$. After all, $1_F \notin \ker(f)$ since $f(1_F) = 1_A \neq 0_A$. It follows that $\ker(f) = \{0\}$ and that proves that $f$ is injective.
	\begin{myIndent}\myComment
		The significance of the prior result is that if $A$ is an $F$-algebra then we can view $F$ as being a field embedded into $A$.
	\end{myIndent}

	\item If $c \in F$ and $a \in A$ satisfy that $ca = 0$, then we must have that either $c = 0$ or $a = 0$. After all, suppose $c \neq 0$ but $ca = 0$. Then $a = c^{-1}ca = c^{-1}0 = 0$.

	\item I mentioned on page 518 that $A$ is an $F$-vector space when we consider defining\\ $c \cdot a = f(c)a$ for all $c \in F$ and $a \in A$. This is easy to show. That said, we do need to explicity assume that $f(c)a = af(c)$ for all $a \in A$ and $c \in F$ in order for us to have that $c \cdot (a_1 a_2) = (c \cdot a_1)a_2 = a_1(c \cdot a_2)$.
	
	\item For an example of a non-commutative  $F$-algebra that we'll be caring about, consider the set of matrices $M_n(F)$ and define $f: F \to M_n(F)$ by $c \mapsto cI$.\retTwo
\end{itemize}

\exTwo\ul{Lemma:} Suppose $A$ is an $F$-algebra (and $A \neq \{0\}$). If $\dim_F(A) < \infty$ then for all $a \in A$ there exists a unique nonconstant monic polynomial $m_{a;F}(x) \in F[x]$ such that for all $g \in F[x]$ we have that $g(a) = 0 \Longleftrightarrow m_{a;F}(x) \divides g(x)$ in $F[x]$.
\begin{myIndent}\exThreeP
	\begin{myIndent}\myComment
		Note that $m_{a;F}$ is called the \udefine{minimal polynomial} of $a$ over $F$.\retTwo
	\end{myIndent}

	Proof:\\
	Let $e_a: F[x] \to A$ be the evaluation homomorphism given by $e_a(f(x)) \coloneqq f(a)$. Then $e_a$ is an $F$-algebra homomorphism (meaning it is a ring homomorphism from $F[x]$ into $A$ and an $F$-linear map).
	\begin{myIndent}\exPPP
		Note that $e_a$ is a ring homomorphism specifically because $ca = ac$ for all $c \in F$ and $a \in A$. I won't prove that $e_a$ is a homomorphism though.\newpage
	\end{myIndent}

	We thus have that $\ker(e_a) \lhd F[x]$. Also, by the first isomorphism theorems for $F$-modules and rings, we have that $\frac{F[x]}{\ker(e_a)} \cong \myIm(e_a) \subseteq A$ as an $F$-algebra. Since $\dim_F(A) < \infty$ and $\dim_F(F[x]) = \infty$, we must have that $\ker(e_a) \neq \{0\}$. Hence as $F[x]$ is a P.I.D. and $F$ is a\\ [2pt] field, there must exist a unique monic polynomial $m_{a;F}(x) \in F[x]$ such that:
	
	{\centering$\ker(e_a) = \langle m_{a;F}\rangle$.\retTwo\par}
	
	Also note that $\ker(e_a) \neq F[x]$ as the only constant polynomial contained in $\ker(e_a)$ is the zero polynomial. Therefore, we can conclude that $m_{a;F} \neq 1$. And finally, $g \in \ker(e_a)$ iff $m_{a;F} \divides g$ in $F[x]$. $\blacksquare$\retTwo
\end{myIndent}

\hTwo For an application of the prior lemma, note that if $a \in M_n(F)$ then $\Ann(V_a) = \langle m_{a;F}(x)\rangle$.
\begin{myIndent}\pracTwo
	Why?\\
	$g(x) \cdot V_a = 0$ iff $\forall v \in F^n$, $g(a) |v{\rangle} = 0$. But the latter happens iff $g(a) = 0$, and that happens iff $m_{a;F}(x) \divides g(x)$.
\end{myIndent}

\hTwo\mySepTwo

Here's some more notes on annihilators.\retTwo

Given a field $F$ and $f \in F[x]$, we have that $\Ann(\frac{F[x]}{\langle f \rangle}) = \langle f \rangle$ (where we are viewing $\frac{F[x]}{\langle f \rangle}$\\ [-3pt] as an $F[x]$-module).
\begin{myIndent}\pracTwo
	Why?\\
	If $g \in \langle f\rangle$ then $g(x) = f(x)h(x)$. In turn, for any $s(x) + \langle f\rangle$ in $F[x]/\langle f\rangle$ we have\\ that $g(x)(s(x) + \langle f\rangle) = f(x)h(x)s(x) + \langle f\rangle \equiv 0 \mMod{f(x)}$. And this proves\\ that $\langle f\rangle \subseteq \Ann(\frac{F[x]}{\langle f \rangle})$. To show the other inclusion, suppose $g \in \Ann(\frac{F[x]}{\langle f \rangle})$. Then\\ $g(x)\left( 1 + \langle f\rangle\right) = g(x) + \langle f\rangle = 0 + \langle f\rangle$. The only way this is possible is if $g \in \langle f\rangle$.\retTwo

	\begin{myIndent}\myComment
		More generally, the above reasoning shows that if $A$ is a unital ring and $\mathfrak{a} \lhd A$,\\ then $\Ann(\frac{A}{\mathfrak{a}}) = \mathfrak{a}$ (where we are viewing $A/\mathfrak{a}$ as an $A$-module).\retTwo
	\end{myIndent}
\end{myIndent}

Suppose $\{M_i\}_{i \in I}$ is a family of $A$-modules. Then $\Ann(\bigoplus_{i \in I} M_i) = \bigcap_{i \in I}\Ann(M_i)$.
\begin{myIndent}\pracTwo
	Why?\\
	If $a \in \Ann(\bigoplus_{i^\prime \in I} M_{i^\prime})$ and we consider the projection $P_i: \bigoplus_{i^\prime \in I} M_{i^\prime} \to M_i$, then we know that $a \cdot m_i = P_i(a \cdot (m_{i^\prime})_{i^\prime \in I}) = P_i((0)_{i^\prime \in I}) = 0$. In particular, this shows that $a \cdot m_i = 0$ for all $i \in I$ and $m_i \in M_i$. Hence $a \in \bigcap_{i \in I}\Ann(M_i)$.\retTwo
	
	Conversely, if $a \in \bigcap_{i \in I}\Ann(M_i)$ then we know that $a \cdot m_i = 0$ for any $m_i \in M_i$ and $i \in I$. In turn, $a \cdot (m_{i^\prime})_{i^\prime \in I} = (a \cdot m_{i^\prime})_{i^\prime \in I} = 0$.\retTwo
\end{myIndent}

As a corollary, if $F$ is a field and $f_1, \ldots, f_m \in F[x]$, then:

{\centering$\Ann(\bigoplus_{i=1}^m \frac{F[x]}{\langle f_i\rangle}) = \bigcap_{i=1}^m \langle f_i\rangle = \langle \lcm(f_1, \ldots, f_m)\rangle$.\retTwo\par}

\begin{myIndent}\pracTwo
	To see what I specifically mean by a least common multiple, note that if $D$ is a P.I.D.\\ and $\langle d_1\rangle, \ldots, \langle d_m\rangle$ are ideals in $D$, then there must exist some element $\ell \in D$ with\\ $\langle \ell\rangle = \bigcap_{i=1}^m \langle d_i\rangle$. In turn, $\ell$ satisfies the property that $d_i \divides \ell$ for all $i$. Furthermore,\\ if $x$ satisfies that $d_i \divides x$ for all $i$, then $\ell \divides x$.\newpage
\end{myIndent}

\exTwo\ul{Proposition:} Suppose $F$ is a field. Then for all $a \in M_n(F)$ we have that $m_{a;F}(x) \in F[x]$ is the largest invariant factor of $a$.
\begin{myIndent}\exThreeP
	Proof:\\
	Suppose $f_1 \divides f_2 \divides \cdots \divides f_m$ are the invariant factors of $a$. Then, we know that:

	{\centering $\langle m_{a;F}(x)\rangle = \Ann(V_a) = \Ann(\frac{F[x]}{\langle f_1\rangle} \oplus \cdots \oplus \frac{F[x]}{\langle f_m\rangle}) = \bigcap_{i=1}^m \langle f_i\rangle = \langle f_m\rangle$ \retTwo\par}

	\begin{myIndent}\exPPP
		As a side note, if $M_1 \cong M_2$ as $A$-modules, then both $\Ann(M_1)$ and $\Ann(M_2)$ are equal subsets of $A$. In other words, it would be incorrect (or at least incredibly\\ misleading) to write $\Ann(M_1) \cong \Ann(M_2)$ as opposed to $\Ann(M_1) =\Ann(M_2)$.\retTwo
	\end{myIndent}

	It follows that $m_{a;F}(x)$ and $f_m(x)$ are companion elements of $F[x]$. Yet as both are monic, the only way this is possible is if $m_{a;F}(x) = f_m(x)$. $\blacksquare$\retTwo
\end{myIndent}

\hTwo\mySepTwo

The \udefine{characteristic polynomial} of a matrix $a \in M_n(A)$ (where $A$ is commutative unital ring) is defined as the polynomial $\chi_a(t) \coloneqq \det(tI - a)$ (where $tI - a \in M_n(A[t])$).\retTwo

\exTwo\ul{Lemma:} If $a_1, \ldots, a_m$ are square matrices over some ring $A$, then:

{\centering$\det(\diag(a_1, \ldots, a_m)) = \prod_{i=1}^m \det(a_i)$.\retTwo\par}

\begin{myIndent}\exThreeP
	Proof:\\
	By noting that $\diag(a_1, \ldots, a_{m-1}, a_m) = \diag(\diag(a_1, \ldots, a_{m-1}), a_m)$, it suffices to prove that $\det(\diag(a, b)) = \det(a)\det(b)$ where $a \in M_{n_1}(A)$ and $b \in M_{n_2}(A)$ for some integers $n_1$ and $n_2$.\retTwo

	Next, we proceed by induction on $n_1$. For our base case, note that if $n_1 = 1$ (so that $a$ is just a scalar in $A$), then we can take the Laplace expansion of the determinant formula to get that:

	{\centering $\det(\diag(a, b)) = (-1)^{1 + 1}a \det(b) + \sum_{i=1}^{n_2} 0 = \det(a)\det(b)$. \retTwo\par}

	As for the induction step, note again by the Laplace expansion formula that:

	{\center\begin{tabular}{l}
		$\det(\diag(a, b)) = \sum_{i=1}^{n_1}(-1)^{1 + i}a_{1, i} \det(\diag(a(1, i), b))$ \\ [4pt]

		$\phantom{\det(\diag(a, b))} = \sum_{i=1}^{n_1}(-1)^{1 + i}a_{1, i} \det(a(1, i))\det(b)$\\ [4pt]
		$\phantom{\det(\diag(a, b))} = \left( \sum_{i=1}^{n_1}(-1)^{1 + i}a_{1, i} \det(a(1, i))\right) \cdot \det(b) = \det(a)\det(b)$. $\blacksquare$
	\end{tabular}\retTwo\par}
\end{myIndent}

\exTwo\ul{Lemma:} If $b = gag^{-1}$ where $a, b, g \in M_n(A)$, then $\chi_a = \chi_{b}$.
\begin{myIndent}\exThreeP
	Proof:\\
	\begin{tabular}{l}
	$\chi_b = \det(tI - b) = \det(bI - gag^{-1}) = \det(g(tI - a)g^{-1})$\\
	$\phantom{\chi_b = \det(tI - b) = \det(bI - gag^{-1})} = \det(g)\det(tI - a)\det(g^{-1}) = \det(tI - a) = \chi_a$.
	\end{tabular}
\end{myIndent}

\ul{Lemma:} Suppose $f$ is a monic nonconstant polynomial in $F[t]$ (where $F$ is a field). If $c_f$ is the companion matrix then $\chi_{c_f} = f$.
\begin{myIndent}\exThreeP
	Proof:\\
	If $f(t) = t^n + c_{n-1}t^{n-1} + \ldots + c_1t + c_0$, then:\newpage

	{\centering $tI - c_f = \begin{bmatrix} t & 0 & \cdots & 0 & c_0 \\ -1 & t & \cdots & 0 & c_1 \\ 0 & -1 & \cdots & 0 & c_2 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \cdots & t & c_{n-2} \\ 0 & 0 & \cdots & -1 & t+c_{n-1}\end{bmatrix}$ \retTwo\par}

	By the laplace expansion formula, we get that:

	{\center\begin{tabular}{l}
		$\chi_{c_f}(t) = t\det\left(\begin{bmatrix} t & \cdots & 0 & c_1 \\ -1 & \cdots & 0 & c_2 \\ \vdots & \ddots & \vdots & \vdots \\ 0 & \cdots & t & c_{n-2} \\ 0 & \cdots & -1 & t+c_{n-1}\end{bmatrix}\right) + (-1)^{1 + n}c_0\det\left(\begin{bmatrix} -1 & t & \cdots & 0  \\ 0 & -1 & \cdots & 0  \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & t \\ 0 & 0 & \cdots & -1\end{bmatrix}\right)$\\ [40pt]

		$\phantom{\chi_{c_f}(t)} = t\chi_{c_{\left(\frac{f - c_0}{t}\right)}} + (-1)^{1 + n}c_0 (-1)^{n-1} = t\chi_{c_{\left(\frac{f - c_0}{t}\right)}} + c_0$
	\end{tabular}\retTwo\par}

	And now it follows by doing induction on the degree of $f$ that $\chi_{c_f} = f$.

	\begin{myIndent}\exPPP
		Technically I still need to show a base case. Suppose $f(t) = t + c_0$. Then\\ $tI - c_f = [t + c_0]$ and we trivially have that $\chi_{c_f}(t) = t + c_0 = f(t)$. \exThreeP $\blacksquare$\retTwo
	\end{myIndent}
\end{myIndent}

\exTwo\ul{Corollary:} If $f_1 \divides f_2 \divides \cdots \divides f_m$ are the invariant factors of a matrix $a \in M_n(F)$ then:

{\centering$m_{a;F}(x) = f_m(x)$ and $\chi_a(x) = \prod_{i=1}^m f_i(x)$.\retTwo\par}

\begin{myIndent}\exThreeP
	Proof:\\
	We already proved that $m_{a;F}(x) = f_m(x)$. To show the other equality, note that as $a$ is similar to $\diag(c_{f_1}, \ldots, c_{f_m})$ we have that:
	
	{\centering$\chi_a = \chi_{\diag(c_{f_1}, \ldots, c_{f_m})} = \prod_{i=1}^m \chi_{c_{f_i}} = \prod_{i=1}^m f_i$. $\blacksquare$\retTwo\par}

	\begin{myIndent}\exPPP
		(Note that $\chi_{\diag(c_{f_1}, \ldots, c_{f_m})} = \prod_{i=1}^m \chi_{c_{f_i}}$ because of the first of our three prior lemmas plus the fact that $tI - \diag(c_{f_1}, \ldots, c_{f_m}) = \diag(tI - c_{f_1}, \ldots, tI - c_{f_m})$.)\retTwo
	\end{myIndent}
\end{myIndent}

\hTwo Consequently, we get the following theorem:\retTwo

\exTwo\ul{Cayley Hamilton Theorem:} Suppose $F$ is a field and $a \in M_n(F)$. Then:
\begin{itemize}
	\item $m_{a;F}(x) \divides \chi_a(x)$,
	\item If $p(x)$ is an irreducible factor of $\chi_a(x)$ then $p(x) \divides m_{a;F}(x)$.
\end{itemize}

\hTwo\mySepTwo

\dispDate{2/2/2026}

\blect{Math 220b Lecture Notes:}\retTwo

The \udefine{Mittag-Leffler Problem on $\mathbb{C}$} goes as follows:\newpage

Suppose $\{a_n\}_{n \in \mathbb{N}}$ is a sequence of distinct elments in $\mathbb{C}$ with $|a_n| \to \infty$ as $n \to \infty$, and for each $n$ suppose we have a function of the form:

{\center$g_n(z) = \frac{A_{n,m_n}}{(z - a_n)^{m_n}} + \frac{A_{n,m_n-1}}{(z - a_n)^{m_n - 1}} + \cdots + \frac{A_{n,1}}{(z - a_n)^{1}}$,\\ [6pt]\par}

where $A_{n,m_n} \neq 0$ and $m_n \in \mathbb{Z}_{> 0}$ for all $n$. Then, does there exists a meromorphic function $f$ on $\mathbb{C}$ with poles only at the $a_n$ and Laurent principal parts $g_n$ on punctured disks about any $a_n$.\retTwo

As we will show, the answer is yes.
\begin{myIndent}\pracOne
	Some remarks:
	\begin{itemize}
		\item If we only have finitely many $a_n$ and $g_n$, then the problem is trivial. After all, we can then just set $f = \sum g_n$. This is why we now focus on the infinite case.\retTwo
		
		\item In the infinite case, we can no longer guarentee that $\sum_{n=1}^\infty g_n$ converges.\\ However, notice that if we subtract a function $h_n$ which is holomorphic on $\mathbb{C}$ (i.e. has no poles), then that will not effect the principal part of the Laurent expansion about any particular point. Therefore, the general idea of the following proof is that we will try to find \udefine{convergence enhancing corrections} $h_n \in O(\mathbb{C})$ such that $\sum_{n=1}^\infty (g_n - h_n)$ converges.
		\begin{myIndent}\pracTwo
			As a side note, the $h_n$ will not be unique.\retTwo
		\end{myIndent}

		\item One other observation is that if both $f_1$ and $f_2$ solve the same Mittag-Leffler problem, then we must have that $f_1 - f_2$ are entire functions. This is because the principal part of $f_1 - f_2$ about any singularity cancels and we are left with a power series.\retTwo
	\end{itemize}
\end{myIndent}

\exTwo\ul{Proof that we can solve the Mittag-Leffler problem:}
\begin{myIndent}\exThreeP
	To start off, without loss of generality assume no $a_n$ is equal to $0$. After all, we can just add $\frac{A_m}{z^m} + \cdots + \frac{A_1}{z}$ at the end if needed.\retTwo

	Let $\{r_n\}_{n \in \mathbb{N}}$ be a sequence such that $r_n \to \infty$ as $n \to \infty$ and $r_n < |a_n|$ for all $n$. Also let $\{c_n\}_{n \in \mathbb{N}}$ be any sequence in $(0, \infty)$ satisfying that $\sum_{n=1}^\infty c_n < \infty$.\retTwo

	Consider $g_n(z) = \frac{A_{n,m_n}}{(z - a_n)^{m_n}} + \frac{A_{n,m_n-1}}{(z - a_n)^{m_n - 1}} + \cdots + \frac{A_{n,1}}{z - a_n}$. Since $a_n \neq 0$, $g_n$ is\\ holomorphic on $\Delta(0, |a_n|)$. So, consider the Taylor expansion of $g_n$ in $\Delta(0, |a_n|)$ around $0$. Since $\overline{\Delta}(0, r_n) \subseteq \Delta(0, |a_n|)$, the Taylor series of $g_n$ converges uniformly in $\overline{\Delta}(0, r_n)$. So, we can pick a Taylor polynomial $h_n$ such that $|g_n - h_n| < c_n$ in $\overline{\Delta}(0, r_n)$.\retTwo

	Let $f = \sum_{n=1}^\infty (g_n - h_n)$.\retTwo

	Claim: $f$ is meromorphic on $\mathbb{C}$ with poles only at the $a_n$ and with principal parts $g_n$ near\\ $a_n$ for each $n$.
	\begin{myIndent}\exPPP
		Proof:\newpage
		Let $r > 0$. Since $r_n \to \infty$, we know that there exists $N$ with $r_n > r$ if $n \geq N$. In turn: $|g_n - h_n| < c_n$ in $\overline{\Delta}(0, r) \subseteq \Delta(0, r_n)$ for all $n \geq N$, and by the Weierstra $M$-test we can conclude that $\sum_{n=N}^\infty (g_n - h_n)$ converges uniformly on $\overline{\Delta}(0, r)$. Also since $|a_n| > r_n > r$ for all $n \geq N$, we have that $g_n - h_n$ is holomorphic in $\Delta(0, r)$. Thus the sum $\sum_{n=N}^\infty (g_n - h_n)$ is also holomorphic in $\Delta(0, r)$.\retTwo

		Meanwhile, the sum $\sum_{n=1}^{N-1} (g_n - h_n)$ is meromorphic in $\Delta(0, r)$ since it is a finite sum of meromorphic functions. It's poles are only at the $a_j$ with $|a_j| < r$, and the Laurent principal parts around each of those $a_j$ are $g_j$.\retTwo

		Finally, $f = (\sum_{n=1}^{N-1} (g_n - h_n)) + (\sum_{n=N}^\infty (g_n - h_n))$ is meromorphic since on $\Delta(0, r)$ since is the sum of a meromorphic and a holomorphic function. Also adding the holomorphic term doesn't change the poles and principal parts of the meromorphic term. Hence, $f$ has poles at $|a_j| < r$ with the desired principal parts about those poles.\retTwo 

		By taking $r \to \infty$ we finish the proof. \exThreeP$\blacksquare$\retTwo
	\end{myIndent}
\end{myIndent}

\hTwo You may note that the prior proof also gives us an algorithm for finding a solution\\ to a Mittag-Leffler problem.\retTwo

\ul{Example 1:} Let's find a function $f$ solving the Mittag-Leffler problem associated with\\ $(-n, \frac{1}{z + n} : n \in \mathbb{Z})$.

{\centering\includegraphics[scale=0.9]{Mittag-Leffler example 1.png}\retTwo\par}

\begin{myIndent}\pracTwo
	Step 0:\\
	We ignore the pole at $n = 0$ for now since at the end we can just add $\frac{1}{z}$ to whatever function we get.\retTwo

	Step 1 (Taylor Expand):\\
	When $|z| < n$, we have that:
	
	{\centering$g_n(z) = \frac{1}{z + n} = \frac{1}{n} \cdot \frac{1}{1 + \frac{z}{n}} = \frac{1}{n}\left(1 - \frac{z}{n} + \frac{z^2}{n^2} - \cdots\right) = \frac{1}{n} - \frac{z}{n^2} + \frac{z^2}{n^3} - \cdots$\retTwo\par}

	It would be convenient if our convergence enhancing correction $h_n$ is just $\frac{1}{n}$ since then all the corrections would cancel in our final sum. So, let's try setting $h_n = \frac{1}{n}$.\retTwo

	Step 2:\\
	When we pick our $r_n$, we must have that $r_n < |n|$. But then for all $|z| \leq r_n$ we shall have that:
	
	{\centering $|g_n - h_n| = |\frac{1}{z + n} - \frac{1}{n}| = \frac{|z|}{|n||n + z|} \leq \frac{r_n}{|n|\cdot \left||n| - |z|\right|} \leq \frac{r_n}{|n|\cdot (|n| - r_n)} = \frac{r_n}{|n|^2 - |n|r_n} \eqqcolon c_n$. \retTwo\par}

	Set $r_n = \frac{1}{2}|n|^{1/2}$. Then $r_n \to \infty$ as $|n| \to \infty$. Also,
	
	{\centering$\sum_{n \in \mathbb{Z} - \{0\}} c_n = \frac{1}{2}\sum_{n \in \mathbb{Z} - \{0\}} \frac{1}{|n|^{3/2} - \frac{1}{2}|n|} < \infty$\newpage\par}

	Step 3:\\
	Our solution to the Mittage-Leffler problem is now $f(z) = \frac{1}{z} + \sum_{n \in \mathbb{Z} - \{0\}} (\frac{1}{z + n} - \frac{1}{n})$. After grouping together the terms for $n$ and $-n$, we get that:

	{\centering $f(z) = \frac{1}{z} + \sum_{n=1}^\infty (\frac{1}{z + n} + \frac{1}{z - n}) = \frac{1}{z} + \sum_{n=1}^\infty \frac{2z}{z^2 - n^2}$ \retTwo\par}

	On \inLinkRap{Math 220b Conway exercise V.2.8}{pages 550-553}, we showed that the latter expression is equal to $\pi \cot( \pi z)$.\retTwo

	\begin{myIndent}\myComment
		As a side note, while the sum $\sum_{n \in \mathbb{Z} - \{0\}} (\frac{1}{z + n} - \frac{1}{n})$ with the convergence\\ enhancing corrections as well as the sum $\sum_{n=1}^\infty \frac{2z}{z^2 - n^2}$ converge absolutely locally\\ uniformly on it's domain, clearly $\sum_{n \in \mathbb{Z} - \{0\}} \frac{1}{z + n}$ does not converge absolutely.\retTwo
	\end{myIndent}
\end{myIndent}

\ul{Example 2:} Let's find a function $f$ solving the Mittag-Leffler problem associated with\\ $(-n, \frac{1}{(z + n)^k}: n \in \mathbb{Z})$ where $k$ is an integer greater than $1$.

{\centering\includegraphics[scale=0.9]{Mittag-Leffler example 2.png}\retTwo\par}

\begin{myIndent}\pracTwo
	Step 1:\\
	Given $g_n = \frac{1}{(z + n)^k}$, set $h_n = 0$.\retTwo

	Step 2:\\
	Set $r_n = \frac{1}{2}|n|^{1/2}$. Then $r_n \to \infty$ as $|n| \to \infty$. Also if $|z| \leq r_n$ then:

	{\centering $|g_n - h_n| = |\frac{1}{(z +n)^k}|\ \leq \frac{1}{(|n| - r_n)^k} \eqqcolon c_n$ \retTwo\par}

	Also $\sum_{n \in \mathbb{Z} - \{0\}} c_n = \sum_{n \in \mathbb{Z} - \{0\}} \frac{1}{(|n| - \frac{1}{2}|n|^{1/2})^k} < \infty$.\retTwo

	Step 3:\\
	We get the solution to our Mittag-Leffler problem:

	{\centering $f = \sum_{n=-\infty}^\infty \frac{1}{(z + n)^2}$ \retTwo\par}

	\begin{myIndent}\myComment
		As a side note, $G_k(z) = \sum_{n \in \mathbb{Z}}\frac{1}{(z+n)^k}$ is called the $k$th \udefine{($1$-dimensional) Eisenstein\\ [-2pt] series}.\retTwo

		Also note in the special case of $G_2(z)$:
		
		\begin{myIndent}
			Since $\pi \cot(\pi z) = \frac{1}{z} + \sum_{n \in \mathbb{Z} - \{0\}}(\frac{1}{z + n} - \frac{1}{n})$ where the latter converges\\ absolutely locally uniformly on it's domain, we can term-by-term differentiate to get that:

			{\centering $\frac{-\pi^2}{\sin^2(\pi z)} = -\frac{1}{z^2} + \sum_{n \in \mathbb{Z} - \{0\}} \frac{-1}{(z + n)^2} = -G_2(z)$. \retTwo\par}

			So, $G_2(z) = \pi^2\csc^2(\pi z)$.\retTwo

			By analogous reasoning we also get that $G_k(z) = \frac{(-1)^{k-1}}{(k-1)!} \cdot \frac{\df^{k-1}}{\df z^{k-1}}(\pi \cot(\pi z))$.\newpage
		\end{myIndent}
	\end{myIndent}
\end{myIndent}

If $X$ is a topological space, we say a collection of functions $\mathcal{F} = \{f_{\alpha} : X \to \mathbb{C}\}_{\alpha \in A}$ is:
\begin{itemize}
	\item[(1.)]	\udefine{pointwise bounded} if for all $x \in X$ there exists $M(x) > 0$ such that $|f_\alpha(x)| \leq M(x)$ for all $\alpha \in A$;
	\item[(2.)] \udefine{uniformly bounded} if there exists $M > 0$ such that $|f_\alpha(x)| \leq M$ for all $x \in X$;
	\item[(3.)] \udefine{locally uniformly bounded} if for all $x \in X$ there exists a neighborhood $N$ such that $\{f_\alpha|_N\}_{\alpha \in A}$ are uniformly bounded;
	\item[(4.)] \udefine{uniformly bounded on compact sets} if for all compact sets $K$ there exists $M(K) > 0$ such that $|f_\alpha(x)| \leq M(K)$ for all $\alpha \in A$ and $x \in K$.\retTwo
\end{itemize}

Note that (3.) and (4.) are equivalent if $X$ is locally compact.\retTwo

\pracOne\ul{Example:} $\{f_n(z) = nz^n : n \in \mathbb{N}\}$ is locally uniformly bounded on $\Delta(0, 1)$ but not\\ uniformly bounded.
\begin{myIndent}\pracTwo
	For $0 \leq r < 1$, $|f_n(z)| \leq nr^n$ in $\overline{\Delta}(0, r)$. Since $\lim_{n \to \infty}nr^n = 0$, we know that there is some $M$ such that $\{nr^n\}$ is bounded by $M$. So $|f_n(z)| \leq M$ in $\overline{\Delta}(0, r)$. This proves that $\{f_n\}_{n \in \mathbb{N}}$ is uniformly bounded on comapct sets in $\Delta(0, 1)$. After all, if $K \subseteq \delta(0, 1)$ is compact, then $K \subseteq \overline{\Delta}(0, r)$ for some $r < 1$.\retTwo

	That said, since $f_n(\frac{1}{\sqrt[n]{2}}) = \frac{n}{2} \to \infty$ as $n \to \infty$, we know that $\{f_n\}_{n \in \mathbb{N}}$ is not uniformly bounded on $\Delta(0, 1)$.\retTwo
\end{myIndent}

\hTwo For sequences in $\mathbb{C}$, we have that boundedness implies that there is some convergent subsequence. We'd like to generalize this notion to sequences of functions.\retTwo

Our dream statement would be that if $\{f_n : U \to \mathbb{C}\}_{n \in \mathbb{N}}$ is a locally uniformly bounded sequence of functions, then $\{f_n\}_{n \in \mathbb{N}}$ would admit a locally convergent subsequence.\\ Unfortunately this statement does not hold in real analysis.
\begin{myIndent}\pracTwo
	For example, let $U = \mathbb{R}$ and define $f_n(x) = \sin(nx)$. Then $f_n$ is uniformly bounded (by $1$) but it has no pointwise convergent subsequence.\retTwo
\end{myIndent}

In math 240b, we were able to partially rescue this goal with the Arzel-Ascoli theorems.
\begin{myIndent}\exTwo
	\begin{itemize}
		\item \ul{(Folland) Theorem 4.43:} Let $X$ be a compact Hausdorff space. If $\mathcal{F}$ is a\\ (pointwise) equicontinuous, pointwise bounded subset of $C(X)$, then\\ $\mathcal{F}$ is totally bounded in the uniform metric. Hence, the closure of $\mathcal{F}$ in $C(X)$\\ is compact.
		\begin{myIndent}\exPPP
			(As a result of that compactness, any sequence in $\mathcal{F}$ must have a uniformly\\ convergent subsequence).\retTwo
		\end{myIndent}
		
		\item \ul{(Folland) Theorem 4.44:} Let $X$ be a $\sigma$-compact LCH space. If $\{f_n\}_{n \in \mathbb{N}}$ is\\ a (pointwise) equicontinuous, pointwise bounded sequence in $C(X)$, then there\\ exists $f \in C(X)$ and a subsequence of $\{f_n\}_{n \in \mathbb{N}}$ converging to $f$ uniformly on compact sets.
		\begin{myIndent}\exPPP
			(Note that any open set $U \subseteq \mathbb{C}$ or $\mathbb{R}$ is a $\sigma$-compact LCH space.)\retTwo
		\end{myIndent}
	\end{itemize}
\end{myIndent}

In math 220b, we shall prove our dream statement holds when all $f_n$ are holomorphic.\newpage

We say a family of functions $\mathcal{F}$ is \udefine{normal} if all sequences in $\mathcal{F}$ admit a locally uniformly convergent subsequence.\retTwo

\exTwo\ul{Lemma:} If $\mathcal{F}$ is a normal family of holomorphic functions then $\mathcal{F}^\prime \coloneqq \{f^\prime : f \in \mathcal{F}\}$ is also normal.
\begin{myIndent}\exThreeP
	Proof:\\
	Let $\{f_n^\prime\}_{n \in \mathbb{N}}\subseteq \mathcal{F}^\prime$ be any sequence. Then $\{f_n\}_{n \in \mathbb{N}}$ is a sequence in $\mathcal{F}$. So by the\\ normality of $\mathcal{F}$ we can find a subseqeunce $\{f_{n_k}^\prime\}_{k \in \mathbb{N}}$ such that $f_{n_k} \locUConverges f$ as $k \to \infty$.\\ By the Weierstra convergence theorem, we thus have that $f_{n_k}^\prime \locUConverges f^\prime$. So, $\mathcal{F}^\prime$ is\\ [2pt] also normal. $\blacksquare$\retTwo
\end{myIndent}

\exTwo\ul{Lemma:} If $U \subseteq \mathbb{C}$ is open and $\mathcal{F} \subseteq O(U)$ is locally uniformly bounded, then so is $\mathcal{F}^\prime$.
\begin{myIndent}\exThreeP
	Proof:\\
	Suppose $a \in U$. Then there exists $r, M > 0$ such that $\Delta(a, r) \subseteq U$ and for all $f \in \mathcal{F}$ we have that $|f| \leq M$ over $\Delta(a, r)$.\retTwo

	Now we can use Cauchy's estimate to bound $|f^\prime|$ over $\Delta(a, r/2)$. Let $b \in \Delta(a, r/2)$. Then as $\Delta(b, r/2) \subseteq \Delta(a, r)$, we know by Cauchy's estimate (see \inLinkRap{Conway Cauchy's Estimate}{page 368})  that:

	{\centering $|f^\prime(b)| \leq \frac{2M}{r}$. $\blacksquare$ \retTwo\par}
\end{myIndent}

\pracOne\mySepTwo
\ul{Example:} Let $\mathcal{F} = \{f \in O(\Delta(0, 1)) : f = \sum_{k=1}^\infty a_k z^k \text{ where each } |a_k| \leq k\}$. Then $\mathcal{F}$ is locally uniformly bounded.
\begin{myIndent}\pracTwo
	Why?\\
	If $K \subseteq \Delta(0, 1)$ is compact then there exists $0 < r < 1$ such that $K \subseteq \overline{\Delta}(0, r)$. But then if $|z| \leq r$ we have that:

	{\centering $|f(z)| \leq \sum_{k=1}^\infty |a_k||z|^k \leq \sum_{k=1}^\infty k r^k = r(\sum_{k=1}^\infty kr^{k-1}) = \frac{r}{(1 - r)^2}$ \retTwo\par}
\end{myIndent}

\mySepTwo

\color{red} Here is something important I just noticed. In Math 140b and also apparently in math 220b, we define that a collection of functions $\mathcal{F}$ from one metric space $(X, d)$ to another metric space $(\Omega, \rho)$ are \udefine{equicontinuous} if:

{\centering$\forall \varepsilon > 0,\hspace{0.4em} \exists \delta > 0 \suchthat \forall f \in \mathcal{F} \text{ and } x, y \in X \text{ with } d(x, y) < \delta,\hspace{0.4em} \rho(f(x), f(y)) < \varepsilon$.\retTwo\par}

This is different from the definition used in my functional analysis notes in this journal and my math 240b notes where we'd say $\mathcal{F}$ is equicontinuous if:

{\centering$\forall \varepsilon > 0 \text{ and } x \in X,\hspace{0.4em} \exists \delta > 0 \suchthat \forall f \in \mathcal{F} \text{ and } y \in X \text{ with } d(x, y) < \delta,\hspace{0.4em} \rho(f(x), f(y)) < \varepsilon$.\par}

\begin{myTindent}
	\color{orange}(In the language of the Math 220b, this would instead define pointwise equicontinuity.)\retTwo
\end{myTindent}

I think the reason for this difference in terminology is that the 140b and 220b definition of equicontinuity only makes sense in metric spaces while the 240b and functional analysis definition makes sense in general topological spaces where we are typically working in those subjects. The convention I will follow is that I'll try to explicity distinguish between these two definitions by writing (uniformly) or (pointwise) whenever there is ambiguity.\newpage

\exTwo\ul{Lemma:} Suppose $(X, d)$ is a locally compact metric space and $(\Omega, \rho)$ is another metric space. Then given a collection $\mathcal{F} \subseteq C(X, \Omega)$, we have that the following are equivalent:
\begin{itemize}
	\item[1.] $\mathcal{F}$ is (pointwise) equicontinuous;
	\item[2.] $\mathcal{F}$ is \udefine{locally equicontinuous}, meaning for all $x \in X$ there exists some $r > 0$ such that $\mathcal{F}$ is (uniformly) equicontinuous on the ball $\Delta(x, r)$.
	\item[3.] $\mathcal{F}$ is (uniformly) equicontinuous on each compact set $K \subseteq X$.
\end{itemize}

\begin{myIndent}\exThreeP
	$(1. \Longrightarrow 3.)$\\
	Suppose $K \subseteq X$ is compact. Then we claim that $\mathcal{F}$ is (uniformly) equicontinuous on $K$.
	\begin{myIndent}\exPPP
		Given any $\varepsilon > 0$, we can pick $\delta_w > 0$ for each $w \in K$ such that for any $z \in K$ and $f \in \mathcal{F}$, we have that $d(z, w) < \delta_w \Longrightarrow \rho(f(z), f(w)) < \sfrac{\varepsilon}{2}$. By the Lebesgue number lemma (see \inLinkRap{Page 124 reference Lebesgue Number Lemma}{page 124}), we can then find some $\delta > 0$ such that for all $z \in K$ there exists $w \in K$ with $\Delta(z, \delta) \subseteq \Delta(w, \delta_w)$.\retTwo

		In particular, consider any $f \in \mathcal{F}$ and $z, z^\prime \in K$ with $d(z, z^\prime) < \delta$. Then after picking $w \in K$ with $z^\prime \in \Delta(z, \delta) \subseteq \Delta(w, \delta_w)$ we have that:

		{\centering $\rho(f(z), f(z^\prime)) \leq \rho(f(z), f(w)) + \rho(f(w), f(z^\prime)) < \sfrac{\varepsilon}{2} + \sfrac{\varepsilon}{2}$. \retTwo\par}
	\end{myIndent}

	$(3. \Longrightarrow 2.)$\\
	Given $x \in X$, we can (by assumption) find a compact neighborhood $K$ containing $x$.\\ In turn, there exists $r > 0$ such that $\Delta(x, r) \subseteq K$. And finally as $\mathcal{F}$ is (uniformly)\\ equicontinuous on $K$, we also have $\mathcal{F}$ is (uniformly) equicontinuous on $\Delta(x, r)$.\retTwo

	$(2. \Longrightarrow 1.)$\\
	This is trivial. $\blacksquare$\retTwo
\end{myIndent}

\exTwo\ul{Proposition:} Suppose $X$ is an LCH space and $\mathcal{F} = \{f_\alpha : X \to \mathbb{C}\}_{\alpha \in A}$ is a pointwise\\ bounded and (pointwise) equicontinuous collection of functions. Then $\mathcal{F}$ is uniformly\\ bounded on compact sets.
\begin{myIndent}\exThreeP 
	Proof:\\
	Suppose $K$ is any compact set. Then by the equicontinuity of $\mathcal{F}$ we can find for each\\ $x \in K$ a neighborhood $U_x \subseteq X$ such that $y \in U_x \Longrightarrow |f(y) - f(x)| < 1$. Next, by the compactness of $K$, we can find $x_1, \ldots, x_m$ such that $K \subseteq \bigcup_{i=1}^m U_{x_i}$. Because $\mathcal{F}$ is pointwise bounded, we can find for $i =1, \ldots, m$ some $M_i > 0$ such that $|f(x_i)| \leq M_i$ for all $f \in \mathcal{F}$. Finally, by setting $M = \max(M_1, \ldots, M_m)$, we have that $|f(x)| < M + 1$ for all $x \in K$ and $f \in \mathcal{F}$. $\blacksquare$\retTwo
\end{myIndent}

\hTwo The converse of the last proposition also holds if $\mathcal{F} \subseteq O(U)$ where $U \subseteq \mathbb{C}$ is an open set.\retTwo

\exTwo\ul{Proposition:} Suppose $U \subseteq \mathbb{C}$ is an open set $\mathcal{F} \subseteq O(U)$ is a locally bounded family\\ of holomorphic functions. Then $\mathcal{F}$ is locally equicontinuous (and also trivially pointwise\\ bounded).
\begin{myIndent}\exThreeP
	Proof:\\
	Fix any $a \in U$. Then there exists $r, M > 0$ such that $\overline{\Delta}(a, 2r) \subseteq U$ and $f|_{\overline{\Delta}(a, 2r)}$ is bounded by $M$ for all $f \in \mathcal{F}$. We claim from this that $\mathcal{F}$ is (uniformly) equicontinuous on $\Delta(a, r)$.\newpage
	\begin{myIndent}\exPPP
		Let $\varepsilon > 0$. Then if $z, w \in \Delta(a, r)$ we have by Cauchy's integral formula (after letting $\gamma = a + 2re^{it}$ where $t \in [0, 2\pi]$) that:
		
		{\centering\begin{tabular}{l}
			$|f(z) - f(w)| = \left|\frac{1}{2\pi i}\int_{\gamma} \frac{f(\xi)}{\xi - z} \df \xi- \frac{1}{2\pi i}\int_{\gamma} \frac{f(\xi)}{\xi - w}\df \xi \right|$\\ [8pt]
			$\phantom{|f(z) - f(a)|} = \frac{1}{2\pi}\left|\int_{\gamma} \frac{f(\xi)}{\xi - z} - \frac{f(\xi)}{\xi - w} \df \xi\right|$\\ [8pt]
			$\phantom{|f(z) - f(a)|} = \frac{|z - w|}{2\pi}\left|\int_{\gamma} \frac{f(\xi)}{(\xi - z)(\xi - w)} \df \xi\right| \leq \frac{|z - w|}{2\pi} \cdot \frac{4\pi rM}{r^2} = \frac{2M}{r}|z - w|$\\ [4pt]
		\end{tabular} \retTwo\par}

		Now just set $\delta = \frac{r\varepsilon}{2M}$. Then if $z, w \in \Delta(a, r)$ satisfy that $|z - w| < \delta$ then we have that $|f(z) - f(w)| < \varepsilon$. \exTwoP$\blacksquare$\retTwo
	\end{myIndent}
\end{myIndent}

\ul{Corollary (Montel's Theorem):} If $U \subseteq \mathbb{C}$ is open and $\mathcal{F} \subseteq O(U)$ is a family of holomorphic functions, then $\mathcal{F}$ is normal if and only if $\mathcal{F}$ is locally bounded.
\begin{myIndent}\exTwoP
	$(\Longleftarrow)$\\
	By Arzel-Ascoli plus the prior proposition, we can see that if $\mathcal{F}$ is locally bounded then $\mathcal{F}$ is normal.\retTwo

	$(\Longrightarrow)$\\
	Suppose for the sake of contradiction that $\mathcal{F}$ is normal but that $\mathcal{F}$ is not locally bounded. Then since $\mathcal{F}$ is not locally bounded, we know there exists some $z \in U$ such that $\mathcal{F}$ is not uniformly bounded on $\Delta(z, r)$ for all $r > 0$. In particular, if we pick $R > 0$ such that $\Delta(z, R) \subseteq U$, then for all $n \in \mathbb{N}$ we can find some $f_n \in \mathcalli{F}$ and $w_n \in \Delta(z, \sfrac{R}{n})$ with $|f_n(w_n)| > n$.\retTwo

	But now as $\mathcal{F}$ is normal, we know that $\{f_n\}_{n \in \mathbb{N}}$ has a subsequence $\{f_{n_k}\}_{k \in \mathbb{N}}$ locally uniformly converging to some $f$. In particular, we must have that $f$ is continuous. Hence, there must exist $\delta > 0$ such that $|f(w) - f(z)| < 1$ whenever $|w - z| < \delta$.\retTwo

	Yet we also know from the local uniform convergence of $\{f_{n_k}\}_{k \in \mathbb{N}}$ that there exists $N \in \mathbb{N}$ such that $|f_{n_k}(w) - f(w)| < 1$ for all $k \geq N$. Furthermore, by increasing $N$ if necessary, we can make it so that for all $k \geq N$ we have that:
	
	{\centering$\sfrac{R}{n_k} < \delta$ and $n_k > |f(z)| + 2$\retTwo\par}

	In turn, if we pick any $k \geq N$ we know $|w_{n_k} - z| < \delta$ and thus $|f(w_{n_k}) - f(z)| < 1$.\\ Yet at the same time, $|f(w_n)| > |f_{n_k}(w_{n_k})| - 1 > |f(z)| + 1$. Hence, we have that $|f(w_{n_k}) - f(z)| \geq |f(w_n)| - |f(z)| \geq 1$. This is a contradiction. $\blacksquare$\retTwo
\end{myIndent}


\newpage


































% \exTwo\ul{Theorem 3.18:} In a locally convex topological vector space $\mathcalli{X}$, $E \subseteq \mathcalli{X}$ is weakly (von\\ Neumann) bounded iff $E$ is originally (von Neumann) bounded.

% \begin{myIndent}\exThreeP
% 	Proof:\\
% 	Since the weak topology on $\mathcalli{X}$ is coarser then the original topology, it's trivial that being originally bounded means that a set is weakly bounded.\retTwo

% 	Meanwhile, suppose $E \subseteq \mathcalli{X}$ is weakly bounded and $U$ is any neighborhood of $0$ in the original topology on $\mathcalli{X}$. Since $\mathcalli{X}$ is locally convex, we can find a convex balanced open set $V$ in the original topology such that $0 \in V \subseteq \overline{V} \subseteq U$.
% 	\begin{myIndent}\exPPP
% 		Specifically, first let $V_1$ be an open set in $\mathcalli{X}$ such that $e \in V_1 \subseteq \overline{V_1} \subseteq U$. (We can do this by the above corollary). Secondly, we can use local convexity to find a convex open set $V_2$ such that $e \in V_2 \subseteq V_1$. Thirdly, by the work on \inLinkRap{Page 230 definition of balanced sets}{pages 230-232} we can find a balanced convex open set $V$ such that $e \in V \subseteq V_2$ Finally, as $V \subseteq V_2 \subseteq V_1 \subseteq \overline{V_1} \subseteq U$, we have that $\overline{V} \subseteq U$.\retTwo

% 		\begin{myIndent}\color{RawerSienna}
% 			I should note by the way that $\overline{V}$ is the closure of $V$ in the original topology.\retTwo
% 		\end{myIndent}
% 	\end{myIndent}

% 	Let $K \coloneqq \{\lambda \in \mathcalli{X}^* : |\lambda(x)| \leq 1 \text{ for all } x \in V\}$. Then we claim that:

% 	{\centering $\overline{V} = \{x \in \mathcalli{X} : |\lambda(x)| \leq 1 \text{ for all } \lambda \in K\}$ \retTwo\par}

% 	\begin{myIndent}\exPPP
% 		By the definition of $K$ we know that $V \subseteq \{x \in \mathcalli{X} : |\lambda(x)| \leq 1 \text{ for all } \lambda \in K\}$. Also, we know the latter set is closed since it is the intersection of a bunch of closed sets. Hence, we've proven that $\overline{V} \subseteq \{x \in \mathcalli{X} : |\lambda(x)| \leq 1 \text{ for all } \lambda \in K\}$.\retTwo

% 		To show the other inclusion, suppose $x \in X - \overline{V}$.



% 	\end{myIndent}


% \end{myIndent}








% Another vector-valued integral is the \udefine{Pettis integral}. To establish this integral, I shall follow Rudin's functional analysis book.\retTwo

% \exTwo\ul{T}



% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\hypertarget{Page 378 Reference}{}
\hypertarget{Math 200a Set 4 Problem 3}{}


\hypertarget{Generalization page 189}{} 


\hypertarget{Math 200a problem set 2 what is a commutator and derived subgroup}{}



\hypertarget{Folland Proposition 11.2}{}
\hypertarget{Folland Lemma 7.15 reference}{}
\hypertarget{Folland Proposition 11.4(b)}{}

\hypertarget{Alireza lemma page 257}{}

\hypertarget{page 337 reference}{}

\hypertarget{Folland Proposition 10.1}{}


\hypertarget{Folland proposition 11.1}{}

\hypertarget{Ergodic reading group notes 3}{}

\hypertarget{existence and uniqueness diff eq notes}{}
\hypertarget{math 241a lecture 5}{}
\hypertarget{idk reference 2}{}

\hypertarget{idk reference 5}{}
\hypertarget{idk reference 6}{}

\end{document}


% \blect{Math 220 Homework:}\\

% \blab{Exercise III.2.2:} Prove that if $b_n, a_n$ are real and positive, $0 < b = \lim_{n \to \infty} b_n$, and $a = \limsup_{n \to \infty} a_n$, then $ab = \limsup_{n \to \infty} (a_nb_n)$.

% \begin{myIndent}\HexOne

% \end{myIndent}



% \hTwo Suppose $|G| = pq$ where $p < q$ are prime numbers. Then $s_q = 1$. Hence there exists a unique Sylow $q$-subgroup $Q$. Furthermore, $Q \lhd G$ and $Q$ is cylic with order $q$.\retTwo

% Next, let $P$ by a Sylow $p$-subgroup. Then because $Q \lhd G$, we have that $PQ < G$. Also, $|P \cap Q| \dividesDeprecated \gcd(p, q) = 1$. So, $P \cap Q = \{1\}$ and from there it follows that $|PQ| = pq = |G|$. So $G / Q = PQ / Q \cong P / (P \cap Q) \cong P$.
