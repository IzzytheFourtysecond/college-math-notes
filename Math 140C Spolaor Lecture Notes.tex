% Note for any github stalkers. I am currently in the process
% of learning LaTeX. I don't know what I'm doing yet. Sorry
% if my code absolutely sucks.

\documentclass{book}

\usepackage{fontspec} % used to import Calibri
\usepackage{anyfontsize} % used to adjust font size

% needed for inch and other length measurements
% to be recognized
\usepackage{calc}

% for colors and text effects as is hopefully obvious
\usepackage[dvipsnames]{xcolor}
\usepackage{soul}

% control over margins
\usepackage[margin=1in]{geometry}
\usepackage[strict]{changepage}

\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{bm}

\usepackage[scr=rsfs, scrscaled=.96]{mathalpha}

\usepackage{amssymb} % originally imported to get the proof square
\usepackage{xfrac}
\usepackage[overcommands]{overarrows} % Get my preferred vector arrows...
\usepackage{relsize}

% Just am using this to get a dashed line in a table...
% Also you apparently want this to be inactive if you aren't
% using it because it slows compilation.
\usepackage{arydshln} \ADLinactivate 
\newenvironment{allowTableDashes}{\ADLactivate}{\ADLinactivate}

\usepackage{graphicx}
\graphicspath{{./158_Images/}}

\usepackage{tikz}
   \usetikzlibrary{arrows.meta}
   \usetikzlibrary{graphs, graphs.standard}

\usepackage{quiver} %commutative diagrams


\newfontfamily{\calibri}{Calibri}
\setlength{\parindent}{0pt}
\definecolor{RawerSienna}{HTML}{945D27}

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%Arrow Commands:

% Thank you Bernard, gernot, and Sigur who I copied this from:
% https://tex.stackexchange.com/questions/364096/command-for-longhookrightarrow
\newcommand{\hooklongrightarrow}{\lhook\joinrel\longrightarrow}
\newcommand{\hooklongleftarrow}{\longleftarrow\joinrel\rhook}
\newcommand{\hookxlongrightarrow}[2][]{\lhook\joinrel\xrightarrow[#1]{#2}}
\newcommand{\hookxlongleftarrow}[2][]{\xleftarrow[#1]{#2}\joinrel\rhook}

% Thank you egreg who I copied from:
% https://tex.stackexchange.com/questions/260554/two-headed-version-of-xrightarrow
\newcommand{\longrightarrowdbl}{\longrightarrow\mathrel{\mkern-14mu}\rightarrow}
\newcommand{\longleftarrowdbl}{\leftarrow\mathrel{\mkern-14mu}\longleftarrow}

\newcommand{\xrightarrowdbl}[2][]{%
  \xrightarrow[#1]{#2}\mathrel{\mkern-14mu}\rightarrow
}
\newcommand{\xleftarrowdbl}[2][]{%
  \leftarrow\mathrel{\mkern-14mu}\xleftarrow[#1]{#2}
}

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newcommand{\hOne}{%
   \color{Black}%
   \fontsize{14}{16}\selectfont%
}
\newcommand{\hTwo}{%
   \color{MidnightBlue}%
   \fontsize{13}{15}\selectfont%
}
\newcommand{\hThree}{%
   \color{PineGreen!85!Orange}
   \fontsize{13}{15}\selectfont%
}
\newcommand{\hFour}{%
   \color{Cerulean}
   \fontsize{12}{14}\selectfont%
}
\newcommand{\myComment}{%
   \color{RawerSienna}%
   \fontsize{12}{14}\selectfont%
}
% \newcommand{\pracOne}{
%    \color{BrickRed}%
%    \fontsize{13}{15}\selectfont%
% }
\newcommand{\teachComment}{
   \color{Orange}%
   \fontsize{12}{14}\selectfont%
}
\newcommand{\exOne}{%
   \color{Purple}%
   \fontsize{14}{16}\selectfont%
}
\newcommand{\exTwo}{%
   \color{RedViolet}%
   \fontsize{13}{15}\selectfont%
}
\newcommand{\exP}{%
   \color{VioletRed}%
   \fontsize{12}{14}\selectfont%
}
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newcommand{\cyPen}[1]{{\vphantom{.}\color{Cerulean}#1}}

\newenvironment{myIndent}{%
   \begin{adjustwidth}{2.5em}{0em}%
}{%
   \end{adjustwidth}%
}

\newenvironment{myDindent}{%
   \begin{adjustwidth}{5em}{0em}%
}{%
   \end{adjustwidth}%
}

\newenvironment{myTindent}{%
   \begin{adjustwidth}{7.5em}{0em}%
}{%
   \end{adjustwidth}%
}

\newenvironment{myConstrict}{%
   \begin{adjustwidth}{2.5em}{2.5em}%
}{%
   \end{adjustwidth}%
}

\newcommand{\udefine}[1]{{%
   \setulcolor{Red}%
   \setul{0.14em}{0.07em}%
   \ul{#1}%
}}

\newcommand{\uuline}[2][.]{%
{\vphantom{a}\color{#1}%
\rlap{\rule[-0.18em]{\widthof{#2}}{0.06em}}%
\rlap{\rule[-0.32em]{\widthof{#2}}{0.06em}}}%
#2}

\newcommand*{\markDate}[1]{%
   {\huge \color{Black} \textbf{#1} \newline}%
}

\newcommand{\pprime}{{\prime\prime}}
\newcommand{\suchthat}{ \hspace{0.5em}s.t.\hspace{0.5em}}
\newcommand{\rea}[1]{\mathrm{Re}(#1)}
\newcommand{\ima}[1]{\mathrm{Im}(#1)}
\newcommand{\comp}{\mathsf{C}}
\newcommand{\myHS}{ \hspace{0.5em}}
\newcommand{\diam}[1]{\mathrm{diam}(#1)}
\newcommand{\domain}[1]{\mathrm{dom}(#1)}
\newcommand{\mySpan}{\mathrm{span}}
\newcommand{\myDim}[1]{\mathrm{dim}(#1)}

\newcommand{\rank}[1]{\mathrm{rk}(#1)}
\newcommand{\nullity}[1]{\mathrm{null}(#1)}
\newcommand{\rangeSp}[1]{\mathscr{R}(#1)}
\newcommand{\nullSp}[1]{\mathscr{N}(#1)}


\newcounter{PropNumber}
\setcounter{PropNumber}{82}
\newcommand{\propCount}[1][1]{%
   \addtocounter{PropNumber}{#1}%
   \thePropNumber%
}
\newcounter{SubPropNumber}
\newcommand{\subPropCount}[1][1]{%
   \addtocounter{SubPropNumber}{1}%
   \theSubPropNumber%
}
\newcommand{\resetSubPropCount}{%
   \setcounter{SubPropNumber}{0}%
}

\newcommand{\myId}{\mathrm{Id}}
\newcommand{\myIm}{\mathrm{im}}
\newcommand{\myObj}{\mathrm{Obj}}
\newcommand{\myHom}{\mathrm{Hom}}
\newcommand{\myEnd}{\mathrm{End}}
\newcommand{\myAut}{\mathrm{Aut}}

\newcommand{\mcateg}[1]{{\bm{\mathsf{#1}}}}

% Thank you Gonzalo Medina and Moriambar who wrote this on stack exchange:
%https://tex.stackexchange.com/questions/74125/how-do-i-put-text-over-symbols%
\newcommand{\myequiv}[1]{\stackrel{\mathclap{\mbox{\footnotesize{$#1$}}}}{\equiv}}

% Thank you chs who wrote this on stack exchange:
%https://tex.stackexchange.com/questions/89821/how-to-draw-a-solid-colored-circle%
\newcommand{\filledcirc}[1][.]{\ensuremath{\hspace{0.05em}{\color{#1}\bullet}\mathllap{\circ}\hspace{0.05em}}}

%Thank you blerbl who wrote this on stack exchange:
%https://tex.stackexchange.com/questions/25348/latex-symbol-for-does-not-divide
\newcommand{\ndiv}{\hspace{-0.3em}\not|\hspace{0.35em}}

\newcommand{\mySepOne}[1][.]{%
   {\noindent\color{#1}{\rule{6.5in}{1mm}}}\\%
}
\newcommand{\mySepTwo}[1][.]{%
   {\noindent\color{#1}{\rule{6.5in}{0.5mm}}}\\%
}

\newenvironment{myClosureOne}[2][.]{%
   \color{#1}%
   \begin{tabular}{|p{#2in}|} \hline \\%
}{%
   \\ \hline \end{tabular}%
}

\newcommand{\fillInBlank}[2][.]{{%
   \color{#1}%
   \rule[-0.12em]{#2em}{0.06em}\rule[-0.12em]{#2em}{0.06em}%
   \rule[-0.12em]{#2em}{0.06em}
}}

\newcommand{\retTwo}{\hfill\bigbreak}

\newcounter{LectureNumber}
\newcommand*{\markLecture}[1]{%
   \stepcounter{LectureNumber}%
   {\huge \color{Black} \textbf{Lecture \theLectureNumber: #1} \newline}%
}

\newcommand{\myVS}{\vphantom{$\int_a^b$}}

% Overarrow stuff:
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\NewOverArrowCommand{myVector}{%
   start = {{\smallermathstyle\relbar}},
   middle = {{\smallermathstyle\relbareda}},
   end={{\rightharpoonup}}, space before arrow=0.15em,
   space after arrow=-0.045em,
}

\NewOverArrowCommand{myBar}{%
   start = {{\relbar}},
   middle = {{\relbar}},
   end={{\relbar}}, space before arrow=0.15em,
   space after arrow=-0.025em,
}

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newcommand{\mVec}[1]{\myVector{#1}}
\newcommand{\mVecAst}[1]{\myVector*{#1}}
\newcommand{\mMat}[1]{\mathbf{#1}}

\title{Math 140C Lecture Notes (Professor: Luca Spolaor)}
\author{Isabelle Mills}

\begin{document}
\maketitle{}
\setul{0.14em}{0.07em}
\calibri

\hOne
\markLecture{4/2/2024}

A set $X \subseteq \mathbb{R}^n$ where $X \neq \emptyset$ is a \udefine{vector space} if:\\ [-20pt]
\begin{itemize}
   \item $\mVec{x}, \mVec{y} \in X \Longrightarrow \mVec{x} + \mVec{y} \in X$\\ [-20pt]
   \item $\mVec{x} \in X$ and $c \in \mathbb{R} \Longrightarrow c\mVec{x} \in X$.\retTwo
\end{itemize}

If $\phi = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\} \subset \mathbb{R}^{n}$, then we define:

{\centering $\mySpan\hspace{0.2em}\phi = \mySpan\{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\} = \left\{ c_1\mVecAst{x}_1 + \ldots + c_k\mVecAst{x}_k \mid c_1, \ldots, c_k \in \mathbb{R} \right\}$.\retTwo\par}

If $E \subseteq \mathbb{R}^n$ and $E = \mySpan\hspace{0.2em}\phi$, then we say $\phi$ \udefine{generates} $E$.\retTwo

{\begin{myIndent}\exOne
   Note that $\mySpan\{\mVecAst{x}_1, \ldots, \mVecAst{x}_2\}$ forms a vector space (this is trivial to check).\retTwo
\end{myIndent}}

$\{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\} \subseteq \mathbb{R}^n$ is called \udefine{linearly independent} if:

{\centering $\sum\limits_{i=1}^{k}{c_i\mVecAst{x}_i} = 0 \Longrightarrow \forall i \in \{1, \ldots, k\},\myHS c_i = 0$.\retTwo\par}

If the above implication does not hold, then we call the set \udefine{linearly dependent}.\retTwo

If $X \subseteq \mathbb{R}^n$ is a vector space, then we define the \udefine{dimension} of $X$ as: 

{\centering ${\myDim{X} = \sup\{k \in \mathbb{N} \cup \{0\} \mid \exists \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}\subset X \text{ which is linearly independent}\}\text{.}}$\par}

{\begin{myDindent} \teachComment
   Also, we define any set containing $\mVec{0}$ to be automatically linearly dependent.\\
   This includes the singleton: $\{\mVec{0}\}$.\retTwo
\end{myDindent}}

$Q = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$ is a \udefine{basis} for $X$ if:\\ [-18pt]
\begin{itemize}
   \item $Q$ is linearly independent.\\ [-20pt]
   \item $\mySpan\hspace{0.2em}Q = X$\retTwo
\end{itemize}

\begin{myIndent}\exOne
   As an example of a basis, for $\mathbb{R}^n$ we define \udefine{the standard basis} as the set\\ $\{e_1, e_2, \ldots, e_n\}$ where $e_i$ is the vector whose $i$th element is $1$ and whose\\ other elements are $0$. It is pretty trivial to check that this set is in fact a\\ basis of $\mathbb{R}^n$.\retTwo
\end{myIndent}

\mySepTwo

{\begin{myIndent} \hTwo
   \uuline{Proposition}: If $B = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$ is a basis of a vector space $X$, then:\\ [-14pt]
   \begin{enumerate}
      \item $\forall \mVec{v} \in X,\myHS c_1, \ldots, c_k \in \mathbb{R} \suchthat \mVec{v} = \sum\limits_{i=1}^{k}{c_i\mVecAst{x}_i}$
      
      {\begin{myIndent} \hThree
         This is true because $X = \mySpan\hspace{0.2em}B$. So by definition of a span, $\mVec{v}$ can\\ be expressed as a linear combination of the vectors of $B$.
      \end{myIndent}}

      \newpage

      \item The $c_i$ such that $\mVec{v} = \sum\limits_{i=1}^{k}{c_i\mVecAst{x}_i}$ are unique.
      
      {\begin{myIndent}\hThree
         Suppose that $\mVec{v} = \sum c_i\mVecAst{x}_i = \sum \alpha_i\mVecAst{x}_i$. Then $\mVec{0} = \sum(c_i - \alpha_i)\mVecAst{x}_i$.\\ Then since $\{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$ are linearly independent, we know for all $i$\\ that $c_i - \alpha_i = 0$. Hence, $c_i = \alpha_i$ for each $i$.\\
      \end{myIndent}}
   \end{enumerate}

   \uuline{Theorem 9.2}: Let $k \in \mathbb{N} \cup \{0\}$. If $X = \mySpan\{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$, then $\myDim{X} \leq k$.\retTwo
   
   {\begin{myIndent}\hThree
      Proof:\\
      Suppose for the sake of contradiction that for any $m \in \mathbb{Z}^+$ , there exists\\ a linearly independent set $Q = \{\mVecAst{y}_1, \ldots, \mVecAst{y}_{k+m}\} \subset X$ which spans $X$.\\ Then, define $S_0 = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$ and note that $S_0$ spans $X$.\retTwo

      Now by induction, assume for $i \in \{0, 1, \ldots, k-1\}$, that $S_i$ contains the\\ first $i$ vectors of $Q$ in addition to $k - i$ vectors of $S_0$, and that $\mySpan\hspace{0.2em}{S_i} = X$.\\
      Then since $S_i$ spans $X$, we know that $y_{i+1} \in X$ is in the span of $S_i$. So,\\ letting $\mVecAst{x}_{n_1}, \ldots, \mVecAst{x}_{n_{k-i}}$ be the elements from $S_0$ in $S_i$, we know that there\\ exists scalars $a_1, \ldots, a_{i+1}, b_1, \ldots, b_{k-i} \in \mathbb{R}$ where $a_{i+1} = 1$ such that:

      {\centering $\sum\limits_{j=1}^{i+1}a_j\mVecAst{y}_j +  \sum\limits_{j=1}^{k-i}b_j\mVecAst{x}_{n_j} = \mVec{0}$ \retTwo\par}

      If all $b_j = 0$, then we have a contradiction. This is because $\{\mVecAst{y}_1, \ldots, \mVecAst{y}_{k+1}\}$\\ is assumed to be linearly independent. So, having all $b_j = 0$ implies that:
      
      {\centering $\sum\limits_{j=1}^{i+1}a_j\mVecAst{y}_j = \sum\limits_{j=1}^{i+1}a_j\mVecAst{y}_j + \sum\limits_{j=i+2}^{k+1}0\cdot\mVecAst{y}_j = \mVec{0}$\par}
      
      In turn this means that all $a_j = 0$, which contradicts that $a_{i+1} = 1$.\retTwo

      So, not all $b_j = 0$. This means that for some $j$ we must have that $\mVecAst{x}_{n_{j}}$ is in\\ the span of $(S_i \setminus \{\mVecAst{x}_{n_{j}}\}) \cup \{\mVecAst{y}_{i+1}\}$. Call this set $S_{i+1}$. Clearly, $S_{i+1}$ contains\\ the first $i + 1$ vectors of $Q$. Also:
      
      {\centering$\mySpan\hspace{0.2em}{S_{i+1}} = \mySpan\hspace{0.2em}(S_{i} \cup \{\mVecAst{y}_{i+1}\}) = \mySpan\hspace{0.2em}S_i = X$.\retTwo\par}

      So $S_{i+1}$ satisfies the same conditions $S_i$ did.\retTwo

      Now we get to the contradiction. Using the above reasoning, we will\\ eventually construct $S_k = \{\mVecAst{y}_1, \ldots, \mVecAst{y}_k\}$ which still spans $X$. However, since\\ $\mVecAst{y}_{k+1} \in X$, that means that $\mVecAst{y}_{k+1}$ equals some linear combination of the other\\ $\mVec{y}$ in $Q$. This contradicts that $Q$ is linearly independent. $\blacksquare$\retTwo
   \end{myIndent}}

   \uuline{Corollary}: If $B = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$ is a basis for $X$, then $\myDim{X} = k$.\retTwo

   {\begin{myIndent}\hThree
      Proof:\\
      Since $B$ is linearly independent, by definition $\myDim{X} \geq k$. Meanwhile,\\ since $B$ spans $X$, we know by the above theorem that $\myDim{X} \leq k$.\\ So $\myDim{X} = k$.
   \end{myIndent}}

   \newpage

   \uuline{Theorem 9.3}: Suppose $X$ is a vector space and $\myDim{X} = n$. Then:
   \begin{itemize}
      \item[(A)] For $E = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_n\} \subset X$, we have that $X = \mySpan\hspace{0.2em}E$ if and only if $E$ is\\ linearly independent.
      
      {\begin{myIndent}\hThree
         Proof:\\
         First, assume $E$ is linearly independent. Then, note that for any\\ $\mVecAst{y} \in X$, we must have that $E \cup \{\mVecAst{y}^{\phantom{|}}\}$ is linearly dependent because\\ $|E \cup \{\mVecAst{y}^{\phantom{|}}\}| > \myDim{X}$. So, there exists $c_1, \ldots, c_n, c_{n+1} \in \mathbb{R}$ such\\ that at least one $c_i$ is nonzero and:

         {\centering $\sum\limits_{i=1}^n{c_i\mVecAst{x}_i} + c_{n+1}\mVecAst{y} = \mVec{0}$\retTwo\par}

         Now if $c_{n+1} = 0$, we have a contradiction because $E$ is linearly\\ independent. So, we conclude that $c_{n+1} \neq 0$. Thus, by rearranging\\ terms we can express $y$ as a linear combination of the vectors of $E$.\\ Therefore, $\mySpan\hspace{0.2em}E = X$ since $y$ can be any vector in $X$.\retTwo

         Secondly, assume $E$ is not linearly independent. Then for some $\mVecAst{x}_i \in E$, we have that $\mySpan\hspace{0.2em}E = \mySpan(E \setminus \{\mVecAst{x}_i\})$. However, $|E \setminus \{\mVecAst{x}_i\}| = n-1$.\\ So if $X = \mySpan\hspace{0.2em}E$, then $\myDim{X} \leq |E \setminus \{\mVecAst{x}_i\}| = n-1$, which contradicts\\ our assumption that $\myDim{X} = n$. Hence, $X \neq \mySpan\hspace{0.2em}E$.\\
      \end{myIndent}}

      \item[(B)] $X$ has a basis and every basis of $X$ consists of $n$ vectors.
      {\begin{myIndent}\hThree
         Proof:\\
         By the definition of $\myDim{X}$, we know that there exists a linearly\\ independent set of $n$ vectors. By the previous part of this theorem,\\ we also know that that set spans $X$. So, it is a basis of $X$. Meanwhile, by\\ the corollary to theorem 9.2, we know that the number of vectors in a\\ basis of $X$ equals the dimension of $X$. Hence, all bases of $X$ must have $n$ vectors.\\
      \end{myIndent}}

      \item[(C)] If $1 \leq m \leq n$ and $\{\mVecAst{y}_1, \ldots, \mVecAst{y}_m\} \subset X$ is linearly independent, then $X$ has a\\ basis that contains $\mVecAst{y}_1, \ldots, \mVecAst{y}_m$.
      {\begin{myIndent}\hThree
         Proof:\\
         Let $S_0 = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_n\}$ be a basis of $X$ and $Q = \{\mVecAst{y}_1, \ldots, \mVecAst{y}_m\}$. Then\\ by the same induction which we used to prove theorem 9.2, we can\\ construct a basis:  $S_m$, of $X$ which contains $\mVecAst{y}_1, \ldots, \mVecAst{y}_m$.
      \end{myIndent}}
   \end{itemize}
\end{myIndent}}

\mySepTwo

Let $X$ and $Y$ be vector spaces. A map $\mMat{A}: X \longrightarrow Y$ is \udefine{linear} if\\ $\mMat{A}(c_1\mVecAst{x}_1 + c_2\mVecAst{x}_2) = c_1\mMat{A}(\mVecAst{x}_1) + c_2\mMat{A}(\mVecAst{x}_2)$ for all $\mVecAst{x}_1, \mVecAst{x}_2 \in X$ and $c_1, c_2 \in \mathbb{R}$.

\newpage

{\begin{myIndent}\exOne
   Observations:
   \begin{enumerate}
      \item A linear map sends $\mVec{0}$ to $\mVec{0}$. This is because: 
      
      {\centering$\mMat{A}(\mVec{0}) = \mMat{A}(\mVec{v} - \mVec{v}) = \mMat{A}(\mVec{v}) - \mMat{A}(\mVec{v}) = \mVec{0}$.\retTwo\par}

      \item If $\mMat{A}: X \longrightarrow Y$ is a linear map and $B = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$ is a basis of $X$,\\
      then $\mMat{A}\left(\sum\limits_{i=1}^k(c_i\mVecAst{x}_i)\right) = \sum\limits_{i=1}^kc_i\mMat{A}(\mVecAst{x}_i)$ for all $c_1, \ldots, c_k \in \mathbb{R}$.\retTwo
   \end{enumerate}
\end{myIndent}}

Given two vector spaces $X$ and $Y$, we define $L(X, Y)$ to be the set of all linear transformations from $X$ into $Y$. Also, we shall abbreviate $L(X, X)$ as $L(X)$.\retTwo

$\nullSp{\mMat{A}} = $ "\udefine{null space / kernel} of $\mMat{A}$" $ = \{\mVec{x} \in X \mid \mMat{A}(\mVec{x}) = \mVec{0}\}$.\retTwo

$\rangeSp{\mMat{A}} =$ "range of $\mMat{A}$" $ = \{\mVec{y} \in Y \mid \exists \mVec{x} \in X \suchthat \mMat{A}\mVec{x} = \mVec{y}\}$.\retTwo

{\begin{myIndent}\hTwo
   \uuline{Proposition}: For any linear map $\mMat{A}: X \longrightarrow Y$, $\nullSp{\mMat{A}}$ and $\rangeSp{\mMat{A}}$ are vector spaces.\\
   \begin{myIndent}\hThree
      Proof:
      \begin{itemize}
         \item Assume $\mVecAst{x}_1, \mVecAst{x}_2 \in \nullSp{\mMat{A}} \subset X$ and $c \in \mathbb{R}$. Then:
         \begin{itemize}
            \item[$\circ$] $\mMat{A}(\mVecAst{x}_1 + \mVecAst{x}_2) = \mMat{A}(\mVecAst{x}_1) + \mMat{A}(\mVecAst{x}_2) = \mVec{0} + \mVec{0} = \mVec{0}$, which means\\ that $\mVecAst{x}_1 + \mVecAst{x}_2 \in \nullSp{\mMat{A}}$.
            
            \item[$\circ$] $\mMat{A}(c\mVecAst{x}_1) = c\mMat{A}(\mVecAst{x}_1) = c\mVec{0} = \mVec{0}$. So $c\mVecAst{x}_1 \in \nullSp{\mMat{A}}$.
         \end{itemize}
         This shows that $\nullSp{\mMat{A}}$ is a vector space.\retTwo

         \item Assume $\mVecAst{y}_1, \mVecAst{y}_2 \in \rangeSp{\mMat{A}} \subset Y$ and $c \in \mathbb{R}$. Then:
         \begin{itemize}
            \item[$\circ$] We know there exists $\mVecAst{x}_1, \mVecAst{x}_2 \in X$ such that $\mMat{A}(\mVecAst{x}_1) = \mVecAst{y}_1$ and\\ $\mMat{A}(\mVecAst{x}_2) = \mVecAst{y}_2$. In turn, $\mMat{A}(\mVecAst{x}_1 + \mVecAst{x}_2) = \mMat{A}(\mVecAst{x}_1) + \mMat{A}(\mVecAst{x}_2) = \mVecAst{y}_1 + \mVecAst{y}_2$.\\
            So $\mVecAst{y}_1 + \mVecAst{y}_2 \in \rangeSp{\mMat{A}}$.

            \item[$\circ$] Now continue letting $\mVecAst{x}_1 \in X$ be a vector such that $\mMat{A}(\mVecAst{x}_1) = \mVecAst{y}_1$.\\ Then $\mMat{A}(c\mVecAst{x}_1) = c\mMat{A}(\mVecAst{x}_1) = c\mVecAst{y}_1$. So $c\mVecAst{y}_1 \in \rangeSp{\mMat{A}}$.
         \end{itemize}
         This shows that $\rangeSp{\mMat{A}}$ is a vector space. \retTwo
      \end{itemize}
   \end{myIndent}
\end{myIndent}}

$\rank{\mMat{A}} = $ "\udefine{rank} of $\mMat{A}$" $ = \myDim{\rangeSp{\mMat{A}}}$.\retTwo

$\nullity{\mMat{A}} = $ "\udefine{nullity} of $\mMat{A}$" $ = \myDim{\nullSp{\mMat{A}}}$.\retTwo

{\begin{myIndent}\hTwo
   \uuline{Rank-Nullity Theorem}: Given any $\mMat{A} \in L(X, Y)$, we have that
   
   {\centering $\myDim{X} = \rank{\mMat{A}} + \nullity{\mMat{A}}$.\retTwo\par}

   {\begin{myIndent} \hThree
      Proof:\\
      Let $\myDim{X} = n$.\retTwo

      $\nullSp{\mMat{A}} \subseteq X$ is a vector space. So pick a basis $\{\mVecAst{v}_1, \ldots, \mVecAst{v}_k\}$ for $\nullSp{\mMat{A}}$ where\\ $k = \nullity{\mMat{A}} \leq \myDim{X}$. Then by theorem 9.3, choose $\mVecAst{w}_1, \ldots, \mVecAst{w}_{m-k}$ such\\ that $\{\mVecAst{v}_1, \ldots, \mVecAst{v}_k, \mVecAst{w}_1, \ldots, \mVecAst{w}_{n-k}\}$ is a basis of $X$. Note that $\myDim{X} = n$.

      \newpage

      Claim: $B = \{\mMat{A}(\mVecAst{w}_1), \ldots, \mMat{A}(\mVecAst{w}_{n-k})\}$ is a basis of $\rangeSp{\mMat{A}}$.
      \begin{itemize}
         \item $\mMat{A}(\mVecAst{v}_i) = \mVec{0}$ for all $i \in \{1, \ldots, k\}$. So:\\
         
         \begin{tabular}{l}
            $ \rangeSp{\mMat{A}} = \mySpan\{\mMat{A}(\mVecAst{v}_1), \ldots,\mMat{A}(\mVecAst{v}_{k}), \mMat{A}(\mVecAst{w}_1), \ldots, \mMat{A}(\mVecAst{w}_{n-k})\}$\\
            $\phantom{\rangeSp{\mMat{A}}} = \mySpan\{\mMat{A}(\mVecAst{w}_1), \ldots, \mMat{A}(\mVecAst{w}_{n-k})\} = \mySpan\hspace{0.2em}B$
         \end{tabular}\retTwo

         \item $B$ is linearly independent.\\
         To see this, note that: $ \sum\limits_{i=1}^{n-k}\left(c_i\mMat{A}(\mVecAst{w}_i)\right) = \mVec{0} \Longrightarrow \mMat{A}\left(\sum\limits_{i=1}^{n-k}c_i\mVecAst{w}_i\right) = \mVec{0}$\retTwo

         Since we picked each $\mVecAst{w}_1, \ldots, \mVecAst{w}_{n-k} \in B$ so that they were not in\\ $\nullSp{A}$,  we know that any vector in the span of $B$ is not mapped to $0$\\ by $\mMat{A}$ unless it is the zero vector. So

         {\center $\sum\limits_{i=1}^{n-k}c_i\mVecAst{w}_i = \mVec{0}$\retTwo\par}

         And since all the $\mVecAst{w}_i$ are linearly independent, all constants $c_i$ equal $0$.\retTwo
      \end{itemize}
      So $\rank{\mMat{A}} = n - k = \myDim{X} - \nullity{\mMat{A}}$.\retTwo
   \end{myIndent}}
\end{myIndent}}

\markLecture{4/4/2024}
   
{\begin{myIndent}\hTwo
   \uuline{Proposition}: Given $\mMat{A} \in L(X, Y)$, then:
   \begin{itemize}
      \item $\mMat{A}$ is injective if and only if $\nullity{\mMat{A}} = 0$.
      
      {\begin{myIndent} \hThree
         Proof:\\
         ($\Longrightarrow$) If $\mMat{A}$ is injective, then since $\mMat{A}(\mVec{0}) = \mVec{0}$, we have that any vector\\ $\mVec{v} \neq \mVec{0}$ is not in $\nullSp{\mMat{A}}$. So $\nullSp{\mMat{A}} = \{\mVec{0}\}$, meaning $\nullity{\mMat{A}} = 0$.\retTwo

         ($\Longleftarrow$) If $\nullity{\mMat{A}} = 0$, then $\mMat{A}(\mVecAst{v}\phantom{|}) = \mVec{0} \Longrightarrow \mVec{v} = \mVec{0}$. So now assume\\ $ \mMat{A}(\mVec{v}\phantom{|}) = \mMat{A}(\mVec{u}\phantom{|})$. Then $\mMat{A}(\mVec{v} - \mVec{u}\phantom{|}) = \mVec{0}$, meaning $\mVec{v} = \mVec{u}$. Hence $\mMat{A}$\\ is injective.\\
      \end{myIndent}}

      \item $\mMat{A}$ is surjective if and only if $\rank{\mMat{A}} = \myDim{Y}$.
      
      {\begin{myIndent} \hThree
         Proof:\\
         ($\Longrightarrow$) If $\mMat{A}$ is surjective then $\rangeSp{\mMat{A}} = Y$. So we automatically have\\ that $\rank{\mMat{A}} = \myDim{Y}$\retTwo

         ($\Longleftarrow$) If $\rank{\mMat{A}} = \myDim{Y}$, then there exists a linearly independent set of\\ vectors $B \subset \rangeSp{\mMat{A}}$ containing $\myDim{Y}$ many vectors and spanning $\rangeSp{\mMat{A}}$.\\ Then by theorem 9.3, since $B \subset \rangeSp{\mMat{A}} \subseteq Y$, we know $\mySpan\hspace{.2em}{B} = Y$.\\ So, $\rangeSp{\mMat{A}} = Y$, meaning $\mMat{A}$ is surjective.
      \end{myIndent}}
   \end{itemize}


   \newpage

   \uuline{Corollary}: Let $\mMat{A} \in L(X)$. Then $\mMat{A}$ is bijective  if and only if $\nullity{\mMat{A}} = 0$.\retTwo
   
   {\begin{myIndent}\hThree
      Proof: (let $\mMat{A}: X \longrightarrow X$ be a linear map)\retTwo
      ($\Longrightarrow$) If $\mMat{A}$ is bijective, then automatically $\mMat{A}$ is injective. So $\nullity{\mMat{A}} = 0$ by\\ the previous proposition.\retTwo

      ($\Longleftarrow$) If $\nullity{\mMat{A}} = 0$, then by the rank-nullity theorem, we know that\\ $\rank{\mMat{A}} = \myDim{X}$. Thus $\mMat{A}$ is both injective and surjective, meaning $\mMat{A}$\\  is bijective.\retTwo
      
   \end{myIndent}}
\end{myIndent}}

For $\mMat{A} \in L(X)$, when $\nullity{\mMat{A}} = 0$, we call $\mMat{\mMat{A}}$ \udefine{invertible} and define $\mMat{A}^{-1}: X \longrightarrow X$\\ by $\mMat{A}^{-1}(\mMat{A}(\mVec{x})) = \mVec{x}$ for all $\mVec{x} \in X$.\retTwo

{\begin{myIndent}\exOne
   Because $\mMat{A}$ must be a bijective set function, we know that $\mMat{A}^{-1}$ must also be\\ a right-inverse of $\mMat{A}$, meaning $\mMat{A}(\mMat{A}^{-1}(\mVec{x})) = \mVec{x}$.\retTwo

   Additionally, consider any $\mVecAst{x}_1, \mVecAst{x}_2 \in X$. Then let $\mVecAst{x}^{\hphantom{|}\prime}_1 = \mMat{A}^{-1}(\mVecAst{x}_1)$ and\\ $\mVecAst{x}^{\hphantom{|}\prime}_2 = \mMat{A}^{-1}(\mVecAst{x}_2)$. Then since $\mMat{A}$ is a linear mapping, we know that for\\ any $c_1, c_2 \in \mathbb{R}$:

   {\centering $\mMat{A}(c_1\mVecAst{x}^{\hphantom{|}\prime}_1 + c_2\mVecAst{x}^{\hphantom{|}\prime}_2) = c_1\mMat{A}(\mMat{A}^{-1}(\mVecAst{x}_1)) + c_2\mMat{A}(\mMat{A}^{-1}(\mVecAst{x}_2)) = c_1\mVecAst{x}_1 + c_2\mVecAst{x}_2$\retTwo\par}

   So: $\mMat{A}^{-1}(c_1\mVecAst{x}_1 + c_2\mVecAst{x}_2) = c_1\mVecAst{x}^{\hphantom{|}\prime}_1 + c_2\mVecAst{x}^{\hphantom{|}\prime}_2 = c_1\mMat{A}^{-1}(\mVecAst{x}_1) + c_2\mMat{A}^{-1}(\mVecAst{x}_2)$. Hence,\\ we've shown that $\mMat{A}^{-1}$ is a linear mapping, meaning that $\mMat{A}^{-1} \in L(X)$.\retTwo 
\end{myIndent}}

Let $\mMat{A} \in L(X, Y)$ and $\mMat{B} \in L(Y, Z)$. Then we define $\mMat{BA}: X \longrightarrow Z$ by the rule\\ that $\mVec{x} \mapsto \mMat{B}(\mMat{A}(\mVec{x}))$.\retTwo

{\begin{myIndent}\exOne
   We can trivially show that $\mMat{B}\mMat{A}$ is a linear mapping. Consider any\\ $\mVecAst{x}_1, \mVecAst{x}_2 \in X$ and $c_1, c_2 \in \mathbb{R}$. Then:\\ [-6pt]
   \begin{center}
      \begin{tabular}{l}
         $\mMat{BA}(c_1\mVecAst{x}_1 + c_2\mVecAst{x}_2) = \mMat{B}(c_1\mMat{A}(\mVecAst{x}_1) + c_2\mMat{A}(\mVecAst{x}_2))$ \\ [2pt]
         $\phantom{\mMat{BA}(c_1\mVecAst{x}_1 + c_2\mVecAst{x}_2)} = c_1\mMat{B}(\mMat{A}(\mVecAst{x}_1)) + c_2\mMat{B}(\mMat{A}(\mVecAst{x}_2))$\\ [2pt]
         $\phantom{\mMat{BA}(c_1\mVecAst{x}_1 + c_2\mVecAst{x}_2)} = c_1\mMat{B}\mMat{A}(\mVecAst{x}_1) + c_2\mMat{B}\mMat{A}(\mVecAst{x}_2)$
      \end{tabular}\retTwo
   \end{center}

   This means that $\mMat{BA} \in L(X, Z)$.\retTwo
\end{myIndent}}

Let $\mMat{A}, \mMat{B} \in L(X, Y)$ and $c_1, c_2 \in \mathbb{R}$. Then we define $(c_1\mMat{A} + c_2\mMat{B}) : X \longrightarrow Y$ by the\\ rule: $\mVec{x} \mapsto c_1\mMat{A}(\mVec{x}) + c_2\mMat{B}(\mVec{x})$.\retTwo

{\begin{myIndent} \exOne
   It is even more trivial to show that $(c_1\mMat{A} + c_2\mMat{B})$ is a linear map.
\end{myIndent}}

\newpage

Let $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$. We define the \udefine{norm} of $\mMat{A}$ as: 

{\centering $\|\mMat{A}\| = \sup\left\{\|\mMat{A}(\mVec{x})\| \mid \mVec{x} \in \mathbb{R}^n \text{ and } \|\mVec{x}\| \leq 1\right\}$. \retTwo\par}

\mySepTwo

Throughout this section, we shall prove that $\|\cdot\| : L(\mathbb{R}^n, \mathbb{R}^m) \longrightarrow \mathbb{R}$ is\\ well-defined and fulfills the properties of a general norm function.\retTwo

{\begin{myIndent}\hTwo

   \uuline{Proposition}: If $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$, then $\|\mMat{A}\|$ exists and is finite.\\ [-6pt]
      
   {\begin{myIndent}\hThree
      Proof:\\
      Let $\{e_1, \ldots, e_n\}$ be the standard basis in $\mathbb{R}^n$. Then for any $\mVec{x} \in \mathbb{R}^n$, there\\ are unique $c_1, \ldots, c_n \in \mathbb{R}$ such that $\mVec{x} = c_1e_1 + \ldots + c_ne_n $.\retTwo

      Since we are working with the standard basis, we know: {\fontsize{12}{14}\selectfont$\|\mVec{x}\| = \sqrt{\sum\limits_{i=1}^nc_i^2}$}.\retTwo
      
      Thus, for $\|\mVec{x}\| \leq 1$, we must have that $|c_i| \leq 1$ for each $c_i$. This means:

      {\center\fontsize{12}{14}\selectfont $\|\mMat{A}(\mVec{x})\| = \left\|\sum\limits_{i=1}^n{c_i\mMat{A}(e_i)}\right\| \leq \sum\limits_{i=1}^n\|c_i\mMat{A}(e_i)\| = \sum\limits_{i=1}^n|c_i|\|\mMat{A}(e_i)\| \leq \sum\limits_{i=1}^n\|\mMat{A}(e_i)\|$ \retTwo\par}

      Importantly, we must have that {\fontsize{12}{14}\selectfont$\sum\limits_{i=1}^n\|\mMat{A}(e_i)\|$} is finite. Additionally, it is an\\ upper bound to the set: $\left\{\|\mMat{A}(\mVec{x})\| \mid \mVec{x} \in \mathbb{R}^n \text{ and } \|\mVec{x}\| \leq 1\right\} \subseteq \mathbb{R}$.\retTwo
      
      So, we showed that the above set is bounded above. Also, the above set\\ is nonempty because it must contain $\|\mVec{0}\| = 0$. Thus by the least upper bound\\ property of $\mathbb{R}$, we know that the supremum of this set exists in $\mathbb{R}$.\retTwo

      Hence, $\|\mMat{A}\|$ exists and is finite.\retTwo

      \begin{myTindent}\hFour
         Side note, the above proof also shows that $\|\mMat{A}\| \geq 0$.\retTwo 
      \end{myTindent}
   \end{myIndent}}

   \uuline{Lemma}: For $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$ and $\mVec{x} \in \mathbb{R}^n$, we have that $\|\mMat{A}(\mVec{x})\| \leq \|\mMat{A}\|\|\mVec{x}\|$.\\ [-6pt]

   {\begin{myIndent}\hThree
      Proof:\\
      Case 1: $\mVec{x} \neq \mVec{0}$.
      \begin{myIndent}
         Then since $\|\mVec{x}\| \neq 0$, we can say that:
         
         {\centering $\|\mMat{A}(\mVec{x})\| = \left\|\mMat{A}\left(\|\mVec{x}\|\frac{\mVec{x}}{\|\mVec{x}\|}\right)\right\| = \left\|\|\mVec{x}\|\mMat{A}\left(\frac{\mVec{x}}{\|\mVec{x}\|}\right)\right\| = \left\|\mMat{A}\left(\frac{\mVec{x}}{\|\mVec{x}\|}\right)\right\|\|\mVec{x}\|$\retTwo\par}

         Now $\frac{\mVec{x}}{\|\mVec{x}\|} \in \mathbb{R}^n$ and $\left\|\frac{\mVec{x}}{\|\mVec{x}\|}\right\| = 1$. So, $\left\|\mMat{A}\left(\frac{\mVec{x}}{\|\mVec{x}\|}\right)\right\|\|\mVec{x}\| \leq \|\mMat{A}\|\|\mVec{x}\|$\retTwo
      \end{myIndent}

      Case 2: $\mVec{x} = \mVec{0}$.
      \begin{myIndent}
         Then trivially $\|\mMat{A}(\mVec{x})\| = \|\mMat{A}(\mVec{0})\| = 0 = \|\mMat{A}\|\|\mVec{0}\| = \|\mMat{A}\|\|\mVec{x}\|$\retTwo
      \end{myIndent}
   \end{myIndent}}

   \newpage

   \uuline{Proposition}: If $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$, then $0 \leq \|\mMat{A}\|$. Also $\|\mMat{A}\| = 0$ if and only if $\mMat{A}$ is the unique function mapping all of $\mathbb{R}^n$ to $\mVec{0}$.\\ [-6pt]
   {\begin{myIndent}\hThree
      Proof:\\
      We already showed previously that $\|\mMat{A}\| \geq 0$. So, it now suffices to\\ show that $\|\mMat{A}\| = 0 \Longleftrightarrow \nullSp{\mMat{A}} = \mathbb{R}^n$.\retTwo

      ($\Longrightarrow$) Assume that $\nullSp{\mMat{A}} \neq \mathbb{R}^n$. Then there exists $\mVec{x} \in \mathbb{R}^n$ such\\ that $\mMat{A}(\mVec{x}) \neq \mVec{0}$. Since $\mVec{x}$ can't be $\mVec{0}$, consider the vector $\hat{x} = \frac{\mVec{x}}{\|\mVec{x}\|}$.\\ [-3pt] By the linearity of $\mMat{A}$, we know $\mMat{A}\left(\hat{x}\right) = \frac{1}{\|\mVec{x}\|}\mMat{A}(\mVec{x}) \neq \mVec{0}$. So,\\ $\|\mMat{A}(\hat{x})\| > 0$. But $\|\mMat{A}(\hat{x})\|$ is in the set that $\|\mMat{A}\|$ is a supremum of,\\ which means that $\|\mMat{A}\| \geq \|\mMat{A}(\hat{x})\| > 0$. Or in other words, $\|\mMat{A}\| \neq 0$.\retTwo

      ($\Longleftarrow$) Assume that $\nullSp{\mMat{A}} = \mathbb{R}^n$. Then,
      
      {\centering $\sup\left\{\|\mMat{A}(\mVec{x})\| \mid \mVec{x} \in \mathbb{R}^n \text{ and } \|\mVec{x}\| \leq 1\right\} = \sup\{0\} =  0$\retTwo\par}
   \end{myIndent}}

   \uuline{Corollary}: Given $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$, we have that $\mMat{A}$ is uniformly continuous.\\ [-6pt]
   
   {\begin{myIndent}\hThree
      Proof:\\
      Case 1: $\|\mMat{A}\| \neq 0$, meaning we can divide by $\|\mMat{A}\|$.
      \begin{myIndent}
         By the previous proposition, $\|\mMat{A}(\mVec{x}) - \mMat{A}(\mVec{y})\| \leq \|\mMat{A}\|\|\mVec{x} - \mVec{y}\|$ for\\ all $\mVec{x}, \mVec{y} \in \mathbb{R}^n$. Hence, for any $\varepsilon > 0$, if we make $\|\mVec{x} - \mVec{y}\| < \frac{\varepsilon}{\|\mMat{A}\|}$,\\ [-2pt] then $\|\mMat{A}(\mVec{x}) - \mMat{A}(\mVec{y})\| < \varepsilon$.\retTwo
      \end{myIndent}

      Case 2: $\|\mMat{A}\| = 0$.
      \begin{myIndent}
         Then $\mMat{A}$ is a constant function, making it automatically uniformly\\ continuous.\retTwo
      \end{myIndent}
   \end{myIndent}}

   \uuline{Subcorollary}: Given $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$. there exists $\mVec{x} \in \mathbb{R}^n$ with $\|\mVec{x}\| \leq 1$ such\\ that $\|\mMat{A}(\mVec{x})\| = \|\mMat{A}\|$.\\ [-6pt]

   {\begin{myIndent}\hThree
      Proof:\\
      Let $S = \{\mVec{x} \in \mathbb{R}^n \mid \|\mVec{x}\| \leq 1\}$ and consider the restriction $\mMat{A}|_{S}$.\retTwo

      Since $S$ is a closed and bounded subset of $\mathbb{R}^n$, we know that $S$ is compact\\ by the Heine-Borel theorem (see proposition 28 in Math 140A notes).\\ This combined with the fact that $\mMat{A}|_{S}$ is still continuous means that by the\\ extreme value theorem, there is $\mVec{x} \in S$ with: 
      
      {\center $\mMat{A}(\mVec{x}) = \mMat{A}|_{S}(\mVec{x}) = \sup\left\{\|\mMat{A}(\mVec{x})\| \mid \mVec{x} \in \mathbb{R}^n \text{ and } \|\mVec{x}\| \leq 1\right\}$.\retTwo\par}
   \end{myIndent}}

   \uuline{Proposition}: If $\mMat{A}, \mMat{B} \in L(\mathbb{R}^n, \mathbb{R}^m)$, then $\|\mMat{A} + \mMat{B}\| \leq \|\mMat{A}\| + \|\mMat{B}\|$.\\[-6pt]
   {\begin{myIndent}\hThree
      Proof:\\
      Let $\mVec{x} \in \mathbb{R}^n$ be a vector such that $\|\mVec{x}\| \leq 1$ and $\|\mMat{A}(\mVec{x})\| = \|\mMat{A}\|$. Then:

      \begin{center}
         \begin{tabular}{l}
            $\|\mMat{A} + \mMat{B}\| = \|(\mMat{A} + \mMat{B})(\mVec{x})\| = \|\mMat{A}(\mVec{x}) + \mMat{B}(\mVec{x})\|$ \\
            $\phantom{\|\mMat{A} + \mMat{B}\| = \|(\mMat{A} + \mMat{B})(\mVec{x})\|} \leq \|\mMat{A}(\mVec{x})\| + \|\mMat{B}(\mVec{x})\| \leq \|\mMat{A}\| + \|\mMat{B}\|$
         \end{tabular}
      \end{center}
   \end{myIndent}}

   \newpage

   \uuline{Proposition}: If $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$ and $c \in \mathbb{R}$, then $\|c\mMat{A}\| = |c|\|\mMat{A}\|$.\\ [-6pt]
   
   {\begin{myIndent} \hThree
      Proof:\\
      Pick $\mVec{x} \in \mathbb{R}^n$ satisfying $\|\mVec{x}\| \leq 1$ and $\|\mMat{A}(\mVec{x})\| = \|\mMat{A}\|$. Then:
      \begin{center}
         $|c|\|\mMat{A}\| = |c|\|\mMat{A}(\mVec{x})\| = \|c\mMat{A}(\mVec{x})\| = \|(c\mMat{A})(\mVec{x})\| \leq \|c\mMat{A}\|$.\retTwo
      \end{center}

      Next, pick $\mVec{y} \in \mathbb{R}^n$ satisfying $\|\mVec{y}\| \leq 1$ and $\|(c\mMat{A})(\mVec{x})\| = \|c\mMat{A}\|$. Then:
      \begin{center}
         $\|c\mMat{A}\| = \|(c\mMat{A})(\mVec{y})\| = \|c\mMat{A}(\mVec{y})\| = |c|\|\mMat{A}\mVec{y}\| \leq |c|\|\mMat{A}\|$.\retTwo
      \end{center}
   \end{myIndent}}
\end{myIndent}}

Specifically because of the four propositions above, we have shown that\\  $\|\cdot\| : L(\mathbb{R}^n, \mathbb{R}^m) \longrightarrow \mathbb{R}$ is well-defined and a valid norm. Consequently, by\\ defining $d(\mMat{A}, \mMat{B}) = \|\mMat{A} - \mMat{B}\|$ for all $\mMat{A}, \mMat{B} \in L(\mathbb{R}^n, \mathbb{R}^m)$, we naturally get that\\ $L(\mathbb{R}^n, \mathbb{R}^m)$ is a metric space.\\ [-6pt]

\begin{myIndent}\exOne
   Given any $\mMat{A}, \mMat{B}, \mMat{C} \in L(\mathbb{R}^n, \mathbb{R}^m)$, we have:
   \begin{itemize}
      \item $d(\mMat{A}, \mMat{B}) = \|\mMat{A} - \mMat{B}\| \geq 0$ with $d(\mMat{A}, \mMat{B}) = 0$
      {\begin{myIndent}\exTwo
         Also $d(\mMat{A}, \mMat{B}) = 0$ if and only if $\mMat{A} = \mMat{B}$.
      \end{myIndent}}
      \item $d(\mMat{A}, \mMat{B}) = \|\mMat{A} - \mMat{B}\| = |-1|\|\mMat{B} - \mMat{A}\| = d(\mMat{B}, \mMat{A})$
      \item $d(\mMat{A}, \mMat{C}) = \|\mMat{A} - \mMat{C}\| \leq \|\mMat{A} - \mMat{B}\| + \|\mMat{B} - \mMat{C}\| = d(\mMat{A}, \mMat{B}) + d(\mMat{B}, \mMat{C})$\retTwo
   \end{itemize}
\end{myIndent}

Before moving on, here is another corollary of the above statements.
{\begin{myIndent} \hTwo
   \uuline{Corollary}: If $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$ and $\mMat{B} \in L(\mathbb{R}^m, \mathbb{R}^k)$, then $\|\mMat{BA}\| \leq \|\mMat{B}\|\|\mMat{A}\|$.\\ [-6pt]

   {\begin{myIndent}\hThree
      Proof:\\
      Pick $\mVec{x} \in \mathbb{R}^n$ satisfying $\|\mVec{x}\| \leq 1$ and $\|(\mMat{BA})(\mVec{x})\| = \|\mMat{BA}\|$. Then:
      \begin{center}
         $\|\mMat{BA}\| = \|(\mMat{BA})(\mVec{x})\| = \|\mMat{B}(\mMat{A}(\mVec{x}))\| \leq \|\mMat{B}\|\|\mMat{A}(\mVec{x})\| \leq \|\mMat{B}\|\|\mMat{A}\|$.\retTwo
      \end{center}
   \end{myIndent}}
\end{myIndent}}

\mySepTwo


{\begin{myIndent} \hTwo
   \uuline{Theorem 9.8}: Let $\Omega \subset L(\mathbb{R}^n)$ be the set of all invertible linear mappings on $\mathbb{R}^n$.
   \begin{itemize}
      \item[(A)] If $\mMat{A} \in \Omega$, $\mMat{B} \in L(\mathbb{R}^n)$, and $\|\mMat{B} - \mMat{A}\| < \dfrac{1}{\|\mMat{A}^{-1}\|}$, then $\mMat{B} \in \Omega$.\\ [-10pt]
      {\begin{myIndent}\hThree
         Proof:\\
         Pick $\mVec{x} \in \mathbb{R}^n$ such that $\|\mVec{x}\| \leq 1$. Then:
         
         \begin{center}
            \begin{tabular}{l}
               $\|\mMat{A}(\mVec{x})\| = \|(\mMat{A} - \mMat{B} + \mMat{B})(\mVec{x})\|$\\
               $\phantom{\|\mMat{A}(\mVec{x})\|} \leq \|(\mMat{A} - \mMat{B})(\mVec{x})\| + \|\mMat{B}(\mVec{x})\|$\\
               $\phantom{\|\mMat{A}(\mVec{x})\|} \leq \|\mMat{A} - \mMat{B}\|\|\mVec{x}\| + \|\mMat{B}(\mVec{x})\| = \|\mMat{B} - \mMat{A}\|\|\mVec{x}\| + \|\mMat{B}(\mVec{x})\|$
            \end{tabular}\retTwo
         \end{center}

         Meanwhile, note that $\|\mMat{A}^{-1}\| \neq 0$. We know this because $\mMat{A}^{-1}$ must be\\ invertible (because $\nullSp{\mMat{A}^{-1}} = \{\mVec{0}\}$) and the one linear transformation\\ in $L(\mathbb{R}^n)$ with norm $0$ is not invertible. So:

         \begin{center}
            \begin{tabular}{l}
               $\frac{\|\mVec{x}\|}{\|\mMat{A}^{-1}\|} = \frac{\|\mMat{A}^{-1}\mMat{A}(\mVec{x})\|}{\|\mMat{A}^{-1}\|} \leq \frac{\|\mMat{A}^{-1}\|\|\mMat{A}(\mVec{x})\|}{\|\mMat{A}^{-1}\|} = \|\mMat{A}(\mVec{x})\|$
            \end{tabular}\retTwo
         \end{center}

         \newpage

         Hence, $\frac{\|\mVec{x}\|}{\|\mMat{A}^{-1}\|} \leq \|\mMat{B} - \mMat{A}\|\|\mVec{x}\| + \|\mMat{B}(\mVec{x})\|$. By rearranging terms, we get\\ this expression: $\left(\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\|\right)\|\mVec{x}\| \leq \|\mMat{B}(\mVec{x})\|$.\retTwo

         Now, note that if $\|\mMat{B}(\mVec{x})\| = 0$ but $\mVec{x} \neq \mVec{0}$, then we must have that:\\ $\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\| \leq 0$. Or in other words, $\|\mMat{B} - \mMat{A}\| \geq \frac{1}{\|\mMat{A}^{-1}\|}$. So,\\ if $\|\mMat{B} - \mMat{A}\| < \frac{1}{\|\mMat{A}^{-1}\|}$, then $\|\mMat{B}(\mVec{x})\| = 0$ only when $\mVec{x} = \mVec{0}$.\\ [2pt] Hence, $\nullity{\mMat{B}} = 0$ and $\mMat{B}$ is invertible.\retTwo
      \end{myIndent}}
      
      \item[(B)] $\Omega$ is an open subset of $L(\mathbb{R}^n)$, and the mapping over $\Omega$ with the rule:\\ $\mMat{A} \mapsto \mMat{A}^{-1}$, is continuous.\\ [-10pt]
      {\begin{myIndent}\hThree
         Proof:\\
         Firstly, by part A we know that for any $\mMat{A} \in \Omega$, if $r = \frac{1}{\|\mMat{A}^{-1}\|}$, then\\ [1pt] $B_r(\mMat{A}) \subseteq \Omega$. So, $\Omega$ is an open set in the metric space $L(\mathbb{R}^n)$.\retTwo

         Now let $\mMat{A}, \mMat{B} \in \Omega$ and recall from part A that:
         
         {\centering $\left(\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\|\right)\|\mVec{x}\| \leq \|\mMat{B}(\mVec{x})\|$.\retTwo\par}

         Since we know $\mMat{B}^{-1}$ exists, set $\mVec{x} = \mMat{B}^{-1}(\mVec{y})$. Then the above\\ [1pt] expression becomes: $\left(\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\|\right)\|\mMat{B}^{-1}(\mVec{y})\| \leq \|\mVec{y}\|$.\\ [1pt] Because we are interested in $\mMat{B}$ close to $\mMat{A}$, we can assume that\\ [4pt] $\|\mMat{B} - \mMat{A}\| < \frac{1}{\|\mMat{A}^{-1}\|}$. Thus it is safe to divide by $\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\|$.\\ [2pt] So, setting $\mVec{y} \in \mathbb{R}^n$ to be the vector satisfying $\|\mVec{y}\| \leq 1$ and\\ [4pt] $\|\mMat{B}^{-1}(\mVec{y})\| = \|\mMat{B}^{-1}\|$, we have that:

         {\center $ \|\mMat{B}^{-1}\| = \|\mMat{B}^{-1}(\mVec{y})\| \leq \frac{\|\mVec{y}\|}{\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\|} \leq \frac{1}{\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\|} = \frac{\|\mMat{A}^{-1}\|}{1 - \|\mMat{A}^{-1}\|\|\mMat{B} - \mMat{A}\|}$ \retTwo\par}

         \uuline{Lemma}: Given $\mMat{A} \in L(Z, W)$, $\myHS\mMat{B}, \mMat{C} \in L(Y, Z)$, and $\mMat{D} \in L(X, Y)$,\\ we have that $\mMat{A}(\mMat{B} + \mMat{C}) = \mMat{A}\mMat{B} + \mMat{A}\mMat{C}$ and $(\mMat{B} + \mMat{C})\mMat{D} = \mMat{B}\mMat{D} + \mMat{C}\mMat{D}$.
         {\begin{myIndent}\hFour
            Proof:
            \begin{itemize}
               \item[$\circ$] $\mMat{A}((\mMat{B} + \mMat{C})(\mVec{v})) = \mMat{A}(\mMat{B}(\mVec{v}) + \mMat{C}(\mVec{v})) = \mMat{A}(\mMat{B}(\mVec{v})) + \mMat{A}(\mMat{C}(\mVec{v}))$
               \item[$\circ$] $ (\mMat{B} + \mMat{C})(\mMat{D}(\mVec{v})) =  \mMat{B}(\mMat{D}(\mVec{v})) + \mMat{C}(\mMat{D}(\mVec{v}))$\\
            \end{itemize}
         \end{myIndent}}

         Based on the above lemma, we have that $\mMat{B}^{-1} - \mMat{A}^{-1} = \mMat{B}^{-1}(\mMat{A} - \mMat{B})\mMat{A}^{-1}$.\\ So:\\ [-26pt]
         \begin{center}
            \begin{tabular}{l}
               $0 \leq \|\mMat{B}^{-1} - \mMat{A}^{-1}\| = \|\mMat{B}^{-1}(\mMat{A} - \mMat{B})\mMat{A}^{-1}\|$ \\
               $\phantom{0 \leq \|\mMat{B}^{-1} - \mMat{A}^{-1}\|} \leq \|\mMat{B}^{-1}\|\|(\mMat{A} - \mMat{B})\|\|\mMat{A}^{-1}\| \leq \frac{\|\mMat{A}^{-1}\|^2}{1 - \|\mMat{A}^{-1}\|\|\mMat{B} - \mMat{A}\|}\|\mMat{B} - \mMat{A}\|$
            \end{tabular}
         \end{center}

         \newpage

         Finally, assume $\mMat{A} \in \Omega^\prime$. This is fine because the mapping is\\ automatically continuous at $\mMat{A}$ if $\mMat{A} \notin \Omega^\prime$. Then we have that:
         
         {\centering $\lim\limits_{\mMat{B} \rightarrow \mMat{A}}\left(\frac{\|\mMat{A}^{-1}\|^2}{1 - \|\mMat{A}^{-1}\|\|\mMat{B} - \mMat{A}\|}\|\mMat{B} - \mMat{A}\|\right) = \|\mMat{A}^{-1}\|^2 \cdot 0 = 0$.\retTwo\par}

         So, $0 \leq \lim\limits_{\vphantom{\int^{b}}\mMat{B} \rightarrow \mMat{A}}(\|\mMat{B}^{-1} - \mMat{A}^{-1}\|) \leq 0$.\retTwo
         
         This means that $d(\mMat{B}^{-1}, \hspace{0.2em} \mMat{A}^{-1}) = \|\mMat{B}^{-1} - \mMat{A}^{-1}\| \rightarrow 0$ as $\mMat{B} \rightarrow \mMat{A}$.\\ Or in other words:

         {\centering $\phantom{\blacksquare\text{.}}$ $\lim\limits_{\mMat{B} \rightarrow \mMat{A}}(\mMat{B}^{-1}) = \mMat{A}^{-1}$. $\blacksquare$ \retTwo\par}
      \end{myIndent}}
   \end{itemize}
\end{myIndent}}

\mySepTwo

\markLecture{4/9/2024}

Suppose $\{\mVecAst{x}_1, \ldots, \mVecAst{x}_n\}$ and $\{\mVecAst{y}_1, \ldots, \mVecAst{y}_m\}$ are bases of the vector spaces $X$ and\\ $Y$ respectively, and let $\mMat{A} \in L(X, Y)$. Then for each $j \in \{1, \ldots, n\}$, since\\ $\mMat{A}(\mVecAst{x}_j) \in Y$, there are unique coefficients $a_{i,j}$ such that:\\ [-10pt]

{\centering $\mMat{A}(\mVecAst{x}_j) = \sum\limits_{i=1}^m a_{i,j}\mVecAst{y}_i$\retTwo\par}

For convenience, we visualize these numbers in an \udefine{$m\times n$ matrix}:\\ [-8pt]

{\centering $[\mMat{A}] = 
\begin{bmatrix}
   a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
   a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
   \vdots & \vdots & \ddots & \vdots \\
   a_{m,1} & a_{m,2} & \cdots & a_{m,n}
\end{bmatrix} $\retTwo\par}

Note that for each $j \in \{1,\ldots,n\}$, we have that the $j$th column of $[\mMat{A}]$ gives\\ the coordinates of $\mMat{A}(\mVecAst{x}_j)$ with respect to the basis $\{\mVecAst{y}_1, \ldots, \mVecAst{y}_m\}$. Thus, we call\\ the vectors $\mMat{A}(\mVecAst{x}_j)$ the \udefine{column vectors} of $[\mMat{A}]$.\retTwo

{\begin{myIndent}\hTwo
   \uuline{Fact 1}: There is a one-to-one correspondence between the set of $m\times n$ real\\ matrices and $L(X, Y)$.\\ [-8pt]
   
   {\begin{myIndent}\hThree
      Take $\{\mVecAst{x}_1, \ldots, \mVecAst{x}_n\}$ and $\{\mVecAst{y}_1, \ldots, \mVecAst{y}_m\}$ to be the bases of the vector spaces\\ $X$ and $Y$ respectively. Then consider $\mMat{A} \in L(X, Y)$. Then we already saw\\ above how to construct a matrix $[\mMat{A}]$ from the linear mapping $\mMat{A}$.\retTwo

      Now observe if $\mVec{x} \in X$, then $\mVec{x} = \sum\limits_{j=1}^n c_j\mVecAst{x}_j$. Thus, because $\mMat{A}$ is linear:\\ [-6pt]

      {\centering\fontsize{12}{14}\selectfont 
      \begin{tabular}{l}
         $\mMat{A}(\mVec{x}) = \mMat{A}\left(\sum\limits_{j=1}^n c_j\mVecAst{x}_j\right) = \sum\limits_{j=1}^n c_j\mMat{A}(\mVecAst{x}_j)$ \\ [-5pt]
         $\phantom{\mMat{A}(\mVec{x}) = \mMat{A}\left(\sum\limits_{j=1}^n c_j\mVecAst{x}_j\right)} = \sum\limits_{j=1}^n c_j \left(\sum\limits_{i=1}^{m}a_{i,j}\mVecAst{y}_i\right) = \sum\limits_{i=1}^m\left(\sum\limits_{j=1}^n c_ja_{i,j}\right)\hspace{-0.2em}\mVecAst{y}_i$
      \end{tabular}\par}

      \newpage

      Thus, we have an equation for $\mMat{A}(\mVec{x})$ in terms of the coefficients of $[\mMat{A}]$.\\ Needless to say, if we were instead starting out with an $m \times n$ real\\ matrix $[\mMat{B}] \in \mathcal{M}_{m\times n}(\mathbb{R})$ with coefficients $b_{i,j}$, then we could define the\\ linear map $\mMat{B} \in (L(X, Y))$ given by the rule:

      {\centering $\mMat{B}(\mVec{x}) = \sum\limits_{i=1}^m\left(\sum\limits_{j=1}^n c_jb_{i,j}\right)\hspace{-0.2em}\mVecAst{y}_i$.\retTwo\par}

      
      \begin{myTindent}\teachComment
         Note that the linear mapping associated above with a matrix\\ $[\mMat{A}]$ is unique up to the bases for $X$ and $Y$ one is using.\retTwo
      \end{myTindent}
   \end{myIndent}}

   \uuline{Fact 2}: Let $\mMat{A} \in L(X, Y)$ and $\mMat{B} \in L(Y, Z)$. Also, use the bases $\{\mVecAst{x}_1, \ldots, \mVecAst{x}_n\}$\\ for $X$,  $\{\mVecAst{y}_1, \ldots, \mVecAst{y}_m\}$ for $Y$, and $\{\mVecAst{z}_1, \ldots, \mVecAst{z}_p\}$ for $Z$. Then for each $\mVecAst{x}_j$, there,\\ are unique coefficients $a_{i,j}$ making up $[\mMat{A}]$ such that:

   {\centering $\mMat{A}(\mVecAst{x}_j) = \sum\limits_{i=1}^m a_{i,j}\mVecAst{y}_i$ \retTwo\par}

   Similarly, for each $\mVec{y}_j$, there are unique coefficients $b_{k, i}$ making up $[\mMat{B}]$ such that:

   {\centering $\mMat{B}(\mVecAst{y}_i) = \sum\limits_{k=1}^p b_{k,i}\mVecAst{z}_k$ \retTwo\par}

   Therefore, for the linear map $\mMat{BA} \in L(X, Z)$, we have that:
   
   {\centering\begin{tabular}{l}
      $\mMat{B}(\mMat{A}(\mVec{x_j})) = \mMat{B}\left(\sum\limits_{i=1}^m a_{i,j}\mVecAst{y}_i\right) = \sum\limits_{i=1}^m a_{i,j}\mMat{B}(\mVecAst{y}_i)$ \\ [2pt]
      $\phantom{\mMat{B}(\mMat{A}(\mVec{x_j})) = \mMat{B}\left(\sum\limits_{i=1}^m a_{i,j}\mVecAst{y}_i\right)} = \sum\limits_{i=1}^m a_{i,j}\hspace{-0.25em}\left(\sum\limits_{k=1}^p b_{k,i}\mVecAst{z}_k\right) = \sum\limits_{k=1}^p\hspace{-0.1em}\left(\sum\limits_{i=1}^m(a_{i,j}b_{k,i})\right)\hspace{-0.25em}\mVecAst{z}_k$
   \end{tabular}\retTwo\par}

   Note that the coefficients generated by the map $\mMat{BA}$ for the matrix $[\mMat{BA}]$\\ match the coefficients of the matrix product: $[\mMat{B}][\mMat{A}]$. So, the typical rule for\\ multiplying the matrices $[\mMat{A}]$ and $[\mMat{B}]$ gives the matrix associated with the\\ composition of the linear map $\mMat{B}$ with the linear map $\mMat{A}$.
\end{myIndent}}

\mySepTwo

Since an $m\times n$ matrix can be thought of as a list of $m\cdot n$ numbers, the "natural" norm to equip $\mathcal{M}_{m\times n}(\mathbb{R})$ with is:\\ [-22pt]

{\centering $\|[\mMat{A}]\|_F = \left(\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n}(a_{i,j})^2\right)^{\frac{1}{2}}$\\[-2pt]\par}


{\begin{myTindent}\myComment
   Note on my notation:
   \begin{myIndent}
      Since I view $|\cdot|$ as having already been reserved for the absolute\\ value function, I am not going to use the same notation as Rudin\\ and my professor use for this matrix norm. Rather, because this\\ norm is also called the \ul{Frobenius norm}, I shall denote it by $\|\cdot\|_F$.\retTwo
   \end{myIndent}

   Also, this is a valid norm for the same reasons that the vector Euclidean\\ norm is a valid norm.
\end{myTindent}}

\newpage

{\begin{myIndent}\hTwo
   \uuline{Proposition}: If $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$ is a linear map and $[\mMat{A}]$ is the matrix generated\\ from $\mMat{A}$ using the standard bases for $\mathbb{R}^n$ and $\mathbb{R}^m$, then $\|\mMat{A}\| \leq \|[\mMat{A}]\|_F$.
\end{myIndent}}

\end{document}


% maybe add after this next section
Meanwhile, using the standard bases for $\mathbb{R}^n$ and $\mathbb{R}^m$, we can equip $\mathcal{M}_{m\times n}(\mathbb{R})$\\ with the norm assigning each $[\mMat{A}] \in \mathcal{M}_{m\times n}(\mathbb{R})$ the value $\|[\mMat{A}]\| = \|\mMat{A}\|$ where\\ $\mMat{A}$ is the linear mapping in $L(\mathbb{R}^n, \mathbb{R}^m)$ corresponding to $[\mMat{A}]$.\retTwo
