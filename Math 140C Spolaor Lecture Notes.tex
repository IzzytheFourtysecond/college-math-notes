% Note for any github stalkers. I am currently in the process
% of learning LaTeX. I don't know what I'm doing yet. Sorry
% if my code absolutely sucks.

\documentclass{book}

\usepackage{fontspec} % used to import Calibri
\usepackage{anyfontsize} % used to adjust font size

% needed for inch and other length measurements
% to be recognized
\usepackage{calc}

% for colors and text effects as is hopefully obvious
\usepackage[dvipsnames]{xcolor}
\usepackage{soul}

% control over margins
\usepackage[margin=1in]{geometry}
\usepackage[strict]{changepage}

\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{bm}

\usepackage[scr=rsfs, scrscaled=.96]{mathalpha}

\usepackage{amssymb} % originally imported to get the proof square
\usepackage{xfrac}
\usepackage[overcommands]{overarrows} % Get my preferred vector arrows...
\usepackage{relsize}

% Just am using this to get a dashed line in a table...
% Also you apparently want this to be inactive if you aren't
% using it because it slows compilation.
\usepackage{arydshln} \ADLinactivate 
\newenvironment{allowTableDashes}{\ADLactivate}{\ADLinactivate}

\usepackage{graphicx}
\graphicspath{{./158_Images/}}

\usepackage{tikz}
   \usetikzlibrary{arrows.meta}
   \usetikzlibrary{graphs, graphs.standard}

\usepackage{quiver} %commutative diagrams


\newfontfamily{\calibri}{Calibri}
\setlength{\parindent}{0pt}
\definecolor{RawerSienna}{HTML}{945D27}

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%Arrow Commands:

% Thank you Bernard, gernot, and Sigur who I copied this from:
% https://tex.stackexchange.com/questions/364096/command-for-longhookrightarrow
\newcommand{\hooklongrightarrow}{\lhook\joinrel\longrightarrow}
\newcommand{\hooklongleftarrow}{\longleftarrow\joinrel\rhook}
\newcommand{\hookxlongrightarrow}[2][]{\lhook\joinrel\xrightarrow[#1]{#2}}
\newcommand{\hookxlongleftarrow}[2][]{\xleftarrow[#1]{#2}\joinrel\rhook}

% Thank you egreg who I copied from:
% https://tex.stackexchange.com/questions/260554/two-headed-version-of-xrightarrow
\newcommand{\longrightarrowdbl}{\longrightarrow\mathrel{\mkern-14mu}\rightarrow}
\newcommand{\longleftarrowdbl}{\leftarrow\mathrel{\mkern-14mu}\longleftarrow}

\newcommand{\xrightarrowdbl}[2][]{%
  \xrightarrow[#1]{#2}\mathrel{\mkern-14mu}\rightarrow
}
\newcommand{\xleftarrowdbl}[2][]{%
  \leftarrow\mathrel{\mkern-14mu}\xleftarrow[#1]{#2}
}

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newcommand{\hOne}{%
   \color{Black}%
   \fontsize{14}{16}\selectfont%
}
\newcommand{\hTwo}{%
   \color{MidnightBlue}%
   \fontsize{13}{15}\selectfont%
}
\newcommand{\hThree}{%
   \color{PineGreen!85!Orange}
   \fontsize{13}{15}\selectfont%
}
\newcommand{\hFour}{%
   \color{Cerulean}
   \fontsize{12}{14}\selectfont%
}
\newcommand{\myComment}{%
   \color{RawerSienna}%
   \fontsize{12}{14}\selectfont%
}
% \newcommand{\pracOne}{
%    \color{BrickRed}%
%    \fontsize{13}{15}\selectfont%
% }
\newcommand{\teachComment}{
   \color{Orange}%
   \fontsize{12}{14}\selectfont%
}
\newcommand{\exOne}{%
   \color{Purple}%
   \fontsize{14}{16}\selectfont%
}
\newcommand{\exTwo}{%
   \color{RedViolet}%
   \fontsize{13}{15}\selectfont%
}
\newcommand{\exP}{%
   \color{VioletRed}%
   \fontsize{12}{14}\selectfont%
}
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newcommand{\cyPen}[1]{{\vphantom{.}\color{Cerulean}#1}}

\newenvironment{myIndent}{%
   \begin{adjustwidth}{2.5em}{0em}%
}{%
   \end{adjustwidth}%
}

\newenvironment{myDindent}{%
   \begin{adjustwidth}{5em}{0em}%
}{%
   \end{adjustwidth}%
}

\newenvironment{myTindent}{%
   \begin{adjustwidth}{7.5em}{0em}%
}{%
   \end{adjustwidth}%
}

\newenvironment{myConstrict}{%
   \begin{adjustwidth}{2.5em}{2.5em}%
}{%
   \end{adjustwidth}%
}

\newcommand{\udefine}[1]{{%
   \setulcolor{Red}%
   \setul{0.14em}{0.07em}%
   \ul{#1}%
}}

\newcommand{\uuline}[2][.]{%
{\vphantom{a}\color{#1}%
\rlap{\rule[-0.18em]{\widthof{#2}}{0.06em}}%
\rlap{\rule[-0.32em]{\widthof{#2}}{0.06em}}}%
#2}

\newcommand*{\markDate}[1]{%
   {\huge \color{Black} \textbf{#1} \newline}%
}

\newcommand{\pprime}{{\prime\prime}}
\newcommand{\suchthat}{ \hspace{0.5em}s.t.\hspace{0.5em}}
\newcommand{\rea}[1]{\mathrm{Re}(#1)}
\newcommand{\ima}[1]{\mathrm{Im}(#1)}
\newcommand{\comp}{\mathsf{C}}
\newcommand{\myHS}{ \hspace{0.5em}}
\newcommand{\diam}[1]{\mathrm{diam}(#1)}
\newcommand{\domain}[1]{\mathrm{dom}(#1)}
\newcommand{\mySpan}{\mathrm{span}}
\newcommand{\myDim}[1]{\mathrm{dim}(#1)}

\newcommand{\rank}[1]{\mathrm{rk}(#1)}
\newcommand{\nullity}[1]{\mathrm{null}(#1)}
\newcommand{\rangeSp}[1]{\mathscr{R}(#1)}
\newcommand{\nullSp}[1]{\mathscr{N}(#1)}


\newcounter{PropNumber}
\setcounter{PropNumber}{82}
\newcommand{\propCount}[1][1]{%
   \addtocounter{PropNumber}{#1}%
   \thePropNumber%
}
\newcounter{SubPropNumber}
\newcommand{\subPropCount}[1][1]{%
   \addtocounter{SubPropNumber}{1}%
   \theSubPropNumber%
}
\newcommand{\resetSubPropCount}{%
   \setcounter{SubPropNumber}{0}%
}

\newcommand{\myId}{\mathrm{Id}}
\newcommand{\myIm}{\mathrm{im}}
\newcommand{\myObj}{\mathrm{Obj}}
\newcommand{\myHom}{\mathrm{Hom}}
\newcommand{\myEnd}{\mathrm{End}}
\newcommand{\myAut}{\mathrm{Aut}}

\newcommand{\mcateg}[1]{{\bm{\mathsf{#1}}}}

% Thank you Gonzalo Medina and Moriambar who wrote this on stack exchange:
%https://tex.stackexchange.com/questions/74125/how-do-i-put-text-over-symbols%
\newcommand{\myequiv}[1]{\stackrel{\mathclap{\mbox{\footnotesize{$#1$}}}}{\equiv}}

% Thank you chs who wrote this on stack exchange:
%https://tex.stackexchange.com/questions/89821/how-to-draw-a-solid-colored-circle%
\newcommand{\filledcirc}[1][.]{\ensuremath{\hspace{0.05em}{\color{#1}\bullet}\mathllap{\circ}\hspace{0.05em}}}

%Thank you blerbl who wrote this on stack exchange:
%https://tex.stackexchange.com/questions/25348/latex-symbol-for-does-not-divide
\newcommand{\ndiv}{\hspace{-0.3em}\not|\hspace{0.35em}}

\newcommand{\mySepOne}[1][.]{%
   {\noindent\color{#1}{\rule{6.5in}{1mm}}}\\%
}
\newcommand{\mySepTwo}[1][.]{%
   {\noindent\color{#1}{\rule{6.5in}{0.5mm}}}\\%
}

\newenvironment{myClosureOne}[2][.]{%
   \color{#1}%
   \begin{tabular}{|p{#2in}|} \hline \\%
}{%
   \\ \hline \end{tabular}%
}

\newcommand{\fillInBlank}[2][.]{{%
   \color{#1}%
   \rule[-0.12em]{#2em}{0.06em}\rule[-0.12em]{#2em}{0.06em}%
   \rule[-0.12em]{#2em}{0.06em}
}}

\newcommand{\retTwo}{\hfill\bigbreak}

\newcounter{LectureNumber}
\newcommand*{\markLecture}[1]{%
   \stepcounter{LectureNumber}%
   {\huge \color{Black} \textbf{Lecture \theLectureNumber: #1} \newline}%
}

\newcommand{\myVS}{\vphantom{\ensuremath{\int_a^b}}}

% Overarrow stuff:
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\NewOverArrowCommand{myVector}{%
   start = {{\smallermathstyle\relbar}},
   middle = {{\smallermathstyle\relbareda}},
   end={{\rightharpoonup}}, space before arrow=0.15em,
   space after arrow=-0.045em,
}

\NewOverArrowCommand{myBar}{%
   start = {{\relbar}},
   middle = {{\relbar}},
   end={{\relbar}}, space before arrow=0.15em,
   space after arrow=-0.025em,
}

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\newcommand{\mVec}[1]{\myVector{#1}}
\newcommand{\mVecAst}[1]{\myVector*{#1}}
\newcommand{\mMat}[1]{\mathbf{#1}}

\title{Math 140C Lecture Notes (Professor: Luca Spolaor)}
\author{Isabelle Mills}

\begin{document}
\maketitle{}
\setul{0.14em}{0.07em}
\calibri

\hOne
\markLecture{4/2/2024}

A set $X \subseteq \mathbb{R}^n$ where $X \neq \emptyset$ is a \udefine{vector space} if:\\ [-20pt]
\begin{itemize}
   \item $\mVec{x}, \mVec{y} \in X \Longrightarrow \mVec{x} + \mVec{y} \in X$\\ [-20pt]
   \item $\mVec{x} \in X$ and $c \in \mathbb{R} \Longrightarrow c\mVec{x} \in X$.\retTwo
\end{itemize}

If $\phi = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\} \subset \mathbb{R}^{n}$, then we define:

{\centering $\mySpan\hspace{0.2em}\phi = \mySpan\{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\} = \left\{ c_1\mVecAst{x}_1 + \ldots + c_k\mVecAst{x}_k \mid c_1, \ldots, c_k \in \mathbb{R} \right\}$.\retTwo\par}

If $E \subseteq \mathbb{R}^n$ and $E = \mySpan\hspace{0.2em}\phi$, then we say $\phi$ \udefine{generates} $E$.\retTwo

{\begin{myIndent}\exOne
   Note that $\mySpan\{\mVecAst{x}_1, \ldots, \mVecAst{x}_2\}$ forms a vector space (this is trivial to check).\retTwo
\end{myIndent}}

$\{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\} \subseteq \mathbb{R}^n$ is called \udefine{linearly independent} if:

{\centering $\sum\limits_{i=1}^{k}{c_i\mVecAst{x}_i} = 0 \Longrightarrow \forall i \in \{1, \ldots, k\},\myHS c_i = 0$.\retTwo\par}

If the above implication does not hold, then we call the set \udefine{linearly dependent}.\retTwo

If $X \subseteq \mathbb{R}^n$ is a vector space, then we define the \udefine{dimension} of $X$ as: 

{\centering ${\myDim{X} = \sup\{k \in \mathbb{N} \cup \{0\} \mid \exists \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}\subset X \text{ which is linearly independent}\}\text{.}}$\par}

{\begin{myDindent} \teachComment
   Also, we define any set containing $\mVec{0}$ to be automatically linearly dependent.\\
   This includes the singleton: $\{\mVec{0}\}$.\retTwo
\end{myDindent}}

$Q = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$ is a \udefine{basis} for $X$ if:\\ [-18pt]
\begin{itemize}
   \item $Q$ is linearly independent.\\ [-20pt]
   \item $\mySpan\hspace{0.2em}Q = X$\retTwo
\end{itemize}

\begin{myIndent}\exOne
   As an example of a basis, for $\mathbb{R}^n$ we define \udefine{the standard basis} as the set\\ $\{e_1, e_2, \ldots, e_n\}$ where $e_i$ is the vector whose $i$th element is $1$ and whose\\ other elements are $0$. It is pretty trivial to check that this set is in fact a\\ basis of $\mathbb{R}^n$.\retTwo
\end{myIndent}

\mySepTwo

{\begin{myIndent} \hTwo
   \uuline{Proposition}: If $B = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$ is a basis of a vector space $X$, then:\\ [-14pt]
   \begin{enumerate}
      \item $\forall \mVec{v} \in X,\myHS c_1, \ldots, c_k \in \mathbb{R} \suchthat \mVec{v} = \sum\limits_{i=1}^{k}{c_i\mVecAst{x}_i}$
      
      {\begin{myIndent} \hThree
         This is true because $X = \mySpan\hspace{0.2em}B$. So by definition of a span, $\mVec{v}$ can\\ be expressed as a linear combination of the vectors of $B$.
      \end{myIndent}}

      \newpage

      \item The $c_i$ such that $\mVec{v} = \sum\limits_{i=1}^{k}{c_i\mVecAst{x}_i}$ are unique.
      
      {\begin{myIndent}\hThree
         Suppose that $\mVec{v} = \sum c_i\mVecAst{x}_i = \sum \alpha_i\mVecAst{x}_i$. Then $\mVec{0} = \sum(c_i - \alpha_i)\mVecAst{x}_i$.\\ Then since $\{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$ are linearly independent, we know for all $i$\\ that $c_i - \alpha_i = 0$. Hence, $c_i = \alpha_i$ for each $i$.\\
      \end{myIndent}}
   \end{enumerate}

   \uuline{Theorem 9.2}: Let $k \in \mathbb{N} \cup \{0\}$. If $X = \mySpan\{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$, then $\myDim{X} \leq k$.\retTwo
   
   {\begin{myIndent}\hThree
      Proof:\\
      Suppose for the sake of contradiction that for any $m \in \mathbb{Z}_+$ , there exists\\ a linearly independent set $Q = \{\mVecAst{y}_1, \ldots, \mVecAst{y}_{k+m}\} \subset X$ which spans $X$.\\ Then, define $S_0 = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$ and note that $S_0$ spans $X$.\retTwo

      Now by induction, assume for $i \in \{0, 1, \ldots, k-1\}$, that $S_i$ contains the\\ first $i$ vectors of $Q$ in addition to $k - i$ vectors of $S_0$, and that $\mySpan\hspace{0.2em}{S_i} = X$.\\
      Then since $S_i$ spans $X$, we know that $y_{i+1} \in X$ is in the span of $S_i$. So,\\ letting $\mVecAst{x}_{n_1}, \ldots, \mVecAst{x}_{n_{k-i}}$ be the elements from $S_0$ in $S_i$, we know that there\\ exists scalars $a_1, \ldots, a_{i+1}, b_1, \ldots, b_{k-i} \in \mathbb{R}$ where $a_{i+1} = 1$ such that:

      {\centering $\sum\limits_{j=1}^{i+1}a_j\mVecAst{y}_j +  \sum\limits_{j=1}^{k-i}b_j\mVecAst{x}_{n_j} = \mVec{0}$ \retTwo\par}

      If all $b_j = 0$, then we have a contradiction. This is because $\{\mVecAst{y}_1, \ldots, \mVecAst{y}_{k+1}\}$\\ is assumed to be linearly independent. So, having all $b_j = 0$ implies that:
      
      {\centering $\sum\limits_{j=1}^{i+1}a_j\mVecAst{y}_j = \sum\limits_{j=1}^{i+1}a_j\mVecAst{y}_j + \sum\limits_{j=i+2}^{k+1}0\cdot\mVecAst{y}_j = \mVec{0}$\par}
      
      In turn this means that all $a_j = 0$, which contradicts that $a_{i+1} = 1$.\retTwo

      So, not all $b_j = 0$. This means that for some $j$ we must have that $\mVecAst{x}_{n_{j}}$ is in\\ the span of $(S_i \setminus \{\mVecAst{x}_{n_{j}}\}) \cup \{\mVecAst{y}_{i+1}\}$. Call this set $S_{i+1}$. Clearly, $S_{i+1}$ contains\\ the first $i + 1$ vectors of $Q$. Also:
      
      {\centering$\mySpan\hspace{0.2em}{S_{i+1}} = \mySpan\hspace{0.2em}(S_{i} \cup \{\mVecAst{y}_{i+1}\}) = \mySpan\hspace{0.2em}S_i = X$.\retTwo\par}

      So $S_{i+1}$ satisfies the same conditions $S_i$ did.\retTwo

      Now we get to the contradiction. Using the above reasoning, we will\\ eventually construct $S_k = \{\mVecAst{y}_1, \ldots, \mVecAst{y}_k\}$ which still spans $X$. However, since\\ $\mVecAst{y}_{k+1} \in X$, that means that $\mVecAst{y}_{k+1}$ equals some linear combination of the other\\ $\mVec{y}$ in $Q$. This contradicts that $Q$ is linearly independent. $\blacksquare$\retTwo
   \end{myIndent}}

   \uuline{Corollary}: If $B = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$ is a basis for $X$, then $\myDim{X} = k$.\retTwo

   {\begin{myIndent}\hThree
      Proof:\\
      Since $B$ is linearly independent, by definition $\myDim{X} \geq k$. Meanwhile,\\ since $B$ spans $X$, we know by the above theorem that $\myDim{X} \leq k$.\\ So $\myDim{X} = k$.
   \end{myIndent}}

   \newpage

   \uuline{Theorem 9.3}: Suppose $X$ is a vector space and $\myDim{X} = n$. Then:
   \begin{itemize}
      \item[(A)] For $E = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_n\} \subset X$, we have that $X = \mySpan\hspace{0.2em}E$ if and only if $E$ is\\ linearly independent.
      
      {\begin{myIndent}\hThree
         Proof:\\
         First, assume $E$ is linearly independent. Then, note that for any\\ $\mVecAst{y} \in X$, we must have that $E \cup \{\mVecAst{y}^{\phantom{|}}\}$ is linearly dependent because\\ $|E \cup \{\mVecAst{y}^{\phantom{|}}\}| > \myDim{X}$. So, there exists $c_1, \ldots, c_n, c_{n+1} \in \mathbb{R}$ such\\ that at least one $c_i$ is nonzero and:

         {\centering $\sum\limits_{i=1}^n{c_i\mVecAst{x}_i} + c_{n+1}\mVecAst{y} = \mVec{0}$\retTwo\par}

         Now if $c_{n+1} = 0$, we have a contradiction because $E$ is linearly\\ independent. So, we conclude that $c_{n+1} \neq 0$. Thus, by rearranging\\ terms we can express $y$ as a linear combination of the vectors of $E$.\\ Therefore, $\mySpan\hspace{0.2em}E = X$ since $y$ can be any vector in $X$.\retTwo

         Secondly, assume $E$ is not linearly independent. Then for some $\mVecAst{x}_i \in E$, we have that $\mySpan\hspace{0.2em}E = \mySpan(E \setminus \{\mVecAst{x}_i\})$. However, $|E \setminus \{\mVecAst{x}_i\}| = n-1$.\\ So if $X = \mySpan\hspace{0.2em}E$, then $\myDim{X} \leq |E \setminus \{\mVecAst{x}_i\}| = n-1$, which contradicts\\ our assumption that $\myDim{X} = n$. Hence, $X \neq \mySpan\hspace{0.2em}E$.\\
      \end{myIndent}}

      \item[(B)] $X$ has a basis and every basis of $X$ consists of $n$ vectors.
      {\begin{myIndent}\hThree
         Proof:\\
         By the definition of $\myDim{X}$, we know that there exists a linearly\\ independent set of $n$ vectors. By the previous part of this theorem,\\ we also know that that set spans $X$. So, it is a basis of $X$. Meanwhile, by\\ the corollary to theorem 9.2, we know that the number of vectors in a\\ basis of $X$ equals the dimension of $X$. Hence, all bases of $X$ must have $n$ vectors.\\
      \end{myIndent}}

      \item[(C)] If $1 \leq m \leq n$ and $\{\mVecAst{y}_1, \ldots, \mVecAst{y}_m\} \subset X$ is linearly independent, then $X$ has a\\ basis that contains $\mVecAst{y}_1, \ldots, \mVecAst{y}_m$.
      {\begin{myIndent}\hThree
         Proof:\\
         Let $S_0 = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_n\}$ be a basis of $X$ and $Q = \{\mVecAst{y}_1, \ldots, \mVecAst{y}_m\}$. Then\\ by the same induction which we used to prove theorem 9.2, we can\\ construct a basis:  $S_m$, of $X$ which contains $\mVecAst{y}_1, \ldots, \mVecAst{y}_m$.
      \end{myIndent}}
   \end{itemize}
\end{myIndent}}

\mySepTwo

Let $X$ and $Y$ be vector spaces. A map $\mMat{A}: X \longrightarrow Y$ is \udefine{linear} if\\ $\mMat{A}(c_1\mVecAst{x}_1 + c_2\mVecAst{x}_2) = c_1\mMat{A}(\mVecAst{x}_1) + c_2\mMat{A}(\mVecAst{x}_2)$ for all $\mVecAst{x}_1, \mVecAst{x}_2 \in X$ and $c_1, c_2 \in \mathbb{R}$.

\newpage

{\begin{myIndent}\exOne
   Observations:
   \begin{enumerate}
      \item A linear map sends $\mVec{0}$ to $\mVec{0}$. This is because: 
      
      {\centering$\mMat{A}(\mVec{0}) = \mMat{A}(\mVec{v} - \mVec{v}) = \mMat{A}(\mVec{v}) - \mMat{A}(\mVec{v}) = \mVec{0}$.\retTwo\par}

      \item If $\mMat{A}: X \longrightarrow Y$ is a linear map and $B = \{\mVecAst{x}_1, \ldots, \mVecAst{x}_k\}$ is a basis of $X$,\\
      then $\mMat{A}\left(\sum\limits_{i=1}^k(c_i\mVecAst{x}_i)\right) = \sum\limits_{i=1}^kc_i\mMat{A}(\mVecAst{x}_i)$ for all $c_1, \ldots, c_k \in \mathbb{R}$.\retTwo
   \end{enumerate}
\end{myIndent}}

Given two vector spaces $X$ and $Y$, we define $L(X, Y)$ to be the set of all linear transformations from $X$ into $Y$. Also, we shall abbreviate $L(X, X)$ as $L(X)$.\retTwo

$\nullSp{\mMat{A}} = $ "\udefine{null space / kernel} of $\mMat{A}$" $ = \{\mVec{x} \in X \mid \mMat{A}(\mVec{x}) = \mVec{0}\}$.\retTwo

$\rangeSp{\mMat{A}} =$ "range of $\mMat{A}$" $ = \{\mVec{y} \in Y \mid \exists \mVec{x} \in X \suchthat \mMat{A}\mVec{x} = \mVec{y}\}$.\retTwo

{\begin{myIndent}\hTwo
   \uuline{Proposition}: For any linear map $\mMat{A}: X \longrightarrow Y$, $\nullSp{\mMat{A}}$ and $\rangeSp{\mMat{A}}$ are vector spaces.\\
   \begin{myIndent}\hThree
      Proof:
      \begin{itemize}
         \item Assume $\mVecAst{x}_1, \mVecAst{x}_2 \in \nullSp{\mMat{A}} \subset X$ and $c \in \mathbb{R}$. Then:
         \begin{itemize}
            \item[$\circ$] $\mMat{A}(\mVecAst{x}_1 + \mVecAst{x}_2) = \mMat{A}(\mVecAst{x}_1) + \mMat{A}(\mVecAst{x}_2) = \mVec{0} + \mVec{0} = \mVec{0}$, which means\\ that $\mVecAst{x}_1 + \mVecAst{x}_2 \in \nullSp{\mMat{A}}$.
            
            \item[$\circ$] $\mMat{A}(c\mVecAst{x}_1) = c\mMat{A}(\mVecAst{x}_1) = c\mVec{0} = \mVec{0}$. So $c\mVecAst{x}_1 \in \nullSp{\mMat{A}}$.
         \end{itemize}
         This shows that $\nullSp{\mMat{A}}$ is a vector space.\retTwo

         \item Assume $\mVecAst{y}_1, \mVecAst{y}_2 \in \rangeSp{\mMat{A}} \subset Y$ and $c \in \mathbb{R}$. Then:
         \begin{itemize}
            \item[$\circ$] We know there exists $\mVecAst{x}_1, \mVecAst{x}_2 \in X$ such that $\mMat{A}(\mVecAst{x}_1) = \mVecAst{y}_1$ and\\ $\mMat{A}(\mVecAst{x}_2) = \mVecAst{y}_2$. In turn, $\mMat{A}(\mVecAst{x}_1 + \mVecAst{x}_2) = \mMat{A}(\mVecAst{x}_1) + \mMat{A}(\mVecAst{x}_2) = \mVecAst{y}_1 + \mVecAst{y}_2$.\\
            So $\mVecAst{y}_1 + \mVecAst{y}_2 \in \rangeSp{\mMat{A}}$.

            \item[$\circ$] Now continue letting $\mVecAst{x}_1 \in X$ be a vector such that $\mMat{A}(\mVecAst{x}_1) = \mVecAst{y}_1$.\\ Then $\mMat{A}(c\mVecAst{x}_1) = c\mMat{A}(\mVecAst{x}_1) = c\mVecAst{y}_1$. So $c\mVecAst{y}_1 \in \rangeSp{\mMat{A}}$.
         \end{itemize}
         This shows that $\rangeSp{\mMat{A}}$ is a vector space. \retTwo
      \end{itemize}
   \end{myIndent}
\end{myIndent}}

$\rank{\mMat{A}} = $ "\udefine{rank} of $\mMat{A}$" $ = \myDim{\rangeSp{\mMat{A}}}$.\retTwo

$\nullity{\mMat{A}} = $ "\udefine{nullity} of $\mMat{A}$" $ = \myDim{\nullSp{\mMat{A}}}$.\retTwo

{\begin{myIndent}\hTwo
   \uuline{Rank-Nullity Theorem}: Given any $\mMat{A} \in L(X, Y)$, we have that
   
   {\centering $\myDim{X} = \rank{\mMat{A}} + \nullity{\mMat{A}}$.\retTwo\par}

   {\begin{myIndent} \hThree
      Proof:\\
      Let $\myDim{X} = n$.\retTwo

      $\nullSp{\mMat{A}} \subseteq X$ is a vector space. So pick a basis $\{\mVecAst{v}_1, \ldots, \mVecAst{v}_k\}$ for $\nullSp{\mMat{A}}$ where\\ $k = \nullity{\mMat{A}} \leq \myDim{X}$. Then by theorem 9.3, choose $\mVecAst{w}_1, \ldots, \mVecAst{w}_{m-k}$ such\\ that $\{\mVecAst{v}_1, \ldots, \mVecAst{v}_k, \mVecAst{w}_1, \ldots, \mVecAst{w}_{n-k}\}$ is a basis of $X$. Note that $\myDim{X} = n$.

      \newpage

      Claim: $B = \{\mMat{A}(\mVecAst{w}_1), \ldots, \mMat{A}(\mVecAst{w}_{n-k})\}$ is a basis of $\rangeSp{\mMat{A}}$.
      \begin{itemize}
         \item $\mMat{A}(\mVecAst{v}_i) = \mVec{0}$ for all $i \in \{1, \ldots, k\}$. So:\\
         
         \begin{tabular}{l}
            $ \rangeSp{\mMat{A}} = \mySpan\{\mMat{A}(\mVecAst{v}_1), \ldots,\mMat{A}(\mVecAst{v}_{k}), \mMat{A}(\mVecAst{w}_1), \ldots, \mMat{A}(\mVecAst{w}_{n-k})\}$\\
            $\phantom{\rangeSp{\mMat{A}}} = \mySpan\{\mMat{A}(\mVecAst{w}_1), \ldots, \mMat{A}(\mVecAst{w}_{n-k})\} = \mySpan\hspace{0.2em}B$
         \end{tabular}\retTwo

         \item $B$ is linearly independent.\\
         To see this, note that: $ \sum\limits_{i=1}^{n-k}\left(c_i\mMat{A}(\mVecAst{w}_i)\right) = \mVec{0} \Longrightarrow \mMat{A}\left(\sum\limits_{i=1}^{n-k}c_i\mVecAst{w}_i\right) = \mVec{0}$\retTwo

         Since we picked each $\mVecAst{w}_1, \ldots, \mVecAst{w}_{n-k} \in B$ so that they were not in\\ $\nullSp{A}$,  we know that any vector in the span of $B$ is not mapped to $0$\\ by $\mMat{A}$ unless it is the zero vector. So

         {\center $\sum\limits_{i=1}^{n-k}c_i\mVecAst{w}_i = \mVec{0}$\retTwo\par}

         And since all the $\mVecAst{w}_i$ are linearly independent, all constants $c_i$ equal $0$.\retTwo
      \end{itemize}
      So $\rank{\mMat{A}} = n - k = \myDim{X} - \nullity{\mMat{A}}$.\retTwo
   \end{myIndent}}
\end{myIndent}}

\markLecture{4/4/2024}
   
{\begin{myIndent}\hTwo
   \uuline{Proposition}: Given $\mMat{A} \in L(X, Y)$, then:
   \begin{itemize}
      \item $\mMat{A}$ is injective if and only if $\nullity{\mMat{A}} = 0$.
      
      {\begin{myIndent} \hThree
         Proof:\\
         ($\Longrightarrow$) If $\mMat{A}$ is injective, then since $\mMat{A}(\mVec{0}) = \mVec{0}$, we have that any vector\\ $\mVec{v} \neq \mVec{0}$ is not in $\nullSp{\mMat{A}}$. So $\nullSp{\mMat{A}} = \{\mVec{0}\}$, meaning $\nullity{\mMat{A}} = 0$.\retTwo

         ($\Longleftarrow$) If $\nullity{\mMat{A}} = 0$, then $\mMat{A}(\mVecAst{v}\phantom{|}) = \mVec{0} \Longrightarrow \mVec{v} = \mVec{0}$. So now assume\\ $ \mMat{A}(\mVec{v}\phantom{|}) = \mMat{A}(\mVec{u}\phantom{|})$. Then $\mMat{A}(\mVec{v} - \mVec{u}\phantom{|}) = \mVec{0}$, meaning $\mVec{v} = \mVec{u}$. Hence $\mMat{A}$\\ is injective.\\
      \end{myIndent}}

      \item $\mMat{A}$ is surjective if and only if $\rank{\mMat{A}} = \myDim{Y}$.
      
      {\begin{myIndent} \hThree
         Proof:\\
         ($\Longrightarrow$) If $\mMat{A}$ is surjective then $\rangeSp{\mMat{A}} = Y$. So we automatically have\\ that $\rank{\mMat{A}} = \myDim{Y}$\retTwo

         ($\Longleftarrow$) If $\rank{\mMat{A}} = \myDim{Y}$, then there exists a linearly independent set of\\ vectors $B \subset \rangeSp{\mMat{A}}$ containing $\myDim{Y}$ many vectors and spanning $\rangeSp{\mMat{A}}$.\\ Then by theorem 9.3, since $B \subset \rangeSp{\mMat{A}} \subseteq Y$, we know $\mySpan\hspace{.2em}{B} = Y$.\\ So, $\rangeSp{\mMat{A}} = Y$, meaning $\mMat{A}$ is surjective.
      \end{myIndent}}
   \end{itemize}


   \newpage

   \uuline{Corollary}: Let $\mMat{A} \in L(X)$. Then $\mMat{A}$ is bijective  if and only if $\nullity{\mMat{A}} = 0$.\retTwo
   
   {\begin{myIndent}\hThree
      Proof: (let $\mMat{A}: X \longrightarrow X$ be a linear map)\retTwo
      ($\Longrightarrow$) If $\mMat{A}$ is bijective, then automatically $\mMat{A}$ is injective. So $\nullity{\mMat{A}} = 0$ by\\ the previous proposition.\retTwo

      ($\Longleftarrow$) If $\nullity{\mMat{A}} = 0$, then by the rank-nullity theorem, we know that\\ $\rank{\mMat{A}} = \myDim{X}$. Thus $\mMat{A}$ is both injective and surjective, meaning $\mMat{A}$\\  is bijective.\retTwo
      
   \end{myIndent}}
\end{myIndent}}

For $\mMat{A} \in L(X)$, when $\nullity{\mMat{A}} = 0$, we call $\mMat{\mMat{A}}$ \udefine{invertible} and define $\mMat{A}^{-1}: X \longrightarrow X$\\ such that $\mMat{A}^{-1}(\mMat{A}(\mVec{x})) = \mVec{x}$ for all $\mVec{x} \in X$.\retTwo

{\begin{myIndent}\exOne
   Because $\mMat{A}$ must be a bijective set function, we know that $\mMat{A}^{-1}$ must also be\\ a right-inverse of $\mMat{A}$, meaning $\mMat{A}(\mMat{A}^{-1}(\mVec{x})) = \mVec{x}$.\retTwo

   Additionally, consider any $\mVecAst{x}_1, \mVecAst{x}_2 \in X$ and let $\mVecAst{x}^{\hphantom{|}\prime}_1 = \mMat{A}^{-1}(\mVecAst{x}_1)$ and\\ $\mVecAst{x}^{\hphantom{|}\prime}_2 = \mMat{A}^{-1}(\mVecAst{x}_2)$. Then since $\mMat{A}$ is a linear mapping, we know that for\\ any $c_1, c_2 \in \mathbb{R}$:

   {\centering $\mMat{A}(c_1\mVecAst{x}^{\hphantom{|}\prime}_1 + c_2\mVecAst{x}^{\hphantom{|}\prime}_2) = c_1\mMat{A}(\mMat{A}^{-1}(\mVecAst{x}_1)) + c_2\mMat{A}(\mMat{A}^{-1}(\mVecAst{x}_2)) = c_1\mVecAst{x}_1 + c_2\mVecAst{x}_2$\retTwo\par}

   So: $\mMat{A}^{-1}(c_1\mVecAst{x}_1 + c_2\mVecAst{x}_2) = c_1\mVecAst{x}^{\hphantom{|}\prime}_1 + c_2\mVecAst{x}^{\hphantom{|}\prime}_2 = c_1\mMat{A}^{-1}(\mVecAst{x}_1) + c_2\mMat{A}^{-1}(\mVecAst{x}_2)$. Hence,\\ we've shown that $\mMat{A}^{-1}$ is a linear mapping, meaning that $\mMat{A}^{-1} \in L(X)$.\retTwo 
\end{myIndent}}

Let $\mMat{A} \in L(X, Y)$ and $\mMat{B} \in L(Y, Z)$. Then we define $\mMat{BA}: X \longrightarrow Z$ by the rule\\ that $\mVec{x} \mapsto \mMat{B}(\mMat{A}(\mVec{x}))$.\retTwo

{\begin{myIndent}\exOne
   We can trivially show that $\mMat{B}\mMat{A}$ is a linear mapping. Consider any\\ $\mVecAst{x}_1, \mVecAst{x}_2 \in X$ and $c_1, c_2 \in \mathbb{R}$. Then:\\ [-6pt]
   \begin{center}
      \begin{tabular}{l}
         $\mMat{BA}(c_1\mVecAst{x}_1 + c_2\mVecAst{x}_2) = \mMat{B}(c_1\mMat{A}(\mVecAst{x}_1) + c_2\mMat{A}(\mVecAst{x}_2))$ \\ [2pt]
         $\phantom{\mMat{BA}(c_1\mVecAst{x}_1 + c_2\mVecAst{x}_2)} = c_1\mMat{B}(\mMat{A}(\mVecAst{x}_1)) + c_2\mMat{B}(\mMat{A}(\mVecAst{x}_2))$\\ [2pt]
         $\phantom{\mMat{BA}(c_1\mVecAst{x}_1 + c_2\mVecAst{x}_2)} = c_1\mMat{B}\mMat{A}(\mVecAst{x}_1) + c_2\mMat{B}\mMat{A}(\mVecAst{x}_2)$
      \end{tabular}\retTwo
   \end{center}

   This means that $\mMat{BA} \in L(X, Z)$.\retTwo
\end{myIndent}}

Let $\mMat{A}, \mMat{B} \in L(X, Y)$ and $c_1, c_2 \in \mathbb{R}$. Then we define $(c_1\mMat{A} + c_2\mMat{B}) : X \longrightarrow Y$ by the\\ rule: $\mVec{x} \mapsto c_1\mMat{A}(\mVec{x}) + c_2\mMat{B}(\mVec{x})$.\retTwo

{\begin{myIndent} \exOne
   It is even more trivial to show that $(c_1\mMat{A} + c_2\mMat{B})$ is a linear map.
\end{myIndent}}

\newpage

Let $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$. We define the \udefine{norm} of $\mMat{A}$ as: 

{\centering $\|\mMat{A}\| = \sup\left\{\|\mMat{A}(\mVec{x})\| \mid \mVec{x} \in \mathbb{R}^n \text{ and } \|\mVec{x}\| \leq 1\right\}$. \retTwo\par}

\mySepTwo

Throughout this section, we shall prove that $\|\cdot\| : L(\mathbb{R}^n, \mathbb{R}^m) \longrightarrow \mathbb{R}$ is\\ well-defined and fulfills the properties of a general norm function.\retTwo

{\begin{myIndent}\hTwo

   \uuline{Proposition}: If $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$, then $\|\mMat{A}\|$ exists and is finite.\\ [-6pt]
      
   {\begin{myIndent}\hThree
      Proof:\\
      Let $\{e_1, \ldots, e_n\}$ be the standard basis in $\mathbb{R}^n$. Then for any $\mVec{x} \in \mathbb{R}^n$, there\\ are unique $c_1, \ldots, c_n \in \mathbb{R}$ such that $\mVec{x} = c_1e_1 + \ldots + c_ne_n $.\retTwo

      Since we are working with the standard basis, we know: {\fontsize{12}{14}\selectfont$\|\mVec{x}\| = \sqrt{\sum\limits_{i=1}^nc_i^2}$}.\retTwo
      
      Thus, for $\|\mVec{x}\| \leq 1$, we must have that $|c_i| \leq 1$ for each $c_i$. This means:

      {\center\fontsize{12}{14}\selectfont $\|\mMat{A}(\mVec{x})\| = \left\|\sum\limits_{i=1}^n{c_i\mMat{A}(e_i)}\right\| \leq \sum\limits_{i=1}^n\|c_i\mMat{A}(e_i)\| = \sum\limits_{i=1}^n|c_i|\|\mMat{A}(e_i)\| \leq \sum\limits_{i=1}^n\|\mMat{A}(e_i)\|$ \retTwo\par}

      Importantly, we must have that {\fontsize{12}{14}\selectfont$\sum\limits_{i=1}^n\|\mMat{A}(e_i)\|$} is finite. Additionally, it is an\\ upper bound to the set: $\left\{\|\mMat{A}(\mVec{x})\| \mid \mVec{x} \in \mathbb{R}^n \text{ and } \|\mVec{x}\| \leq 1\right\} \subseteq \mathbb{R}$.\retTwo
      
      So, we showed that the above set is bounded above. Also, the above set\\ is nonempty because it must contain $\|\mVec{0}\| = 0$. Thus by the least upper bound\\ property of $\mathbb{R}$, we know that the supremum of this set exists in $\mathbb{R}$.\retTwo

      Hence, $\|\mMat{A}\|$ exists and is finite.\retTwo

      \begin{myTindent}\hFour
         Side note, the above proof also shows that $\|\mMat{A}\| \geq 0$.\retTwo 
      \end{myTindent}
   \end{myIndent}}

   \uuline{Lemma}: For $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$ and $\mVec{x} \in \mathbb{R}^n$, we have that $\|\mMat{A}(\mVec{x})\| \leq \|\mMat{A}\|\|\mVec{x}\|$.\\ [-6pt]

   {\begin{myIndent}\hThree
      Proof:\\
      Case 1: $\mVec{x} \neq \mVec{0}$.
      \begin{myIndent}
         Then since $\|\mVec{x}\| \neq 0$, we can say that:
         
         {\centering $\|\mMat{A}(\mVec{x})\| = \left\|\mMat{A}\left(\|\mVec{x}\|\frac{\mVec{x}}{\|\mVec{x}\|}\right)\right\| = \left\|\|\mVec{x}\|\mMat{A}\left(\frac{\mVec{x}}{\|\mVec{x}\|}\right)\right\| = \left\|\mMat{A}\left(\frac{\mVec{x}}{\|\mVec{x}\|}\right)\right\|\|\mVec{x}\|$\retTwo\par}

         Now $\frac{\mVec{x}}{\|\mVec{x}\|} \in \mathbb{R}^n$ and $\left\|\frac{\mVec{x}}{\|\mVec{x}\|}\right\| = 1$. So, $\left\|\mMat{A}\left(\frac{\mVec{x}}{\|\mVec{x}\|}\right)\right\|\|\mVec{x}\| \leq \|\mMat{A}\|\|\mVec{x}\|$\retTwo
      \end{myIndent}

      Case 2: $\mVec{x} = \mVec{0}$.
      \begin{myIndent}
         Then trivially $\|\mMat{A}(\mVec{x})\| = \|\mMat{A}(\mVec{0})\| = 0 = \|\mMat{A}\|\|\mVec{0}\| = \|\mMat{A}\|\|\mVec{x}\|$\retTwo
      \end{myIndent}
   \end{myIndent}}

   \newpage

   \uuline{Proposition}: If $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$, then $0 \leq \|\mMat{A}\|$. Also $\|\mMat{A}\| = 0$ if and only if $\mMat{A}$ is the unique function mapping all of $\mathbb{R}^n$ to $\mVec{0}$.\\ [-6pt]
   {\begin{myIndent}\hThree
      Proof:\\
      We already showed previously that $\|\mMat{A}\| \geq 0$. So, it now suffices to\\ show that $\|\mMat{A}\| = 0 \Longleftrightarrow \nullSp{\mMat{A}} = \mathbb{R}^n$.\retTwo

      ($\Longrightarrow$) Assume that $\nullSp{\mMat{A}} \neq \mathbb{R}^n$. Then there exists $\mVec{x} \in \mathbb{R}^n$ such\\ that $\mMat{A}(\mVec{x}) \neq \mVec{0}$. Since $\mVec{x}$ can't be $\mVec{0}$, consider the vector $\hat{x} = \frac{\mVec{x}}{\|\mVec{x}\|}$.\\ [-3pt] By the linearity of $\mMat{A}$, we know $\mMat{A}\left(\hat{x}\right) = \frac{1}{\|\mVec{x}\|}\mMat{A}(\mVec{x}) \neq \mVec{0}$. So,\\ $\|\mMat{A}(\hat{x})\| > 0$. But $\|\mMat{A}(\hat{x})\|$ is in the set that $\|\mMat{A}\|$ is a supremum of,\\ which means that $\|\mMat{A}\| \geq \|\mMat{A}(\hat{x})\| > 0$. Or in other words, $\|\mMat{A}\| \neq 0$.\retTwo

      ($\Longleftarrow$) Assume that $\nullSp{\mMat{A}} = \mathbb{R}^n$. Then,
      
      {\centering $\sup\left\{\|\mMat{A}(\mVec{x})\| \mid \mVec{x} \in \mathbb{R}^n \text{ and } \|\mVec{x}\| \leq 1\right\} = \sup\{0\} =  0$\retTwo\par}
   \end{myIndent}}

   \uuline{Corollary}: Given $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$, we have that $\mMat{A}$ is uniformly continuous.\\ [-6pt]
   
   {\begin{myIndent}\hThree
      Proof:\\
      Case 1: $\|\mMat{A}\| \neq 0$, meaning we can divide by $\|\mMat{A}\|$.
      \begin{myIndent}
         By the previous proposition, $\|\mMat{A}(\mVec{x}) - \mMat{A}(\mVec{y})\| \leq \|\mMat{A}\|\|\mVec{x} - \mVec{y}\|$ for\\ all $\mVec{x}, \mVec{y} \in \mathbb{R}^n$. Hence, for any $\varepsilon > 0$, if we make $\|\mVec{x} - \mVec{y}\| < \frac{\varepsilon}{\|\mMat{A}\|}$,\\ [-2pt] then $\|\mMat{A}(\mVec{x}) - \mMat{A}(\mVec{y})\| < \varepsilon$.\retTwo
      \end{myIndent}

      Case 2: $\|\mMat{A}\| = 0$.
      \begin{myIndent}
         Then $\mMat{A}$ is a constant function, making it automatically uniformly\\ continuous.\retTwo
      \end{myIndent}
   \end{myIndent}}

   \uuline{Subcorollary}: Given $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$, there exists $\mVec{x} \in \mathbb{R}^n$ with $\|\mVec{x}\| \leq 1$ such\\ that $\|\mMat{A}(\mVec{x})\| = \|\mMat{A}\|$.\\ [-6pt]

   {\begin{myIndent}\hThree
      Proof:\\
      Let $S = \{\mVec{x} \in \mathbb{R}^n \mid \|\mVec{x}\| \leq 1\}$ and consider the restriction $\mMat{A}|_{S}$.\retTwo

      Since $S$ is a closed and bounded subset of $\mathbb{R}^n$, we know that $S$ is compact\\ by the Heine-Borel theorem (see proposition 28 in Math 140A notes).\\ This combined with the fact that $\mMat{A}|_{S}$ is still continuous means that by the\\ extreme value theorem, there is $\mVec{x} \in S$ with: 
      
      {\center $\mMat{A}(\mVec{x}) = \mMat{A}|_{S}(\mVec{x}) = \sup\left\{\|\mMat{A}(\mVec{x})\| \mid \mVec{x} \in \mathbb{R}^n \text{ and } \|\mVec{x}\| \leq 1\right\}$.\retTwo\par}
   \end{myIndent}}

   \uuline{Proposition}: If $\mMat{A}, \mMat{B} \in L(\mathbb{R}^n, \mathbb{R}^m)$, then $\|\mMat{A} + \mMat{B}\| \leq \|\mMat{A}\| + \|\mMat{B}\|$.\\[-6pt]
   {\begin{myIndent}\hThree
      Proof:\\
      Let $\mVec{x} \in \mathbb{R}^n$ be a vector such that $\|\mVec{x}\| \leq 1$ and $\|\mMat{A}(\mVec{x})\| = \|\mMat{A}\|$. Then:

      \begin{center}
         \begin{tabular}{l}
            $\|\mMat{A} + \mMat{B}\| = \|(\mMat{A} + \mMat{B})(\mVec{x})\| = \|\mMat{A}(\mVec{x}) + \mMat{B}(\mVec{x})\|$ \\
            $\phantom{\|\mMat{A} + \mMat{B}\| = \|(\mMat{A} + \mMat{B})(\mVec{x})\|} \leq \|\mMat{A}(\mVec{x})\| + \|\mMat{B}(\mVec{x})\| \leq \|\mMat{A}\| + \|\mMat{B}\|$
         \end{tabular}
      \end{center}
   \end{myIndent}}

   \newpage

   \uuline{Proposition}: If $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$ and $c \in \mathbb{R}$, then $\|c\mMat{A}\| = |c|\|\mMat{A}\|$.\\ [-6pt]
   
   {\begin{myIndent} \hThree
      Proof:\\
      Pick $\mVec{x} \in \mathbb{R}^n$ satisfying $\|\mVec{x}\| \leq 1$ and $\|\mMat{A}(\mVec{x})\| = \|\mMat{A}\|$. Then:
      \begin{center}
         $|c|\|\mMat{A}\| = |c|\|\mMat{A}(\mVec{x})\| = \|c\mMat{A}(\mVec{x})\| = \|(c\mMat{A})(\mVec{x})\| \leq \|c\mMat{A}\|$.\retTwo
      \end{center}

      Next, pick $\mVec{y} \in \mathbb{R}^n$ satisfying $\|\mVec{y}\| \leq 1$ and $\|(c\mMat{A})(\mVec{x})\| = \|c\mMat{A}\|$. Then:
      \begin{center}
         $\|c\mMat{A}\| = \|(c\mMat{A})(\mVec{y})\| = \|c\mMat{A}(\mVec{y})\| = |c|\|\mMat{A}\mVec{y}\| \leq |c|\|\mMat{A}\|$.\retTwo
      \end{center}
   \end{myIndent}}
\end{myIndent}}

Specifically because of the four propositions above, we have shown that\\  $\|\cdot\| : L(\mathbb{R}^n, \mathbb{R}^m) \longrightarrow \mathbb{R}$ is well-defined and a valid norm. Consequently, by\\ defining $d(\mMat{A}, \mMat{B}) = \|\mMat{A} - \mMat{B}\|$ for all $\mMat{A}, \mMat{B} \in L(\mathbb{R}^n, \mathbb{R}^m)$, we naturally get that\\ $L(\mathbb{R}^n, \mathbb{R}^m)$ is a metric space.\\ [-6pt]

\begin{myIndent}\exOne
   Given any $\mMat{A}, \mMat{B}, \mMat{C} \in L(\mathbb{R}^n, \mathbb{R}^m)$, we have:
   \begin{itemize}
      \item $d(\mMat{A}, \mMat{B}) = \|\mMat{A} - \mMat{B}\| \geq 0$ with $d(\mMat{A}, \mMat{B}) = 0$ if and only if $\mMat{A} = \mMat{B}$.
      \item $d(\mMat{A}, \mMat{B}) = \|\mMat{A} - \mMat{B}\| = |-1|\|\mMat{B} - \mMat{A}\| = d(\mMat{B}, \mMat{A})$
      \item $d(\mMat{A}, \mMat{C}) = \|\mMat{A} - \mMat{C}\| \leq \|\mMat{A} - \mMat{B}\| + \|\mMat{B} - \mMat{C}\| = d(\mMat{A}, \mMat{B}) + d(\mMat{B}, \mMat{C})$\retTwo
   \end{itemize}
\end{myIndent}

Before moving on, here is another corollary of the above statements.
{\begin{myIndent} \hTwo
   \uuline{Corollary}: If $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$ and $\mMat{B} \in L(\mathbb{R}^m, \mathbb{R}^k)$, then $\|\mMat{BA}\| \leq \|\mMat{B}\|\|\mMat{A}\|$.\\ [-6pt]

   {\begin{myIndent}\hThree
      Proof:\\
      Pick $\mVec{x} \in \mathbb{R}^n$ satisfying $\|\mVec{x}\| \leq 1$ and $\|(\mMat{BA})(\mVec{x})\| = \|\mMat{BA}\|$. Then:
      \begin{center}
         $\|\mMat{BA}\| = \|(\mMat{BA})(\mVec{x})\| = \|\mMat{B}(\mMat{A}(\mVec{x}))\| \leq \|\mMat{B}\|\|\mMat{A}(\mVec{x})\| \leq \|\mMat{B}\|\|\mMat{A}\|$.\retTwo
      \end{center}
   \end{myIndent}}
\end{myIndent}}

\mySepTwo


{\begin{myIndent} \hTwo
   \uuline{Theorem 9.8}: Let $\Omega \subset L(\mathbb{R}^n)$ be the set of all invertible linear mappings on $\mathbb{R}^n$.
   \begin{itemize}
      \item[(A)] If $\mMat{A} \in \Omega$, $\mMat{B} \in L(\mathbb{R}^n)$, and $\|\mMat{B} - \mMat{A}\| < \dfrac{1}{\|\mMat{A}^{-1}\|}$, then $\mMat{B} \in \Omega$.\\ [-10pt]
      {\begin{myIndent}\hThree
         Proof:\\
         Pick $\mVec{x} \in \mathbb{R}^n$ such that $\|\mVec{x}\| \leq 1$. Then:
         
         \begin{center}
            \begin{tabular}{l}
               $\|\mMat{A}(\mVec{x})\| = \|(\mMat{A} - \mMat{B} + \mMat{B})(\mVec{x})\|$\\
               $\phantom{\|\mMat{A}(\mVec{x})\|} \leq \|(\mMat{A} - \mMat{B})(\mVec{x})\| + \|\mMat{B}(\mVec{x})\|$\\
               $\phantom{\|\mMat{A}(\mVec{x})\|} \leq \|\mMat{A} - \mMat{B}\|\|\mVec{x}\| + \|\mMat{B}(\mVec{x})\| = \|\mMat{B} - \mMat{A}\|\|\mVec{x}\| + \|\mMat{B}(\mVec{x})\|$
            \end{tabular}\retTwo
         \end{center}

         Meanwhile, note that $\|\mMat{A}^{-1}\| \neq 0$. We know this because $\mMat{A}^{-1}$ must be\\ invertible (because $\nullSp{\mMat{A}^{-1}} = \{\mVec{0}\}$) and the one linear transformation\\ in $L(\mathbb{R}^n)$ with norm $0$ is not invertible. So:

         \begin{center}
            \begin{tabular}{l}
               $\frac{\|\mVec{x}\|}{\|\mMat{A}^{-1}\|} = \frac{\|\mMat{A}^{-1}\mMat{A}(\mVec{x})\|}{\|\mMat{A}^{-1}\|} \leq \frac{\|\mMat{A}^{-1}\|\|\mMat{A}(\mVec{x})\|}{\|\mMat{A}^{-1}\|} = \|\mMat{A}(\mVec{x})\|$
            \end{tabular}\retTwo
         \end{center}

         \newpage

         Hence, $\frac{\|\mVec{x}\|}{\|\mMat{A}^{-1}\|} \leq \|\mMat{B} - \mMat{A}\|\|\mVec{x}\| + \|\mMat{B}(\mVec{x})\|$. By rearranging terms, we get\\ this expression: $\left(\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\|\right)\|\mVec{x}\| \leq \|\mMat{B}(\mVec{x})\|$.\retTwo

         Now, note that if $\|\mMat{B}(\mVec{x})\| = 0$ but $\mVec{x} \neq \mVec{0}$, then we must have that:\\ $\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\| \leq 0$. Or in other words, $\|\mMat{B} - \mMat{A}\| \geq \frac{1}{\|\mMat{A}^{-1}\|}$. So,\\ if $\|\mMat{B} - \mMat{A}\| < \frac{1}{\|\mMat{A}^{-1}\|}$, then $\|\mMat{B}(\mVec{x})\| = 0$ only when $\mVec{x} = \mVec{0}$.\\ [2pt] Hence, $\nullity{\mMat{B}} = 0$ and $\mMat{B}$ is invertible.\retTwo
      \end{myIndent}}
      
      \item[(B)] $\Omega$ is an open subset of $L(\mathbb{R}^n)$, and the mapping over $\Omega$ with the rule:\\ $\mMat{A} \mapsto \mMat{A}^{-1}$, is continuous.\\ [-10pt]
      {\begin{myIndent}\hThree
         Proof:\\
         Firstly, by part A we know that for any $\mMat{A} \in \Omega$, if $r = \frac{1}{\|\mMat{A}^{-1}\|}$, then\\ [1pt] $B_r(\mMat{A}) \subseteq \Omega$. So, $\Omega$ is an open set in the metric space $L(\mathbb{R}^n)$.\retTwo

         Now let $\mMat{A}, \mMat{B} \in \Omega$ and recall from part A that:
         
         {\centering $\left(\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\|\right)\|\mVec{x}\| \leq \|\mMat{B}(\mVec{x})\|$.\retTwo\par}

         Since we know $\mMat{B}^{-1}$ exists, set $\mVec{x} = \mMat{B}^{-1}(\mVec{y})$. Then the above\\ [1pt] expression becomes: $\left(\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\|\right)\|\mMat{B}^{-1}(\mVec{y})\| \leq \|\mVec{y}\|$.\\ [1pt] Because we are interested in $\mMat{B}$ close to $\mMat{A}$, we can assume that\\ [4pt] $\|\mMat{B} - \mMat{A}\| < \frac{1}{\|\mMat{A}^{-1}\|}$. Thus it is safe to divide by $\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\|$.\\ [2pt] So, setting $\mVec{y} \in \mathbb{R}^n$ to be the vector satisfying $\|\mVec{y}\| \leq 1$ and\\ [4pt] $\|\mMat{B}^{-1}(\mVec{y})\| = \|\mMat{B}^{-1}\|$, we have that:

         {\center $ \|\mMat{B}^{-1}\| = \|\mMat{B}^{-1}(\mVec{y})\| \leq \frac{\|\mVec{y}\|}{\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\|} \leq \frac{1}{\frac{1}{\|\mMat{A}^{-1}\|} - \|\mMat{B} - \mMat{A}\|} = \frac{\|\mMat{A}^{-1}\|}{1 - \|\mMat{A}^{-1}\|\|\mMat{B} - \mMat{A}\|}$ \retTwo\par}

         \uuline{Lemma}: Given $\mMat{A} \in L(Z, W)$, $\myHS\mMat{B}, \mMat{C} \in L(Y, Z)$, and $\mMat{D} \in L(X, Y)$,\\ we have that $\mMat{A}(\mMat{B} + \mMat{C}) = \mMat{A}\mMat{B} + \mMat{A}\mMat{C}$ and $(\mMat{B} + \mMat{C})\mMat{D} = \mMat{B}\mMat{D} + \mMat{C}\mMat{D}$.
         {\begin{myIndent}\hFour
            Proof:
            \begin{itemize}
               \item[$\circ$] $\mMat{A}((\mMat{B} + \mMat{C})(\mVec{v})) = \mMat{A}(\mMat{B}(\mVec{v}) + \mMat{C}(\mVec{v})) = \mMat{A}(\mMat{B}(\mVec{v})) + \mMat{A}(\mMat{C}(\mVec{v}))$
               \item[$\circ$] $ (\mMat{B} + \mMat{C})(\mMat{D}(\mVec{v})) =  \mMat{B}(\mMat{D}(\mVec{v})) + \mMat{C}(\mMat{D}(\mVec{v}))$\\
            \end{itemize}
         \end{myIndent}}

         Based on the above lemma, we have that $\mMat{B}^{-1} - \mMat{A}^{-1} = \mMat{B}^{-1}(\mMat{A} - \mMat{B})\mMat{A}^{-1}$.\\ So:\\ [-26pt]
         \begin{center}
            \begin{tabular}{l}
               $0 \leq \|\mMat{B}^{-1} - \mMat{A}^{-1}\| = \|\mMat{B}^{-1}(\mMat{A} - \mMat{B})\mMat{A}^{-1}\|$ \\
               $\phantom{0 \leq \|\mMat{B}^{-1} - \mMat{A}^{-1}\|} \leq \|\mMat{B}^{-1}\|\|(\mMat{A} - \mMat{B})\|\|\mMat{A}^{-1}\| \leq \frac{\|\mMat{A}^{-1}\|^2}{1 - \|\mMat{A}^{-1}\|\|\mMat{B} - \mMat{A}\|}\|\mMat{B} - \mMat{A}\|$
            \end{tabular}
         \end{center}

         \newpage

         Finally, assume $\mMat{A} \in \Omega^\prime$. This is fine because the mapping is\\ automatically continuous at $\mMat{A}$ if $\mMat{A} \notin \Omega^\prime$. Then we have that:
         
         {\centering $\lim\limits_{\mMat{B} \rightarrow \mMat{A}}\left(\frac{\|\mMat{A}^{-1}\|^2}{1 - \|\mMat{A}^{-1}\|\|\mMat{B} - \mMat{A}\|}\|\mMat{B} - \mMat{A}\|\right) = \|\mMat{A}^{-1}\|^2 \cdot 0 = 0$.\retTwo\par}

         So, $0 \leq \lim\limits_{\vphantom{\int^{b}}\mMat{B} \rightarrow \mMat{A}}(\|\mMat{B}^{-1} - \mMat{A}^{-1}\|) \leq 0$.\retTwo
         
         This means that $d(\mMat{B}^{-1}, \hspace{0.2em} \mMat{A}^{-1}) = \|\mMat{B}^{-1} - \mMat{A}^{-1}\| \rightarrow 0$ as $\mMat{B} \rightarrow \mMat{A}$.\\ Or in other words:

         {\centering $\phantom{\blacksquare\text{.}}$ $\lim\limits_{\mMat{B} \rightarrow \mMat{A}}(\mMat{B}^{-1}) = \mMat{A}^{-1}$. $\blacksquare$ \retTwo\par}
      \end{myIndent}}
   \end{itemize}
\end{myIndent}}

\mySepTwo

\markLecture{4/9/2024}

Let $X$ and $Y$ be vector spaces and fix two bases $\{\mVecAst{x}_1, \ldots, \mVecAst{x}_n\}$ and $\{\mVecAst{y}_1, \ldots, \mVecAst{y}_m\}$\\ of $X$ and $Y$ respectively. Then given any $\mMat{A} \in L(X, Y)$, since $\mMat{A}(\mVecAst{x}_j) \in Y$ for\\ each $j \in \{1, \ldots, n\}$, we have that there are unique scalars $a_{i,j}$ such that:\\ [-10pt]

{\centering $\mMat{A}(\mVecAst{x}_j) = \sum\limits_{i=1}^m a_{i,j}\mVecAst{y}_i$\retTwo\par}

For convenience, we can visualize these numbers in an \udefine{$m\times n$ matrix}:\\ [-8pt]

{\centering $[\mMat{A}] = 
\begin{bmatrix}
   a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
   a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
   \vdots & \vdots & \ddots & \vdots \\
   a_{m,1} & a_{m,2} & \cdots & a_{m,n}
\end{bmatrix} $\retTwo\par}

Note that for each $j \in \{1,\ldots,n\}$, we have that the $j$th column of $[\mMat{A}]$ gives\\ the coordinates of $\mMat{A}(\mVecAst{x}_j)$ with respect to the basis $\{\mVecAst{y}_1, \ldots, \mVecAst{y}_m\}$. Thus, we call\\ the vectors $\mMat{A}(\mVecAst{x}_j)$ the \udefine{column vectors} of $[\mMat{A}]$.\\ [-6pt]

{\begin{myIndent}\hTwo
   \uuline{Fact 1}: Given any $\mVec{x} \in X$, there are unique scalars $c_1, \ldots, c_n$ such that\\ $\mVec{x} = \sum\limits_{j=1}^n c_j\mVecAst{x}_j$. Then, the coordinates of $\mMat{A}(\mVec{x})$ with respect to our basis of $Y$\\ is given by the commonly defined matrix-vector product:\\ [-6pt]

   {\center $\begin{bmatrix}
      a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
      a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{m,1} & a_{m,2} & \cdots & a_{m,n}
   \end{bmatrix}
   \begin{bmatrix}
      c_1 \\ c_2 \\ \vdots \\ c_n
   \end{bmatrix}$\par}

   \newpage 

   {\begin{myIndent}\hThree
      This is true because $\mMat{A}$ is linear. Therefore:\\ [-6pt]

      {\centering\fontsize{12}{14}\selectfont 
      \begin{tabular}{l}
         $\mMat{A}(\mVec{x}) = \mMat{A}\left(\sum\limits_{j=1}^n c_j\mVecAst{x}_j\right) = \sum\limits_{j=1}^n c_j\mMat{A}(\mVecAst{x}_j)$ \\ [-5pt]
         $\phantom{\mMat{A}(\mVec{x}) = \mMat{A}\left(\sum\limits_{j=1}^n c_j\mVecAst{x}_j\right)} = \sum\limits_{j=1}^n c_j \left(\sum\limits_{i=1}^{m}a_{i,j}\mVecAst{y}_i\right) = \sum\limits_{i=1}^m\left(\sum\limits_{j=1}^n c_ja_{i,j}\right)\hspace{-0.2em}\mVecAst{y}_i$
      \end{tabular}\retTwo\par}
   \end{myIndent}}

   \uuline{Fact 2}: When we said how to generate an $m\times n$ matrix $[\mMat{A}]$ for any $A \in L(X, Y)$,\\ we were implicitely creating a mapping $\phi : L(X, Y) \longrightarrow \mathcal{M}_{m\times n}(\mathbb{R})$ (the set\\ of $m\times n$ real matrices). Importantly, this map is invertible.\\  [-8pt]
   
   {\begin{myIndent}\hThree
      Let us define a mapping $\varphi: \mathcal{M}_{m\times n}(\mathbb{R}) \longrightarrow L(X, Y)$ such that for any\\ $[\mMat{B}] \in \mathcal{M}_{m\times n}(\mathbb{R})$ where

      {\centering $[\mMat{B}] = \begin{bmatrix}
         b_{1,1} & b_{1,2} & \cdots & b_{1,n} \\
         b_{2,1} & b_{2,2} & \cdots & b_{2,n} \\
         \vdots & \vdots & \ddots & \vdots \\
         b_{m,1} & b_{m,2} & \cdots & b_{m,n}
      \end{bmatrix}$\retTwo\par}

      \dots we define $\varphi([\mMat{B}]) \in L(X, Y)$ by $\varphi([\mMat{B}])(\mVec{x}) = \sum\limits_{i=1}^m\left(\sum\limits_{j=1}^n c_jb_{i,j}\right)\hspace{-0.2em}\mVecAst{y}_i$ where\\ [-4pt] $c_1, \ldots, c_n$ are the coefficients such that $\mVec{x} = \sum\limits_{j=1}^n c_j\mVecAst{x}_j$.\retTwo

      Firstly, $\varphi([\mMat{B}])$ is well defined because $c_1, \ldots, c_n$ are unique with respect\\ to our basis of $X$. Also, $\varphi([\mMat{B}])$ is linear because sums are linear.\retTwo

      Meanwhile, by fact 1 we know that for any $\mMat{A} \in L(X, Y),\myHS \mMat{A} = \varphi(\phi(\mMat{A}))$.\\ At the same time, you can easily check that for any $[\mMat{B}] \in \mathcal{M}_{m\times n}(\mathbb{R})$,\\ $[\mMat{B}] = \phi(\varphi([\mMat{B}]))$. Hence, $\varphi = \phi^{-1}$.\retTwo
      
      \begin{myTindent}\hFour
         Thus, from now on we shall say that the linear map $\mMat{A}$ and\\ matrix $[\mMat{A}]$ are associated with each other if $\phi(\mMat{A}) = [\mMat{A}]$\\ and $\varphi([\mMat{A}]) = \mMat{A}$.\\ [5pt]
      \end{myTindent}
   \end{myIndent}}

   \uuline{Fact 3}: In addition to our bases for $X$ and $Y$, fix $\{\mVecAst{z}_1, \ldots, \mVecAst{z}_p\}$ as our basis for\\ [1pt] $Z$. Then, given the linear maps $\mMat{A} \in L(X, Y)$ and $\mMat{B} \in L(Y, Z)$ and their\\ [1pt] associated matrices $[\mMat{A}] \in \mathcal{M}_{m\times n}(\mathbb{R})$ and $[\mMat{B}] \in \mathcal{M}_{p\times m}(\mathbb{R})$, we have that\\ [1pt] the map $\mMat{BA}$ is associated with the matrix $[\mMat{B}][\mMat{A}]$.\\  [-8pt]
   
   {\begin{myIndent}\hThree
      Let us use $a_{i,j}$ and $b_{k,i}$ to refer to the entries of $[\mMat{A}]$ and $[\mMat{B}]$ respectively.\\ Then note that:

      {\centering{\fontsize{12}{14}\selectfont \begin{tabular}{l}
         $\mMat{BA}(\mVec{x_j}) = \mMat{B}(\mMat{A}(\mVec{x_j})) = \mMat{B}\left(\sum\limits_{i=1}^m a_{i,j}\mVecAst{y}_i\right) = \sum\limits_{i=1}^m a_{i,j}\mMat{B}(\mVecAst{y}_i)$ \\ [10pt]

         $\phantom{\mMat{BA}(\mVec{x_j}) = \mMat{B}(\mMat{A}(\mVec{x_j}))} = \sum\limits_{i=1}^m a_{i,j}\hspace{-0.25em}\left(\sum\limits_{k=1}^p b_{k,i}\mVecAst{z}_k\right) = \sum\limits_{k=1}^p\hspace{-0.1em}\left(\sum\limits_{i=1}^m(a_{i,j}b_{k,i})\right)\hspace{-0.25em}\mVecAst{z}_k$
      \end{tabular}}\par}

      \newpage

      So, the $(k, j)$th. entry of the matrix associated with $\mMat{BA}$ is $\sum\limits_{i=1}^m b_{k,i}a_{i,j}$.\retTwo

      Hence, the matrix associated with the map $\mMat{B}\mMat{A}$ is precisely the matrix\\ product $[\mMat{B}][\mMat{A}]$.\\ [5pt]
   \end{myIndent}}

   \uuline{Fact 4}: Suppose that $\mMat{A}$ and $\mMat{B}$ are linear maps in $L(X, Y)$ and that $c_1$ and $c_2$\\ are scalars. Then the matrix $c_1[\mMat{A}] + c_2[\mMat{B}]$ is associated with the linear map\\ $c_1\mMat{A} + c_2\mMat{B}$.\\ [-6pt]

   {\begin{myIndent}\hThree
      This is rather trivial to prove compared to the other facts. So, since I'm\\ really behind, I'm just not going to prove it here. Frick you <3.\retTwo
   \end{myIndent}}
\end{myIndent}}

Now from a rigor point of view, we'd rather work with linear maps than matrices.\\ This is because the defintion of a matrix depends on what bases we fix, whereas\\ linear maps are defined independently of any bases. That said, matrices are too\\ convenient to not be discussed.\retTwo

Going foward, here are three notational things from linear we shall adopt when talking about linear maps:
\begin{enumerate}
   \item We shall abbreviate $\mMat{A}(\mVec{x})$ as $\mMat{A}\mVec{x}$.
   \item We shall denote $\mMat{0} \in L(X, Y)$ as the linear map with $\nullSp{\mMat{0}} = X$. After\\ all, $[\mMat{0}]$ is the zero matrix.
   \item We shall denote $\mMat{I} \in L(X)$ as the identity map on $X$. After all, $[\mMat{I}]$ is the\\ identity matrix.
\end{enumerate}

\mySepTwo

Since an $m\times n$ matrix can be thought of as a list of $m\cdot n$ numbers, the "natural" norm to equip $\mathcal{M}_{m\times n}(\mathbb{R})$ with is:\\ [-22pt]

{\centering $\|[\mMat{A}]\|_F = \left(\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n}(a_{i,j})^2\right)^{\frac{1}{2}}$\\[-2pt]\par}


{\begin{myTindent}\myComment
   Note on my notation:
   \begin{myIndent}
      Since I view $|\cdot|$ as having already been reserved for the absolute\\ value function, I am not going to use the same notation as Rudin\\ and my professor use for this matrix norm. Rather, because this\\ norm is also called the \ul{Frobenius norm}, I shall denote it by $\|\cdot\|_F$.\retTwo
   \end{myIndent}

   Also, this is a valid norm for the same reasons that the vector Euclidean\\ norm is a valid norm.\retTwo
\end{myTindent}}

\newpage

If we define $d([\mMat{B}], [\mMat{A}]) = \|[\mMat{B}] - [\mMat{A}]\|_F$, then we can treat $\mathcal{M}_{m\times n}(\mathbb{R})$ as a\\ metric space with the metric $d$.\retTwo

{\begin{myIndent}\hTwo
   \uuline{Proposition}: Using the standard bases for $\mathbb{R}^n$ and $\mathbb{R}^m$, we have that for any\\ associated linear map $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$ and matrix $[\mMat{A}] \in \mathcal{M}_{m\times n}(\mathbb{R})$ with\\ coefficients $a_{i,j}$ for $1 \leq i \leq m$ and $1 \leq j \leq n$:

   {\center $\|\mMat{A}\| \leq \|[\mMat{A}]\|_F $\retTwo\par}
   
   {\begin{myIndent}\hThree
      Proof:\\
      Let $\mVec{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n$. Then by the Cauchy-Schwarz inequality:    
      \begin{center}
         \begin{tabular}{l}
            $\|\mMat{A}\mVec{x}\|^2 = \sum\limits_{i=1}^m\left(\sum\limits_{j=1}^n a_{i,j}x_j\right)^2 \leq \sum\limits_{i=1}^m\left(\sum\limits_{j=1}^n a_{i,j}^2 \cdot \sum\limits_{j=1}^n x_j^2\right) = \|\mVec{x}\|^2 \cdot \sum\limits_{i=1}^m\sum\limits_{j=1}^n a_{i,j}^2$
         \end{tabular}\retTwo
      \end{center}

      So $\|\mMat{A}\mVec{x}\|^2 \leq \|\mVec{x}\|^2\cdot \|[\mMat{A}]\|_F^2$. Or in other words, $\|\mMat{A}\|^2 \leq 1\cdot \|[\mMat{A}]\|_F^2$.\retTwo
   \end{myIndent}}

   \uuline{Corollary 1}: Using the standard bases for $\mathbb{R}^n$ and $\mathbb{R}^m$, consider any matrix\\ $[\mMat{A}] \in \mathcal{M}_{m\times n}(\mathbb{R})$. Then the mapping $[\mMat{A}] \mapsto \mMat{A}$ is continuous.\retTwo

   {\begin{myIndent}\hThree
      Proof:\\
      Pick any matrices $[\mMat{A}], [\mMat{B}] \in \mathcal{M}_{n\times n}(\mathbb{R})$ and let $\varepsilon > 0$. Then if\\ $\|[\mMat{B}] - [\mMat{A}]\|_F < \varepsilon$, we have that $\|\mMat{B} - \mMat{A}\| \leq \|[\mMat{B}] - [\mMat{A}]\|_F < \varepsilon$.\retTwo
   \end{myIndent}}

   \uuline{Corollary 2}: Suppose that $S$ is a matric space, that $a_{1,1}, \ldots, a_{m,n}$ are real\\ continuous functions on $S$, and that for each $p \in S,\myHS \mMat{A}_p$ is the linear map\\ from $\mathbb{R}^n$ to $\mathbb{R}^m$ whose associated matrix is:
   
   {\center$[\mMat{A}_p] =\begin{bmatrix}
      a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
      a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{m,1} & a_{m,2} & \cdots & a_{m,n}
   \end{bmatrix}$\retTwo\par}

   Then the mapping $p \mapsto \mMat{A}_p$ is a continuous mapping of $S$ into $L(\mathbb{R}^n, \mathbb{R}^m)$.\retTwo

   {\begin{myIndent}\hThree
      Proof:\\
      Firstly, the mapping $p \mapsto [\mMat{A}_p]$ is continuous for the same reason that a\\ vector valued function is continuous if and only if its component functions\\ are all continuous. Additionally, the mapping $[\mMat{B}] \mapsto \mMat{B}$ is continuous. Thus,\\ because the composition of two continuous functions is itself continuous,\\ we have that the mapping $p \mapsto \mMat{A}_p$ is continuous.
   \end{myIndent}}
\end{myIndent}}

\newpage

To finish this lecture, here is one last helpful fact:

{\begin{myIndent}\hTwo
   \uuline{Lemma}: For any associated map $\mMat{A} \in L(X)$ and matrix $[\mMat{A}] \in \mathcal{M}_{n\times n}(\mathbb{R})$,\\ we have that $\mMat{A}$ is invertible if and only if $[\mMat{A}]$ is invertible.\\ [-6pt]

   {\begin{myIndent}\hThree
      Proof:\\
      If $\mMat{B}$ exists such that $\mMat{B}\mMat{A} = \mMat{I}$ and $\mMat{A}\mMat{B} = \mMat{I}$, then we have that\\ $[\mMat{B}][\mMat{A}] = [\mMat{I}]$ and $[\mMat{A}][\mMat{B}] = [\mMat{I}]$. So $[\mMat{B}] = [\mMat{A}]^{-1}$.\retTwo
      
      Similarly, if $[\mMat{B}]$ exists such that $[\mMat{B}][\mMat{A}] = [\mMat{I}]$ and $[\mMat{A}][\mMat{B}] = [\mMat{I}]$,\\ then we have that $\mMat{BA} = \mMat{I}$ and $\mMat{AB} = \mMat{I}$. Hence, $\mMat{B} = \mMat{A}^{-1}$.\retTwo

      So, we have that $\mMat{A}^{-1}$ exists if and only if $[\mMat{A}]^{-1}$ exists.\\ Also, $[\mMat{A}^{-1}] = [\mMat{A}]^{-1}$.\retTwo
   \end{myIndent}}
\end{myIndent}}

\mySepTwo

\markLecture{4/11/2024}

Suppose that $E$ is an open set in $\mathbb{R}^n$, and that $ f$ is a function from $E$ to $\mathbb{R}^m$.\\ Then consider any $\mVec{x} \in E$. We say that $ f$ is \udefine{differentiable} at $\mVec{x}$ if there exists\\ $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$ such that:

{\center$\lim\limits_{\mVec{h} \rightarrow \mVec{0}}\dfrac{\| f(\mVec{x} + \mVec{h}) -  f(\mVec{x}) - \mMat{A}\mVec{h}\|}{\|\mVec{h}\|} = 0$\retTwo\par}

{\begin{myIndent}\hTwo
   \uuline{Theorem 9.12}: Suppose both $\mMat{A}_1$ and $\mMat{A}_2$ satisfy the above limit. Then $\mMat{A}_1 = \mMat{A}_2$.\\ [-6pt]
   
   {\begin{myIndent}\hThree
      Proof:\\
      Note that:\\ [-12pt]

      {\fontsize{11}{13}
      \begin{tabular}{l}
         $\|\mMat{A}_2\mVec{h} - \mMat{A}_1\mVec{h}\| = \|\mMat{A}_2\mVec{h} +  f(\mVec{x} + \mVec{h}) -  f(\mVec{x} + \mVec{h}) +  f(\mVec{x}) -  f(\mVec{x}) - \mMat{A}_1\mVec{h}\|$\\
         $\phantom{\|\mMat{A}_2\mVec{h} - \mMat{A}_1\mVec{h}\|} \leq \|\mMat{A}_2\mVec{h} +  f(\mVec{x}) -  f(\mVec{x} + \mVec{h})\| + \| f(\mVec{x} + \mVec{h}) -  f(\mVec{x}) - \mMat{A}_1\mVec{h}\|$\\
         $\phantom{\|\mMat{A}_2\mVec{h} - \mMat{A}_1\mVec{h}\|}  = \| f(\mVec{x} + \mVec{h}) -  f(\mVec{x}) - \mMat{A}_2\mVec{h}\| + \| f(\mVec{x} + \mVec{h}) -  f(\mVec{x}) - \mMat{A}_1\mVec{h}\|$
      \end{tabular}}\retTwo

      It then follows that $\lim\limits_{\mVec{h}\rightarrow \mVec{0}}\frac{\|\mMat{A}_2\mVec{h} - \mMat{A}_1\mVec{h}\|}{\|\mVec{h}\|} = 0$.\retTwo

      So, let's fix $\mVecAst{h}_0 \in \mathbb{R}^n \setminus \{\mVec{0}\}$. Then we know that $\lim\limits_{t\rightarrow 0}\frac{\|(\mMat{A}_2 - \mMat{A}_1)t\mVecAst{h}_0\|}{\|t\mVecAst{h}_0\|} = 0$.\\ [3pt]
      But note that $\frac{\|(\mMat{A}_2 - \mMat{A}_1)t\mVecAst{h}_0\|}{\|t\mVecAst{h}_0\|} = \frac{|t|\|(\mMat{A}_2 - \mMat{A}_1)\mVecAst{h}_0\|}{|t|\|\mVecAst{h}_0\|} = \frac{\|(\mMat{A}_2 - \mMat{A}_1)\mVecAst{h}_0\|}{\|\mVecAst{h}_0\|}$.\\ [3pt]
      Thus $\lim\limits_{t\rightarrow 0}\frac{\|(\mMat{A}_2 - \mMat{A}_1)t\mVecAst{h}_0\|}{\|t\mVecAst{h}_0\|} = \frac{\|(\mMat{A}_2 - \mMat{A}_1)\mVecAst{h}_0\|}{\|\mVecAst{h}_0\|}$ for all $\mVecAst{h}_0 \in \mathbb{R}^n \setminus \{\mVec{0}\}$.\\ [3pt]
      Thus we know that $(\mMat{A}_2 - \mMat{A}_1)\mVecAst{h}_0 = \mVec{0}$ for all $\mVec{h_0} \in \mathbb{R}$. Or in other\\ words, $\mMat{A}_2 = \mMat{A}_1$.\retTwo
   \end{myIndent}}
\end{myIndent}}

\newpage

Since any $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$ satisfying that $\lim\limits_{\mVec{h} \rightarrow \mVec{0}}\frac{\| f(\mVec{x} + \mVec{h}) -  f(\mVec{x}) - \mMat{A}\mVec{h}\|}{\|\mVec{h}\|} = 0$ is\\ [2pt] unique, we denote $f^\prime(\mVec{x}) = \mMat{A}$ and call $f^\prime(\mVec{x})$ the \udefine{differential} of $f$ at $\mVec{x}$.\retTwo\retTwo

Notes:\\ [-22pt]
\begin{itemize}
   \item If $f$ is differentiable on all of $E$, then we say $f$ is "differentiable in" $E$. In that\\ case, note that $f^\prime$ can be interpreted as a function from $E$ to $L(\mathbb{R}^n, \mathbb{R}^m)$.
   \item If we define $r(\mVec{h}) = f(\mVec{x} + \mVec{h}) - f(\mVec{x}) - f^\prime(\mVec{x})\mVec{h}$, then we\\ [-3pt] can say that $f(\mVec{x} + \mVec{h}) - f(\mVec{x}) = f^\prime(x)\mVec{h} + r(\mVec{h})$ where $\lim\limits_{\mVec{h}\rightarrow\mVec{0}}\frac{\|r(\mVec{h})\|}{\|\mVec{h}\|} = 0$.
   {\begin{myIndent}\hTwo
      \begin{itemize}
         \item[$\circ$] \uuline{Proposition}: If $f$ is differentiable at $\mVec{x}$, then $f$ is continuous at $\mVec{x}$.
         
         {\begin{myIndent}\hThree
            Proof:\\
            \begin{tabular}{l}
               $\|f(\mVec{x} + \mVec{h}) - f(\mVec{x})\| = \|f^\prime(x)\mVec{h} + r(\mVec{h})\|$ \\ [3pt]
               $\phantom{\|f(\mVec{x} + \mVec{h}) - f(\mVec{x})\|} \leq \|f^\prime(x)\mVec{h}\| + \|r(\mVec{h})\|$ \\ [3pt]
               $\phantom{\|f(\mVec{x} + \mVec{h}) - f(\mVec{x})\|} = \|f^\prime(x)\mVec{h}\| + \|\mVec{h}\|\frac{\|r(\mVec{h})\|}{\|\mVec{h}\|}$
            \end{tabular}\retTwo

            Now because any $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$ is uniformly continuous, we\\ know that $f^\prime(\mVec{x})\mVec{h} \rightarrow \mVec{0}$ as $\mVec{h} \rightarrow \mVec{0}$. Also, since both $\|\mVec{h}\|$ and\\ $\frac{\|r(\mVec{h})\|}{\|\mVec{h}\|}$ approach $\mVec{0}$ as $\mVec{h} \rightarrow \mVec{0}$, we know their product does as well.\retTwo
            
            So by comparison we know that $\lim\limits_{\mVec{h}\rightarrow \mVec{0}}\hspace{-0.3em}\|f(\mVec{x} + \mVec{h}) - f(\mVec{x})\| = 0$.\\ Hence, $f(\mVec{y}) \rightarrow f(\mVec{x})$ as $\mVec{y} \rightarrow \mVec{x}$, which means that $f$ is\\ continuous at $\mVec{x}$.\retTwo
         \end{myIndent}}
      \end{itemize}
   \end{myIndent}}
\end{itemize}

\mySepTwo

Here are some simple facts whose proofs are trivial.
{\begin{myIndent}\hTwo
   \begin{itemize}
      \item \uuline{Sum Rule}: 
      \begin{myIndent}
         Suppose that both $f$ and $g$ are functions going into $\mathbb{R}^m$, and that both\\ are differentiable at $\mVec{x} \in \mathbb{R}^n$. Then $(f + g)^\prime(\mVec{x}) = f^\prime(\mVec{x}) + g^\prime(\mVec{x})$.\retTwo
      \end{myIndent}
   
      \item \uuline{Scalar Multiplication Rule}:
      \begin{myIndent}
         Suppose that $f$ is a function going into $\mathbb{R}^m$ that is differentiable at\\ $\mVec{x} \in \mathbb{R}^n$, and that $c \in \mathbb{R}$. Then $(cf)^\prime(\mVec{x}) = cf^\prime(\mVec{x})$.\retTwo
      \end{myIndent}
   
      \item If $\mMat{A} \in L(\mathbb{R}^n, \mathbb{R}^m)$, then for all $\mVec{x} \in \mathbb{R}^n$, $\myHS\mMat{A}^\prime(\mVec{x}) = \mMat{A}$.
   \end{itemize}

   \newpage

   \uuline{Theorem 9.15 (The Chain Rule)}: Suppose that $E \subseteq \mathbb{R}^m$ is open and that\\ [1pt ]$f: E \longrightarrow \mathbb{R}^m$ is differentiable at $\mVecAst{x}_0 \in E$. Also suppose that $f(\mVecAst{x}_0)$ is in\\ [1pt] an open subset of $f(E)$, and that $g: f(E) \longrightarrow \mathbb{R}^k$ is differentiable at\\ [1pt] $f(\mVec{x}) = \mVecAst{y}_0$. Then $F = g \circ f$ is differentiable at $\mVecAst{x}_0$ and:

   {\center $F^\prime(\mVecAst{x}_0) = g^\prime(f(\mVecAst{x}_0)) f^\prime(\mVecAst{x}_0)$ \retTwo\par}

   {\begin{myIndent}\hThree
      Proof:\\
      Set $\mMat{A} = f^\prime(\mVecAst{x}_0)$ and $\mMat{B} = g^\prime(\mVecAst{y}_0)$. Then define the functions:
      \begin{itemize}
         \item $u(\mVec{h}) = f(\mVecAst{x}_0 + \mVec{h}) - f(\mVecAst{x}_0) - \mMat{A}\mVec{h}$
         \item $v(\mVec{k}) = g(\mVecAst{y}_0 + \mVec{k}) - g(\mVecAst{y}_0) - \mMat{B}\mVec{k}$\\ [-10pt]
      \end{itemize}

      Next, we define the function $\eta(\mVec{k}) = \left\{
         \begin{matrix}
            \frac{\|v(\mVec{k})\|}{\|\mVec{k}\|} & \text{ if } v(\mVec{k}) \neq \mVec{0} \\
            0 & \text{ if } v(\mVec{k}) = \mVec{0}
         \end{matrix}\right.$\\ [4pt]

      Then, we always have that $\|\mVec{k}\|\eta(\mVec{k}) = \|v(\mVec{k})\|$. Also, $\eta$ is continuous at\\ $\mVec{k} = 0$. After all $v(\mVec{0}) = \mVec{0}$ and $\frac{\|v(\mVec{k})\|}{\|\mVec{k}\|} \rightarrow 0$ as $\mVec{k} \rightarrow \mVec{0}$.\retTwo

      With all that setup out of the way, we now need to show that:
      
      {\centering$\lim\limits_{\mVec{h} \rightarrow \mVec{0}}\frac{\|F(\mVecAst{x}_0 + \mVec{h}) - F(\mVecAst{x}_0) - \mMat{BA}\mVec{h}\|}{\|\mVec{h}\|} = 0$. \retTwo\par}


      So, put $\mVec{k} = f(\mVec{x} + \mVec{h}) - f(\mVec{x})$ and note that:
      \begin{center}
         \begin{tabular}{l}
            $F(\mVecAst{x}_0 + \mVec{h}) - F(\mVecAst{x}_0) - \mMat{BA}\mVec{h} = g(\mVecAst{y}_0 + \mVec{k}) - g(\mVecAst{y}_0) - \mMat{B}(\mMat{A}\mVec{h})$ \\ [1pt]
            $\phantom{F(\mVecAst{x}_0 + \mVec{h}) - F(\mVecAst{x}_0) - \mMat{BA}\mVec{h}} = \mMat{B}\mVec{k} + v(\mVec{k}) - \mMat{B}(\mMat{A}\mVec{h})$\\ [1pt]
            $\phantom{F(\mVecAst{x}_0 + \mVec{h}) - F(\mVecAst{x}_0) - \mMat{BA}\mVec{h}} = v(\mVec{k}) + \mMat{B}(\mVec{k} - \mMat{A}\mVec{h})$\\ [1pt]
         \end{tabular}\retTwo
      \end{center}

      Meanwhile, notice that $\mVec{k} = \mMat{A}\mVec{h} + u(\mVec{h})$. Therefore, we have that:
      {\begin{center}\fontsize{12}{14}\selectfont
         \begin{tabular}{l}
            $\frac{\|F(\mVecAst{x}_0 + \mVec{h}) - F(\mVecAst{x}_0) - \mMat{BA}\mVec{h}\|}{\|\mVec{h}\|} = \frac{\|v(\mVec{k}) + \mMat{B}(\mVec{k} - \mMat{A}\mVec{h})\|}{\|\mVec{h}\|}$\\ [9pt]
            
            $\phantom{\frac{\|F(\mVecAst{x}_0 + \mVec{h}) - F(\mVecAst{x}_0) - \mMat{BA}\mVec{h}\|}{\|\mVec{h}\|}} \leq \frac{\|v(\mMat{A}\mVec{h} + u(\mVec{h}))\|}{\|\mVec{h}\|} + \frac{\|\mMat{B}(u(\mVec{h}))\|}{\|\mVec{h}\|}$\\ [9pt]

            $\phantom{\frac{\|F(\mVecAst{x}_0 + \mVec{h}) - F(\mVecAst{x}_0) - \mMat{BA}\mVec{h}\|}{\|\mVec{h}\|}} \leq \frac{\|\mMat{A}\mVec{h} + u(\mVec{h})\|}{\|\mVec{h}\|}\eta(\mMat{A}\mVec{h} + u(\mVec{h})) + \|\mMat{B}\|\frac{\|u(\mVec{h})\|}{\|\mVec{h}\|}$\\ [9pt]

            $\phantom{\frac{\|F(\mVecAst{x}_0 + \mVec{h}) - F(\mVecAst{x}_0) - \mMat{BA}\mVec{h}\|}{\|\mVec{h}\|}} \leq \left(\frac{\|\mMat{A}\|\|\mVec{h}\|}{\|\mVec{h}\|} + \frac{\|u(\mVec{h})\|}{\|\mVec{h}\|}\right)\eta(\mVec{k}) + \|\mMat{B}\|\frac{\|u(\mVec{h})\|}{\|\mVec{h}\|}$\\ [9pt]
         \end{tabular}\retTwo
      \end{center}}

      Now we know $f$ is continuous at $\mVecAst{x}_0$ because $f$ is also differentiable there.\\ So, $\mVec{k} = f(\mVecAst{x}_0 + \mVec{h}) - f(\mVecAst{x}_0) \rightarrow 0$ as $\mVec{h} \rightarrow 0$. That combined with the\\ fact that $\eta$ is continuous at $\mVec{k} = \mVec{0}$ means that $\eta(\mVec{k}) \rightarrow 0$ as $\mVec{h} \rightarrow 0$.

      \newpage

      Combining that with the fact that $\frac{\|u(\mVec{h})\|}{\|\mVec{h}\|} \rightarrow 0$ as $\mVec{h} \rightarrow \mVec{0}$, we know that:

      {\centering $\left(\frac{\|\mMat{A}\|\|\mVec{h}\|}{\|\mVec{h}\|} + \frac{\|u(\mVec{h})\|}{\|\mVec{h}\|}\right)\eta(\mVec{k}) + \|\mMat{B}\|\frac{\|u(\mVec{h})\|}{\|\mVec{h}\|} \rightarrow 0$ as $\mVec{h} \rightarrow 0$.\retTwo\par}
      
      Hence, we can conclude by comparison that:
      
      {\centering$\lim\limits_{\mVec{h} \rightarrow \mVec{0}}\frac{\|F(\mVecAst{x}_0 + \mVec{h}) - F(\mVecAst{x}_0) - \mMat{BA}\mVec{h}\|}{\|\mVec{h}\|} = 0$. \retTwo\par}
   \end{myIndent}}
\end{myIndent}}

\mySepTwo

Let $E \subseteq \mathbb{R}^n$ be open and consider a function $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$. Also let\\ $\{e_1, \ldots, e_n\}$ and $\{u_1, \ldots, u_m\}$ be the standard bases of $\mathbb{R}^n$ and $\mathbb{R}^m$\\ respectively. Then, expressing $f$ in terms of its component functions,\\ we have that:

{\centering $f(\mVec{x}) = \sum\limits_{i=1}^mf_{i}(\mVec{x})u_i = (f_1(\mVec{x}), \ldots, f_m(\mVec{x}))$.\retTwo\par}

Equivalently, we can write that $f_i(\mVec{x}) = f(\mVec{x}) \cdot u_i$.\retTwo

Now for each $1 \leq j \leq n$ and $1 \leq i \leq m$, we define:\\ [-26pt]

\[(D_jf_i)(\mVec{x}) = \lim\limits_{t\rightarrow 0}\frac{f_i(\mVec{x} + te_j) - f_i(\mVec{x})}{t}\]

\phantom{.}\\Each of these $D_jf_i$ are called \udefine{partial derivatives}.\retTwo


\begin{myTindent}\teachComment
   Note that to calculate the partial derivative $(D_jf_i)(\mVec{x})$, all you need to do\\ is treat all the components of $\mVec{x}$ as constant except the $j$th component.\\ Then, the partial derivative is just a single variable limit with respect to\\ that component.\retTwo
\end{myTindent}

{\begin{myIndent}\hTwo
   \uuline{Theorem 9.17}: Suppose $E \subseteq \mathbb{R}^n$ is open and that $f: E \longrightarrow \mathbb{R}^m$ is differentiable\\ [1pt] at a point $\mVec{x} \in E$. Then all partial derivatives $(D_jf_i)(\mVec{x})$ exist and:

   {\center $f^\prime(\mVec{x})e_j = \sum\limits_{i=1}^k(D_jf_i)(x)u_i$.\\[-6pt]\par}
   
   {\begin{myIndent}\hThree
      Proof:\\
      Fix $j \in \{1, \ldots, n\}$ and note that since $f$ is differentiable, we have that\\ $f(\mVec{x} + te_j) - f(\mVec{x}) = f^\prime(\mVec{x})(te_j) + r(te_j)$ such that $\frac{\|r(te_j)\|}{\|te_j\|} \rightarrow 0$ as $t \rightarrow 0$.\retTwo

      Then as $\|te_j\| = |t|$, we have that $\left\|\frac{r(te_j)}{t}\right\| = \frac{\|r(te_j)\|}{|t|} = \frac{\|r(te_j)\|}{\|te_j\|} \rightarrow 0$ as\\ [-2pt] $t \rightarrow 0$. Thus, $\frac{r(te_j)}{t} \rightarrow \mVec{0}$ as $t \rightarrow 0$. And since $f^\prime(\mVec{x})(te_j) = tf^\prime(\mVec{x})e_j$,\\ we have that: $\frac{f^\prime(\mVec{x})(te_j)}{t} + \frac{r(te_j)}{t} \rightarrow f^\prime(\mVec{x})e_j$ as $t \rightarrow 0$.\retTwo

      So, we now know that $\lim\limits_{t\rightarrow 0}\frac{f(\mVec{x} + te_j) - f(\mVec{x})}{t} = f^\prime(\mVec{x})(e_j)$.

      \newpage

      Next, consider that:
      \begin{center}
         \begin{tabular}{l}
            $(f^\prime(x)e_j) \cdot u_i = u_i \cdot \lim\limits_{t\rightarrow 0}\frac{f(\mVec{x} + te_j) - f(\mVec{x})}{t}$\\ [7pt]
            $\phantom{(f^\prime(x)e_j) \cdot u_i} = \lim\limits_{t\rightarrow 0}\left(u_i \cdot \frac{f(\mVec{x} + te_j) - f(\mVec{x})}{t}\right)$\\ [7pt]
            $\phantom{(f^\prime(x)e_j) \cdot u_i} = \lim\limits_{t\rightarrow 0}\frac{(f(\mVec{x} + te_j) \cdot u_i) - (f(\mVec{x}) \cdot u_i)}{t}$\\ [7pt]
            $\phantom{(f^\prime(x)e_j) \cdot u_i} = \lim\limits_{t\rightarrow 0}\frac{f_i(\mVec{x} + te_j) - f_i(\mVec{x})}{t} = (D_jf_i)(\mVec{x})$\\ [7pt]
         \end{tabular}\retTwo
      \end{center}

      It immediately follows that $f^\prime(\mVec{x})e_j = \sum\limits_{i=1}^k(D_jf_i)(x)u_i$.\retTwo
   \end{myIndent}}
\end{myIndent}}

As a result of the above theorem, we have that if $f$ is differentiable at $\mVec{x}$, then\\ when using $\{e_1, \ldots, e_n\}$ and $\{u_1, \ldots, u_m\}$ as our bases for $\mathbb{R}^n$ and $\mathbb{R}^m$ respectively,\\ then:

{\centering$[f^\prime(\mVec{x})] = \begin{bmatrix}
   (D_1f_1)(\mVec{x}) & (D_2f_1)(\mVec{x}) & \cdots & (D_nf_1)(\mVec{x}) \\
   (D_1f_2)(\mVec{x}) & (D_2f_2)(\mVec{x}) & \cdots & (D_nf_2)(\mVec{x}) \\
   \vdots & \vdots & \ddots & \vdots \\
   (D_1f_m)(\mVec{x}) & (D_2f_m)(\mVec{x}) & \cdots & (D_nf_m)(\mVec{x})
\end{bmatrix}$\retTwo\par}

However, as I'm about to demonstrate, the converse of theorem 9.17 is not true.\\ Thus, we can't automatically rely on calculating the partial derivatives of $f$ to find\\ the differential of $f$ at $\mVec{x}$.\retTwo


{\begin{myIndent}\exOne
   \textbf{Exercise 9.6}: For $(x, y) \in \mathbb{R}^2$, define $f(x, y) = \left\{
   \begin{matrix}
      \frac{xy}{x^2 + y^2} & \text{ if } (x, y) \neq (0, 0) \\
      0 & \text{ if } (x, y) = (0, 0)
   \end{matrix}\right.$\\ [6pt]
   Then we can show that the partial derivatives of $f$ exist at every point\\ of $\mathbb{R}^2$.
   {\begin{myIndent}\exTwo
      Clearly, we have that $(D_1f)(x, y) = \frac{y(x^2 + y^2)}{(x^2 + y^2)^2}$ and $(D_2f)(x, y) = \frac{x(x^2 + y^2)}{(x^2 + y^2)^2}$\\ when $(x, y) \neq 0$. Meanwhile, at $(x, y) = 0$ we have when $h \neq 0$ that:
      {\begin{center}\fontsize{12}{14}\selectfont
         \begin{tabular}{c c c}
            $\frac{f(h, 0) - f(0, 0)}{h} = \frac{\frac{0}{h^2 + 0} -\hspace{0.1em} 0}{h} = 0$ & and & $\frac{f(0, h) - f(0, 0)}{h} = \frac{\frac{0}{0 + h^2} -\hspace{0.1em}0}{h} = 0$
         \end{tabular}\retTwo
      \end{center}}

      Therefore:\\ [-16pt]

      \begin{center}{\fontsize{11.7}{13}\selectfont
         \begin{tabular}{c c c}
            $(D_1f)(0, 0) = \lim\limits_{h\rightarrow 0}\frac{f(h, 0) - f(0, 0)}{h} = 0$ & and & $(D_2f)(0, 0) = \lim\limits_{h\rightarrow 0}\frac{f(0, h) - f(0, 0)}{h} = 0$
         \end{tabular}}\retTwo
      \end{center}
   \end{myIndent}}

   That said, $f(x, y)$ isn't even continuous at $(0, 0)$.
   {\begin{myIndent}\exTwo
      Whenever $y = x$, we have that $f(x, y) = \frac{1}{2}$. Hence, given the\\ sequence $(x_n, y_n) = (\frac{1}{n}, \frac{1}{n})$, we have that $(x_n, y_n) \rightarrow (0, 0)$ and that\\ $(x_n, y_n) \neq (0, 0)$ for any $n \in \mathbb{Z}_+$. But, $f(x_n, y_n) \not\rightarrow 0$.\retTwo
      
      Thus, $\lim\limits_{
      \begin{smallmatrix}
         x\rightarrow 0\\ y\rightarrow 0
      \end{smallmatrix}}f(x, y) \neq f(0, 0)$, which means $f$ is not continuous at $(0, 0)$.\retTwo
   \end{myIndent}}

   Since continuity is necessary for differentiability, this also demonstrates\\ that the existence of partial derivatives does not imply a function is\\ differentiable.\retTwo
\end{myIndent}}

\mySepTwo

Let $\gamma$ be a differentiable mapping from $(a, b) \subset \mathbb{R}$ to $E \subseteq \mathbb{R}^n$ where $E$ is open\\ and $a$ and $b$ are finite. Also let $f: E \longrightarrow \mathbb{R}$ be a differentiable function. Finally,\\ define $g(t) = f(\gamma(t))$ for $a < t < b$. Then we know $g$ is differentiable and that:

{\centering $g^\prime(t) = f^\prime(\gamma(t))\gamma^\prime(t)$. \retTwo\par}


Since $g^\prime$ is a real function, letting $\gamma(t) = (\gamma_1(t), \ldots, \gamma_n(t))$ we can rewrite the\\ above expression as:

{\centering $g^\prime(t) = \sum\limits_{i=1}^n D_if(\gamma(t))\gamma^\prime_i(t)$ \retTwo\par}

Now, this situation is so common that we have special notation just for it.\\ Letting $\{e_1, \ldots, e_n\}$ be the standard basis for $\mathbb{R}^n$, we define the \udefine{gradient}\\ of $f$ at $\mVec{x}$ as:

{\centering$\nabla f(\mVec{x}) = \sum\limits_{i=1}^n D_if(\mVec{x})e_i$\retTwo\par}

Thus, $g^\prime(t) = \nabla f(\gamma(t)) \cdot \gamma^\prime(t)$.\retTwo

Now suppose that $\gamma: (a, b) \longrightarrow \mathbb{R}^n$ is defined such that $a < 0 < b$, $\mVec{x} \in E$,\\ and $\gamma(t) = \mVec{x} + t\mVec{u}$ where $\mVec{u}$ is a unit vector. Then we define the \udefine{directional\\ derivative} of $f$ at $\mVec{x}$ in the direction of $\mVec{u}$ as:

{\centering $D_{\mVec{u}}f(\mVec{x}) = \lim\limits_{t\rightarrow 0}f(\gamma(t)) = \nabla f(\mVec{x}) \cdot \mVec{u}$\retTwo\par}


{\begin{myIndent}\hTwo
   From this it is really trivial to see that $|D_uf(\mVec{x})|$ is maximized when $\mVec{u}$ is\\ a scalar multiple of $\nabla f(\mVec{x})$.\retTwo
\end{myIndent}}

To finish off lecture, note that if $E \subseteq \mathbb{R}^n$ is open and $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ is\\ differentiable and written as $f(\mVec{x}) = (f_1(\mVec{x}), \ldots, f_m(\mVec{x}))$, then using\\ the standard bases for $\mathbb{R}^n$ and $\mathbb{R}^m$, we can abuse notation and say that:

{\center $[f^\prime(x)] = 
\begin{bmatrix}
   \nabla f_1(\mVec{x})\\
   \vdots \\
   \nabla f_m(\mVec{x})\\
\end{bmatrix}$ \par}

\newpage

\markLecture{4/16/2024}

A set $E \subseteq \mathbb{R}^k$ is \udefine{convex} if $t\mVec{x} + (1 - t)\mVec{y} \in E$ whenever $x \in E$, $y \in E$,\\ and $0 < t < 1$.\retTwo

{\begin{myIndent}\hTwo
   \uuline{Theorem 9.19}: Suppose that $E \subseteq \mathbb{R}^n$ is convex and open, that $f: E \longrightarrow \mathbb{R}^m$\\ [4pt] is differentiable, and there is a real number $M$ such that $\|f^\prime(x)\| \leq M$ for\\ [0pt] every $x \in E$. Then $\|f(\mVec{b}) - f(\mVec{a})\| \leq M\|\mVec{b} - \mVec{a}\|$ for all $\mVec{a}, \mVec{b} \in E$.\\ [-6pt]

   {\begin{myIndent}\hThree
      Proof:\\
      Fix $\mVec{a}, \mVec{b} \in E$ and define $\gamma(t) = (1 - t)\mVec{a} + t\mVec{b}$ for $0 < t < 1$. Next,\\ define $\mVec{g}(t) = f(\gamma(t))$ for $0 < t < 1$. Since $E$ is both convex and\\ differentiable, we know that both $\mVec{g}$ and $\mVec{g}^\prime$ are well defined on the\\ interval $(0, 1)$. So, by the mean value theorem for vector-valued\\ functions (proposition 93 in the Math 140B notes), we know that\\ for some $x \in (0, 1)$:

      {\centering $\|\mVec{g}(1) - \mVec{g}(0)\| \leq |1 - 0|\|\mVec{g}^\prime(x)\|$\retTwo\par}

      Or in other words, $\|f(\mVec{b}) - f(\mVec{a})\| \leq \|f^\prime(\gamma(x))\gamma^\prime(x)\| \leq M\|\mVec{b} - \mVec{a}\|$.\retTwo
   \end{myIndent}}

   \uuline{Corollary}: If additionally $f^\prime(\mVec{x}) = \mMat{0}$ for all $\mVec{x} \in E$, then $f$ is constant.\\ [-6pt]

   {\begin{myIndent}\hThree
      Proof:\\
      If $f^\prime(\mVec{x}) = \mMat{0}$ for all $\mVec{x}$, then $M = 0$. So $\|f(\mVec{b}) - f(\mVec{a})\| = 0$ for all\\ $\mVec{a}, \mVec{b} \in E$.\retTwo
   \end{myIndent}}
\end{myIndent}}

\mySepTwo

Let $E \subseteq \mathbb{R}^n$ be an open set and $f: E \longrightarrow \mathbb{R}^m$ be differentiable. Then $f$ is called\\ \udefine{continuously differentiable} if $f^\prime: E \longrightarrow L(\mathbb{R}^n, \mathbb{R}^m)$ is continuous. Or in other\\ words, $\forall \varepsilon > 0,\myHS \exists \delta > 0\suchthat \|\mVec{y} - \mVec{x}\| < \delta \Longrightarrow \|f^\prime(\mVec{y}) - f^\prime(\mVec{x})\| < \varepsilon$ \retTwo

When this is the case, we say $f$ is a \udefine{$\mathscr{C}^1$-mapping} and that $f \in \mathscr{C}^1(E, \mathbb{R}^m)$.\retTwo

{\begin{myIndent}\hTwo
   \uuline{Theorem 9.21}: Suppose $f$ is a function from an open set $E \subseteq \mathbb{R}^n$ to $\mathbb{R}^m$. Then\\ [1pt] $f \in \mathscr{C}^1(E, \mathbb{R}^m)$ if and only if the partial derivatives $D_jf_i$ exist and are\\ [1pt] continuous on $E$ for all $1 \leq i \leq m$ and $1 \leq j \leq n$.\\ [-6pt]

   {\begin{myIndent}\hThree
      Proof:\\
      Let $\{e_1, \ldots, e_n\}$ and $\{u_1, \ldots, u_m\}$ be the standard bases for $\mathbb{R}^n$ and $\mathbb{R}^m$\\ respectively.\retTwo

      ($\Longrightarrow$) Say $f \in \mathscr{C}^1(E, \mathbb{R}^m)$. Then $f$ is differentiable, which means that\\ $(D_jf_i)(\mVec{x}) = (f^\prime(\mVec{x})e_j) \cdot u_i$ exists for all $1 \leq i \leq m$, $\hspace{0.3em}1 \leq j \leq n$,\\ and $\mVec{x} \in E$.

      \newpage

      Thus for any $\mVec{x}, \mVec{y} \in E$, we have that:\\ [-12pt]
      \begin{center}
         \begin{tabular}{l}
            $|(D_jf_i)(\mVec{y}) - (D_jf_i)(\mVec{x})| = |(f^\prime(\mVec{y})e_j) \cdot u_i - (f^\prime(\mVec{x})e_j) \cdot u_i|$\\ [3pt]

            $\phantom{|(D_jf_i)(\mVec{y}) - (D_jf_i)(\mVec{x})|} = |((f^\prime(\mVec{x}) - f^\prime(\mVec{x}))e_j) \cdot u_i|$\\ [3pt]

            $\phantom{|(D_jf_i)(\mVec{y}) - (D_jf_i)(\mVec{x})|} \leq \|((f^\prime(\mVec{x}) - f^\prime(\mVec{x}))e_j)\|\|u_i\|$\\ [3pt]

            $\phantom{|(D_jf_i)(\mVec{y}) - (D_jf_i)(\mVec{x})|} \leq \|f^\prime(\mVec{x}) - f^\prime(\mVec{x})\|\|e_j\|\|u_i\|$\\ [3pt]

            $\phantom{|(D_jf_i)(\mVec{y}) - (D_jf_i)(\mVec{x})|} = \|f^\prime(\mVec{x}) - f^\prime(\mVec{x})\|$
         \end{tabular}\retTwo
      \end{center}

      Now, since $f \in \mathscr{C}^1(E, \mathbb{R}^m)$, we know that for any $\varepsilon > 0$, there exists\\ $\delta > 0$ such that $\|\mVec{y} - \mVec{x}\| < \delta \Longrightarrow \|f^\prime(\mVec{x}) - f^\prime(\mVec{x})\| < \varepsilon$. Thus, by\\ the above inequality, we also have that:
      
      {\centering $\|\mVec{y} - \mVec{x}\| < \delta \Longrightarrow |(D_jf_i)(\mVec{y}) - (D_jf_i)(\mVec{x})| < \varepsilon$.\retTwo\par}

      ($\Longleftarrow$) Firstly, we'll assume that $m = 1$. That way $f$ is a real-valued\\ function.\retTwo

      Fix $\mVec{x} \in E$ and $\varepsilon > 0$. Then because $E$ is open, there exists $r_0 > 0$\\ such that $B_{r_0}(\mVec{x}) \subseteq E$. Additionally, because each partial derivative\\ is continuous, there exists $r_j > 0$ such that:
      
      {\centering $\|\mVec{y} - \mVec{x}\| < r_j \Longrightarrow |(D_jf)(\mVec{y}) - (D_jf)(\mVec{x})| < \frac{\varepsilon}{n}$.\retTwo\par}

      Set $R = \min\{r_0, r_1, \ldots, r_n\}$. Then choose $\mVec{h} = (h_1, \ldots, h_n) \in B_R(\mVec{x})$,\\ and define $\mVecAst{v}_j$ such that $v_0 = \mVec{0}$ and $v_j = \mVecAst{v}_{j-1} + h_je_j$ for each\\ $1 \leq j \leq n$. Then note that: 
      \begin{center}
         {\fontsize{12}{14}\selectfont
            $f(\mVec{x} + \mVec{h}) - f(\mVec{x}) = \sum\limits_{j=1}^n\left(\myVS f(\mVec{x} + \mVecAst{v}_j) - f(\mVec{x} + \mVecAst{v}_{j-1})\right)$
         \retTwo}
      \end{center}

      Now for each $1 \leq j \leq n$, define $\widetilde{f}_j(t) = f(\mVec{x} + \mVecAst{v}_{j-1} + th_je_j)$. Then\\ firstly, $\widetilde{f}_j$ is well defined on the interval $[0, 1]$ because:
      \begin{itemize}
         \item $\widetilde{f}_j(1) = f(\mVec{x} + \mVecAst{v}_j) \in B_R(\mVec{x})$
         \item $\widetilde{f}_j(0) = f(\mVec{x} + \mVecAst{v}_{j-1}) \in B_R(\mVec{x})$
         \item $B_R(\mVec{x}) \subseteq E$ is convex.
      \end{itemize}\retTwo

      Additionally, $\widetilde{f}_j$ is differentiable on $[0, 1]$ with:
      
      {\center$\widetilde{f}_j^\prime(t) = h_j(D_jf)(\mVec{x} + \mVecAst{v}_{j-1} + th_je_j)$\retTwo\par}
      
      Therefore, by the mean value theorem, there exists $\theta \in (0, 1)$ such that:
      
      {\center $\widetilde{f}_j(1) - \widetilde{f}_j(0) = \widetilde{f}_j^\prime(\theta) = h_j(D_jf)(\mVec{x} + \mVecAst{v}_{j-1} + \theta h_je_j)$\retTwo\par}
      
      Importantly, we have that $(\mVec{x} + \mVecAst{v}_{j-1} + \theta h_je_j) \in B_R(\mVec{x})$. Therefore:

      {\center $|(D_jf)(\mVec{x} + \mVecAst{v}_{j-1} + \theta h_je_j) - (D_jf)(\mVec{x})| < \frac{\varepsilon}{n}$\retTwo\par}

      In turn, this means that:

      {\center{\fontsize{12}{14}\selectfont
      \begin{tabular}{l}
         $|f(\mVec{x} + \mVecAst{v}_j) - f(\mVec{x} + \mVecAst{v}_{j-1}) - h_j(D_jf)(\mVec{x})|$ \\
         $\phantom{aaaaaaaaaaa} = |h_j(D_jf)(\mVec{x} + \mVecAst{v}_{j-1} + \theta h_je_j) - h_j(D_jf)(\mVec{x})| < |h_j|\frac{\varepsilon}{n}$
      \end{tabular}}\par}

      \newpage

      So now, we can say that:

      {\center{\fontsize{12}{14}\selectfont
      \begin{tabular}{l}
         $\left|f(\mVec{x} + \mVec{h}) - f(\mVec{x}) - \sum\limits_{j=1}^nh_j(D_jf)(\mVec{x})\right|$ \\ [14pt] 
         $\phantom{aaaaaaaaa} = \left|\sum\limits_{j=1}^n\left(\myVS f(\mVec{x} + \mVecAst{v}_j) - f(\mVec{x} + \mVecAst{v}_{j-1}) - h_j(D_jf)(\mVec{x})\right)\right|$ \\ [14pt]
         $\phantom{aaaaaaaaa} \leq \sum\limits_{j=1}^n\left|\myVS f(\mVec{x} + \mVecAst{v}_j) - f(\mVec{x} + \mVecAst{v}_{j-1}) - h_j(D_jf)(\mVec{x})\right|$ \\ [14pt]
         $\phantom{aaaaaaaaaaaaaaaaaaaaaa} < \sum\limits_{j=1}^n|h_j|\frac{\varepsilon}{n} \leq \varepsilon\sum\limits_{j=1}^n\frac{\|\mVec{h}\|}{n} = \varepsilon \|\mVec{h}\|$ \\ [14pt]
      \end{tabular}}\retTwo\par}

      And thus we have that: $\lim\limits_{\mVec{h} \rightarrow \mVec{0}}\frac{\left|\myVS\right.f(\mVec{x} + \mVec{h}) - f(\mVec{x}) - \sum\limits_{j=1}^nh_j(D_jf)(\mVec{x})\left.\myVS\right|}{\|\mVec{h}\|} = 0$.\retTwo

      Note that $f^\prime(\mVec{x}) = \sum\limits_{j=1}^nh_j(D_jf)(\mVec{x}) \in L(\mathbb{R}^n, \mathbb{R})$, and that $f^\prime(\mVec{x})$ is the\\ [-2pt] sum of a bunch of continuous functions. Thus, $f^\prime(\mVec{x})$ is itself continuous\\ [5pt] at $\mVec{x}$, meaning that $f$ is a $\mathscr{C}^1$-mapping.\\ [8pt]

      Now that we've shown this theorem holds when $m = 1$, lets consider\\ when $m > 1$. Let $f(\mVec{x}) = (f_1(\mVec{x}), \ldots, f_m(\mVec{x}))$ and then note that $f$\\ is continuous at $\mVec{x}$ if and only if $f_i$ is continuous at $\mVec{x}$ for each\\ $1 \leq i \leq m$. Luckily, we already showed that all $D_jf_i$ being continuous\\ for each $1 \leq j \leq n$ implies that $f_i$ is continuously differentiable. Hence,\\ having all $D_jf_i$ be continuous for each $1 \leq j \leq n$ and $1 \leq i \leq m$ implies\\ that $f \in \mathscr{C}^{1}(E, \mathbb{R}^m)$.\retTwo
   \end{myIndent}}
\end{myIndent}}

\exOne
\mySepTwo

\textbf{Exercise 9.7}: Suppose that $f$ is a real-valued function defined in an open set\\ $E \subseteq \mathbb{R}^n$, and that the partial derivatives $D_1f, \ldots, D_nf$ are defined and bounded\\ in $E$. Then we can prove that $f$ is continuous in $E$.\\ [-6pt]

{\begin{myIndent}\exTwo
   Pick $M > |(D_jf)(\mVec{x})|$ for all $1 \leq j \leq n$ and $\mVec{x} \in E$. Then fix $\mVec{x} \in E$ and $\varepsilon > 0$.\\ [1pt]
   Next, choose $r > 0$ such that $B_r(\mVec{x}) \subseteq E$ and set $R = \min\{r, \frac{\varepsilon}{nM}\}$.\retTwo
   
   Like in the proof above, choose $\mVec{h} = (h_1, \ldots, h_n) \in B_R(\mVec{x})$ and define $\mVecAst{v}_j$\\ [-2pt] such that $v_0 = \mVec{0}$ and $v_j = \mVecAst{v}_{j-1} + h_je_j$ for each $1 \leq j \leq n$. That way:
   \begin{center}
      {\fontsize{12}{14}\selectfont
         $f(\mVec{x} + \mVec{h}) - f(\mVec{x}) = \sum\limits_{j=1}^n\left(\myVS f(\mVec{x} + \mVecAst{v}_j) - f(\mVec{x} + \mVecAst{v}_{j-1})\right)$}
   \end{center}

   \newpage

   Next, for each $1 \leq j \leq n$, define $\widetilde{f}_j(t) = f(\mVec{x} + \mVecAst{v}_{j-1} + th_je_j)$. Then like\\ before, $\widetilde{f}_j$ is well defined on the interval $[0, 1]$ because:
   \begin{itemize}
      \item $\widetilde{f}_j(1) = f(\mVec{x} + \mVecAst{v}_j) \in B_R(\mVec{x})$
      \item $\widetilde{f}_j(0) = f(\mVec{x} + \mVecAst{v}_{j-1}) \in B_R(\mVec{x})$
      \item $B_R(\mVec{x}) \subseteq E$ is convex.
   \end{itemize}\retTwo

   Additionally, $\widetilde{f}_j$ is differentiable on $[0, 1]$ with:
   
   {\center$\widetilde{f}_j^\prime(t) = h_j(D_jf)(\mVec{x} + \mVecAst{v}_{j-1} + th_je_j)$\retTwo\par}
   
   Therefore, by the mean value theorem, there exists $\theta \in (0, 1)$ such that:
   
   {\center $\widetilde{f}_j(1) - \widetilde{f}_j(0) = \widetilde{f}_j^\prime(\theta) = h_j(D_jf)(\mVec{x} + \mVecAst{v}_{j-1} + \theta h_je_j)$\retTwo\par}

   And then because $|D_jf|$ is bounded by $M$:

   {\center{\fontsize{12}{14}\selectfont
   \begin{tabular}{l}
      $|f(\mVec{x} + \mVecAst{v}_j) - f(\mVec{x} + \mVecAst{v}_{j-1})| = |\widetilde{f}_j(1) - \widetilde{f}_j(0)|$ \\[2pt]

      $\phantom{|f(\mVec{x} + \mVecAst{v}_j) - f(\mVec{x} + \mVecAst{v}_{j-1})|} = |h_j(D_jf)(\mVec{x} + \mVecAst{v}_{j-1} + \theta h_je_j)|$\\ [2pt]

      $\phantom{|f(\mVec{x} + \mVecAst{v}_j) - f(\mVec{x} + \mVecAst{v}_{j-1})|} = |h_j||(D_jf)(\mVec{x} + \mVecAst{v}_{j-1} + \theta h_je_j)| < |h_j|M$
   \end{tabular}}\retTwo\par}

   So now:

   {\center{\fontsize{12}{14}\selectfont
      \begin{tabular}{l}
         $|f(\mVec{x} + \mVec{h}) - f(\mVec{x})| \leq \sum\limits_{j=1}^n\left|\myVS f(\mVec{x} + \mVecAst{v}_j) - f(\mVec{x} + \mVecAst{v}_{j-1})\right| < \sum\limits_{j=1}^n|h_j|M \leq \|\mVec{h}\|\cdot nM $
      \end{tabular}}\retTwo\par}

   Hence, since $R = \frac{\varepsilon}{nM}$, we have that $|f(\mVec{x} + \mVec{h}) - f(\mVec{x})| < \varepsilon$ for any\\ $\mVec{h} \in B_R(\mVec{x})$. So, $f$ is continuous.\retTwo
\end{myIndent}}

\mySepTwo

\textbf{Exercise 9.8}: Suppose that $f$ is a differentiable real function on an open set\\ $E \subseteq \mathbb{R}^n$, and that $f$ has a local maximum at a point $x \in E$. Then $f^\prime(x) = \mMat{0}$.\retTwo

{\begin{myIndent}\exTwo
   Proof:\\
   Let $r > 0$ such that $B_r \subseteq E$ and $\|\mVec{h}\| < r \Longrightarrow f(\mVec{x} + \mVec{h}) \leq f(\mVec{x})$. Then pick\\ [-2pt] any such $\mVec{h} \in B_r$ and define $\gamma: (-1, 1) \longrightarrow \mathbb{R}^n$ by $\gamma(t) = \mVec{x} + t\mVec{h}$. Then note\\ [-2pt] that $(f \circ \gamma)^\prime(t) = f^\prime(\mVec{x} + t\mVec{h})\mVec{h}$.\retTwo

   Crucially, we know that $(f \circ \gamma)^\prime$ reaches a local maximum at $t = 0$. And\\ since $f \circ \gamma$ is a function mapping an interval of $\mathbb{R}$ to $\mathbb{R}$, we know from\\ Math 140B that $(f \circ \gamma)^\prime(0) = f^\prime(\mVec{x})\mVec{h} = 0$. Then as $\mVec{h}$ can be a scalar\\ multiple of any vector, we must have that $f^\prime(\mVec{x}) = \mMat{0}$.\retTwo
\end{myIndent}}

\mySepTwo
\hOne
\newpage

Let $X$ be a metric space with the metric $d$, and consider some $\varphi: X \longrightarrow X$.\\ We call $\varphi$ a \udefine{contraction} if there exists $c < 1$ such that for all $x, y \in X$:

{\centering $d(\varphi(x), \varphi(y)) \leq c\cdot d(x, y)$.\retTwo\par}


{\begin{myIndent}\hTwo
   \uuline{Theorem 9.23 (Banach Fixed Point Theorem)}:\\ [1pt] Let $X$ be a complete metric space with metric $d$, and let $\varphi: X \longrightarrow X$ be a\\ contraction. Then there exists a unique $x \in X$ such that $\varphi(x) = x$\\ [-6pt]

   {\begin{myIndent}\hThree
      Proof:\\
      We can establish that $x$ is unique if it exists very easily.
      {\begin{myIndent}\hFour
         Assume $x, y \in X$ such that $\varphi(x) = x$ and $\varphi(y) = y$. Then:\\ $d(x, y) = d(\varphi(x), \varphi(y)) \leq c\cdot d(x, y)$. Since $c < 1$, the only\\ way that this is possible is if $d(x , y) = 0$. So, $x = y$.\retTwo
      \end{myIndent}}

      Therefore, we now will focus on showing that $x$ exists.\retTwo

      Pick any $x_0 \in X$ and then recursively define the sequence $(x_n)$ by\\ setting $x_{n+1} = \varphi(x_n)$ for all $n$. Then because $f$ is a contraction, there\\ exists $c < 1$ such that $d(\varphi(a), \varphi(b)) \leq c\cdot d(a, b)$ for all $a, b \in X$.\\ Hence, for all $n \geq 1$, we have that:
      
      {\centering $d(x_{n+1}, x_n) = d(\varphi(x_n), \varphi(x_{n-1})) \leq c \cdot d(x_n, x_{n-1})$. \retTwo\par}

      By induction, we thus get that $d(x_{n+1}, x_n) \leq c^nd(x_1, x_0)$ for all $n$\\ (including $0$).\retTwo

      Now consider that for any $m, n \in \mathbb{Z}_+$ such that $m > n \geq 1$, we\\ have that:
      
      \begin{center}
         \begin{tabular}{l}
            $d(x_m, x_n) \leq \sum\limits_{i=n}^{m-1}d(x_{i+1}, x_{i}) \leq d(x_1, x_0)(c^n + \ldots + c^{m-1})$\\

            $\phantom{d(x_m, x_n) \leq \sum\limits_{i=n}^{m-1}d(x_{i+1}, x_{i})} \leq d(x_1, x_0)c^n{\displaystyle\sum\limits_{i=1}^{m-n-1}}c^i \leq \frac{c^n}{1-c}d(x_1, x_0)$\\
         \end{tabular}\retTwo
      \end{center}

      This shows that $(x_n)$ is a Cauchy sequence. After all, for any $\varepsilon > 0$, we\\ can set $N$ such that $\frac{c^N}{1 - c}d(x_1, x_0) < \varepsilon$. That way, for all $m > n > N$\\ we have that $d(x_m, x_n) < \varepsilon$.\retTwo
      
      So because $X$ is complete, we know that $(x_n)$ converges. Therefore let\\ $x$ equal the limit of $(x_n)$ and consider any $\varepsilon > 0$.\retTwo
      
      Since $x_n \rightarrow x$, we know that there exists $N$ such that $\forall m > N$, we have\\ that $d(x, x_m) < \sfrac{\varepsilon}{2}$. And since $m$ is a contraction, we also know that\\ $d(\varphi(x), \varphi(x_m)) \leq c\frac{\varepsilon}{2} < \sfrac{\varepsilon}{2}$ for all $m > N$. Hence letting $m > N$:
      
      {\center $d(\varphi(x), x) \leq d(\varphi(x), \varphi(x_{m})) + d(x_{m+1}, x) < \sfrac{\varepsilon}{2} + \sfrac{\varepsilon}{2} = \varepsilon$ \retTwo\par}

      And as $\varepsilon$ is arbitrary, we thus know that $d(\varphi(x), x) = 0$. So, $\varphi(x) = x$.
   \end{myIndent}}
\end{myIndent}}

\newpage

\markLecture{4/18/2024}

{\begin{myIndent}\hTwo
   \uuline{The Inverse Function Theorem}:\\ [2pt]
   Suppose that $f \in \mathscr{C}^1(E, \mathbb{R}^n)$ where $E \subseteq \mathbb{R}^n$ is open, that $f^\prime(\mVec{a})$ is invertible\\ for some $\mVec{a} \in E$, and that $\mVec{b} = f(\mVec{a})$. Then:
   \begin{itemize}
      \item[(A)] There exist open subsets $U$ and $V$ of $\mathbb{R}^n$ such that $\mVec{a} \in U$,\hspace{0.1em} $\mVec{b} \in V$,\\ $f(U) = V$, and $f$ is one-to-one on $U$.
      \item[(B)] If $g$ is defined such that $(g \circ f)(\mVec{x}) = \mVec{x}$ for all $x \in U$, then\\ $g \in \mathscr{C}^{1}(V, \mathbb{R}^n)$ with $g^\prime(\mVec{y}) = f^\prime(g(\mVec{y}))$ for all $\mVec{y} \in V$.\retTwo
   \end{itemize}

   {\begin{myIndent}\hThree
      \ul{Proof of part A}:\retTwo
      
      Set $\mMat{A} = f^\prime(\mVec{a})$ and set $\lambda = \frac{1}{2\|\mMat{A}\|^{-1}}$. Then since $f \in \mathscr{C}^1(E, \mathbb{R}^n)$, we know \\ that there exists $\delta > 0$ such that $\mVec{x} \in B_\delta(\mVec{a}) \Longrightarrow \|f^\prime(\mVec{x}) - \mMat{A}\| < \lambda$.\retTwo
      
      So, for any $r$ satisfying that $0 < r < \delta$ and $B_r \subseteq E$, set $U = B_r(\mVec{a})$.\\ Then let $\mVec{y} \in \mathbb{R}^n$ and consider the function:
      
      {\centering $ \varphi(\mVec{x}) = \mVec{x} + \mMat{A}^{-1}(\mVec{y} - f(\mVec{x}))\quad$ (defined for all $\mVec{x} \in E$).\retTwo\par}

      Now firstly, observe that $f(\mVec{x}) = \mVec{y}$ if and only if $\mVec{x}$ is a fixed point of $\varphi$.
      {\begin{myIndent}\hFour
         Proof:\\ [-2pt]
         ($\Longrightarrow$) If $f(\mVec{x}) = \mVec{y}$, then $\varphi(\mVec{x}) = \mVec{x} + \mMat{A}^{-1}(\mVec{0}) = \mVec{x}$.\retTwo
         ($\Longleftarrow$) If $\varphi(\mVec{x}) = \mVec{x}$, then $\mVec{0} = \mMat{A}^{-1}(\mVec{y} - f(\mVec{x}))$. As $\nullity{\mMat{A}^{-1}} = 0$,\\ we therefore must have that $\mVec{y} - f(\mVec{x}) = \mVec{0}$. So $f(\mVec{x}) = \mVec{y}$.
         \retTwo
      \end{myIndent}}

      Secondly, note that $\varphi^\prime(\mVec{x}) = \mMat{I} - \mMat{A}^{-1}f^\prime(\mVec{x}) = \mMat{A}^{-1}\left(\mMat{A} - f^\prime(\mVec{x})\right)$.\\ Therefore, assuming $\mVec{x} \in U$, we know that:

      {\centering $\|\varphi^\prime(\mVec{x})\| \leq \|\mMat{A}^{-1}\|\|\mMat{A} - f^\prime(\mVec{x})\| < \|\mMat{A}^{-1}\|\lambda = \frac{1}{2}$ \retTwo\par}

      In turn, since $U = B_r(\mVec{a})$ is convex, we can apply theorem 9.19 to say\\ that $\|\varphi(\mVecAst{x}_2) - \varphi(\mVecAst{x}_1)\| \leq \frac{1}{2}\|\mVecAst{x}_2 - \mVecAst{x}_1\|$. Now, be aware that this does\\ not imply that $\varphi$ is a contraction because we still don't know where $\varphi$\\ maps $U$ to. However, this is enough information to do the proof on the\\ previous page showing that $\varphi$ has at most one fixed point on $U$.\retTwo
      
      Hence, $f$ is one-to-one on $U$ since for any $\mVec{y} \in \mathbb{R}^n$, there can be at most\\ one $\mVec{x} \in U$ such that $\mVec{y} = f(\mVec{x})$\retTwo\retTwo

      Now, we move on to showing that $V = f(U)$ is open.\retTwo
      
      Let $\mVecAst{y}_0 \in V$ and note that there exists a unique $\mVecAst{x}_0 \in U$ such that\\ [2pt] $f(\mVecAst{x}_0) = \mVecAst{y}_0$. Also, because $U$ is open, we know there exists $R > 0$\\ [2pt] such that $\overline{B_R(\mVecAst{x}_0)} \subset U$. So let $B$ equal that closed ball.

      \newpage

      Next, fix $\mVec{y}$ such that $\|\mVec{y} - \mVecAst{y}_0\| < \lambda R$ and define $\varphi$ using $\mVec{y}$ as was\\ described on the last page. Then note that:
      
      {\center $\|\varphi(\mVecAst{x}_0) - \mVecAst{x}_0\| = \|\mMat{A}^{-1}(\mVec{y} - \mVecAst{y}_0)\| < \|\mMat{A}^{-1}\|\lambda R = \frac{R}{2}$ \retTwo\par}

      Therefore, we have that for all $\mVec{x} \in B$:
      
      {\center
      \begin{tabular}{l}
         $\|\varphi(\mVec{x}) - \mVecAst{x}_0\| \leq \|\varphi(\mVec{x}) - \varphi(\mVecAst{x_0})\| + \|\varphi(\mVecAst{x}_0) - \mVecAst{x}_0\|$\\ [6pt]
         $\phantom{\|\varphi(\mVec{x}) - \mVecAst{x}_0\|} < \frac{1}{2}\|\mVec{x} - \mVecAst{x}_0\| + \frac{R}{2} < \frac{R}{2} + \frac{R}{2} = R$
      \end{tabular}\retTwo\par}

      Or in other words, $\mVec{x} \in B \Longrightarrow \phi(\mVec{x}) \in B$.\retTwo
      
      Hence, $\varphi$ is a contraction over $B$ (for real this time). Also, because $B$ is\\ a closed and bounded subset of $\mathbb{R}^n$, we know $B$ is compact and thus\\ complete. So by theorem 9.23, $\varphi$ has a fixed point $\mVec{x} \in B$. It follows\\ then that $\mVec{y} \in V$.\retTwo

      Since $\mVec{y}$ was arbitary, we thus know that $B_{\lambda R}(\mVecAst{y}_0) \subseteq V$. So, $V$ is open.\retTwo\retTwo

      \ul{Proof of part B}:\retTwo

      Based on the previous part, we know that $f$ is a bijective map from $U$\\ to $V$. So, there does exist $g$ such that $(g \circ f)(\mVec{x}) = \mVec{x}$ for all $\mVec{x} \in U$.\retTwo

      Now pick $\mVec{y} \in V$ and $\mVec{y} + \mVec{k} \in V$. Then there exists $\mVec{x} \in U$ and\\ $\mVec{x} + \mVec{h} \in U$ such that $f(\mVec{x}) = \mVec{y}$ and $f(\mVec{x} + \mVec{h}) = \mVec{y} + \mVec{k}$.\retTwo

      Next, having defined $\varphi$ one more time using any $\mVecAst{y}_0 \in \mathbb{R}^n$ we have that:

      {\centering 
      {\fontsize{11.6}{13.6}\selectfont \begin{tabular}{l}
         $\varphi(\mVec{x} + \mVec{h}) - \varphi(\mVec{x}) = \mVec{x} + \mVec{h} + \mMat{A}^{-1}(\mVecAst{y}_0 - f(\mVec{x} + \mVec{h})) - \mVec{x} - \mMat{A}^{-1}(\mVecAst{y}_0 - f(\mVec{x}))$\\ [3pt]

         $\phantom{\varphi(\mVec{x} + \mVec{h}) - \varphi(\mVec{x})} = \mVec{h} + \mMat{A}^{-1}(f(\mVec{x}) - f(\mVec{x} + \mVec{h}))$\\ [3pt]

         $\phantom{\varphi(\mVec{x} + \mVec{h}) - \varphi(\mVec{x})} = \mVec{h} - \mMat{A}^{-1}\mVec{k}$
      \end{tabular}}\retTwo\par}

      At the same time, we know from part A that {\fontsize{12.25}{14.25}\selectfont$\|\varphi(\mVec{x} + \mVec{h}) - \varphi(\mVec{x})\| < \frac{1}{2}\|\mVec{h}\|$.}\\ [0pt] Therefore, we have that $\|\mVec{h} - \mMat{A}^{-1}\mVec{k}\| \leq \frac{1}{2}\|\mVec{h}\|$, and in turn this means\\ that $\|\mMat{A}^{-1}\mVec{k}\| \geq \frac{1}{2}\|\mVec{h}\|$.

      {\begin{myIndent}\hFour
         This is because $\|\mVec{h}\| \leq \|\mVec{h} - \mMat{A}^{-1}\mVec{k}\| + \|\mMat{A}^{-1}\mVec{k}\| \leq \frac{1}{2}\|\mVec{h}\| + \|\mMat{A}^{-1}\mVec{k}\|$.\retTwo
      \end{myIndent}}

      So $\|\mVec{h}\| \leq 2\|\mMat{A}^{-1}\mVec{k}\| \leq 2\|\mMat{A}^{-1}\|\|\mVec{k}\| = \frac{1}{\lambda}\|\mVec{k}\|$.\\ [2pt]

      Then, because $\mVec{x} \in U$, we know that $\|f^\prime(\mVec{x}) - \mMat{A}\| \leq \lambda < \frac{1}{\|\mMat{A}^{-1}\|}$. Thus,\\ $f^\prime(\mVec{x})$ has an inverse which we shall call $\mMat{T}$. Now note that:

      {\center 
      \begin{tabular}{l}
         $g(\mVec{y} + \mVec{k}) - g(\mVec{y}) - \mMat{T}\mVec{k} = \mVec{h} - \mMat{T}\mVec{k}$\\

         $\hphantom{g(\mVec{y} + \mVec{k}) - g(\mVec{y}) - \mMat{T}\mVec{k}} = \mMat{T}f^\prime(\mVec{x})\mVec{h} - \mMat{T}(f(\mVec{x} + \mVec{h}) - f(\mVec{x}))$\\

         $\hphantom{g(\mVec{y} + \mVec{k}) - g(\mVec{y}) - \mMat{T}\mVec{k}} = -\mMat{T}(f(\mVec{x} + \mVec{h}) - f(\mVec{x}) - f^\prime(\mVec{x})\mVec{h})$
      \end{tabular} \par\retTwo}

      \newpage

      Combining that with the fact that $\|\mVec{h}\|\lambda \leq \|\mVec{k}\|$, we know that:

      {\center{$\frac{\|g(\mVec{y} + \mVec{k}) - g(\mVec{y}) - \mMat{T}\mVec{k}\|}{\|\mVec{k}\|} \leq \frac{\|\mMat{T}\|}{\lambda}\cdot \frac{\|f(\mVec{x} + \mVec{h}) - f(\mVec{x}) - f^\prime(\mVec{x})\mVec{h}\|}{\|\mVec{h}\|}$ }\retTwo\par}

      So because $\mVec{h} \rightarrow \mVec{0}$ as $\mVec{k} \rightarrow \mVec{0}$, we know that {\fontsize{12}{14}\selectfont $\hspace{-0.4em}\lim\limits_{\mVec{k} \rightarrow \mVec{0}}\hspace{-0.4em}\frac{\|g(\mVec{y} + \mVec{k}) - g(\mVec{y}) - \mMat{T}\mVec{k}\|}{\|\mVec{k}\|} = 0$}.\\ [-8pt]
      And thus $g^\prime(\mVec{y}) = \mMat{T} = f^\prime(g(\mVec{y}))^{-1}$.\retTwo

      Importantly, $\mVec{y}$ was arbitrary. So we generally have that $g: V \longrightarrow \mathbb{R}^n$\\ is differentiable with $g^\prime(\mVec{y}) = f^\prime(g(\mVec{y}))^{-1}$. Additionally, note that:
      \begin{itemize}
         \item $g$ being differentiable implies that $g$ is continuous.
         \item $f^\prime$ is continuous by the assumption of this theorem.
         \item The map $\mMat{A} \mapsto \mMat{A}^{-1}$ is continuous by theorem 9.8.\retTwo
      \end{itemize}

      Therefore, $g^\prime$ is the composition of three continuous functions, which\\ means that $g^\prime$ is continuous. So $g \in \mathscr{C}^1(V, \mathbb{R}^n)$. $\blacksquare$\retTwo
   \end{myIndent}}

   \uuline{Corollary}: If $E \subseteq \mathbb{R}^n$ is open and $f \in \mathscr{C}^1(E, \mathbb{R}^n)$ satisfies that $f^\prime(\mVec{x})$ is\\ invertible for every $\mVec{x} \in E$, then for every open $W \subseteq E$, $f(W)$ is open.\retTwo

   \begin{myIndent}\hThree
      This is a direct result of part A of the previous theorem.\retTwo
   \end{myIndent}
\end{myIndent}}

\mySepTwo

\markLecture{4/25/2024}

For the next theorem we need to go over some notation.\retTwo

Given any $\mVec{x} = (x_1, \ldots, x_n) \in \mathbb{R}^n$ and $\mVec{y} = (y_1, \ldots, y_m) \in \mathbb{R}^m$, we shall write\\ $(\mVec{x}, \mVec{y})$ for the vector $(x_1, \ldots, x_n, y_1, \ldots, y_m) \in \mathbb{R}^{n + m}$.\retTwo

Also, given any $\mMat{A} \in L(\mathbb{R}^{n + m}, \mathbb{R}^n)$, we shall define $\mMat{A}_x(\mVec{h}) = \mMat{A}(\mVec{h}, \mVec{0})$ and\\ $\mMat{A}_y(\mVec{k}) = \mMat{A}(\mVec{0}, \mVec{k})$ for all $\mVec{h} \in \mathbb{R}^n$ and $\mVec{k} \in \mathbb{R}^m$. That way, $\mMat{A}_x \in L(\mathbb{R}^n)$,\\ $\mMat{A}_y \in L(\mathbb{R}^m, \mathbb{R}^n)$, and $\mMat{A}(\mVec{h}, \mVec{k}) = \mMat{A}_x\mVec{h} + \mMat{A}_y\mVec{k}$.\retTwo

{\begin{myIndent}\hTwo
   \uuline{Proposition}: If $\mMat{A} \in L(\mathbb{R}^{n + m}, \mathbb{R}^n)$ and if $\mMat{A}_x$ is invertible, then there corresponds\\ to every $\mVec{k} \in \mathbb{R}^m$ a unique $\mVec{h} \in \mathbb{R}^n$ such that $\mMat{A}(\mVec{h}, \mVec{k}) = \mVec{0}$. Specifically, this $\mVec{h}$\\ is equal to $-\mMat{A}_x^{-1}\mMat{A}_y\mVec{k}$.\\ [-12pt]
   
   \begin{myIndent}\hThree
      Proof:\\
      $\mMat{A}(\mVec{h}, \mVec{k}) = \mVec{0} \Longrightarrow \mMat{A}_x\mVec{h} + \mMat{A}_y\mVec{k} = \mVec{0} \Longrightarrow \mVec{h} = -\mMat{A}_x^{-1}\mMat{A}_y\mVec{k}$\retTwo
   \end{myIndent}
\end{myIndent}}

Now what we want to do on the next page is extend this theorem somehow to\\ a more general function $f: E \longrightarrow \mathbb{R}^{n}$ where $E \subseteq \mathbb{R}^{n+m}$ is open.

\newpage

{\begin{myIndent}\hTwo
   \uuline{Implicit Function Theorem}:\\ [2pt]
   Suppose that $E \subseteq \mathbb{R}^{n + m}$ is open and that $f \in \mathscr{C}^1(E, \mathbb{R}^n)$ satisfies that\\ [-1pt] $f(\mVec{a}, \mVec{b}) = \mVec{0}$ for some $(\mVec{a}, \mVec{b}) \in E$. Also, set $\mMat{A} = f^\prime(\mVec{a}, \mVec{b})$ and suppose\\ that $\mMat{A}_x$ is invertible. Then:\\ [-16pt]
   \begin{enumerate}
      \item[(A)] There exists open sets $U \subseteq \mathbb{R}^{n + m}$ and $W \subseteq \mathbb{R}^m$ such that $(\mVec{a}, \mVec{b}) \in U$,\\$\mVec{b} \in W$, and $\forall \mVec{y} \in W$ there exists a unique $\mVec{x}$ such that $(\mVec{x}, \mVec{y}) \in U$ and\\ $f(\mVec{x}, \mVec{y}) = \mVec{0}$.
      \item[(B)] If we define $g: W \longrightarrow \mathbb{R}^n$ such that each $\mVec{y} \in W$ is mapped to the unique\\ $\mVec{x}$ described above, then $f(g(\mVec{y}), \mVec{y}) = \mVec{0}$ for all $\mVec{y} \in W$,\hspace{0.3em} $g(\mVec{b}) = \mVec{a}$, and\\ $g \in \mathscr{C}^1(W, \mathbb{R}^m)$ with $g^\prime(\mVec{b}) = -\mMat{A}_x^{-1}\mMat{A}_y$.\retTwo
   \end{enumerate}
   
   {\begin{myIndent}\hThree
      \ul{Proof of part A}:\retTwo

      Define $F: \mathbb{R}^{n + k} \longrightarrow \mathbb{R}^{n + k}$ by the rule $F(\mVec{x}, \mVec{y}) = (f(\mVec{x}, \mVec{y}), \mVec{y})$.\\ Then, $F \in \mathscr{C}^1(E, \mathbb{R}^{n + m})$.
      {\begin{myIndent}\hFour
         (You can check that all the partial derivatives of $F$ are continuous)\retTwo
      \end{myIndent}}

      So, let us now consider $F^\prime(\mVec{a}, \mVec{b})$. Importantly, you can check that\\ $F^\prime(\mVec{a}, \mVec{b})(\mVec{h}, \mVec{k}) = (f^\prime(\mVec{a}, \mVec{b})(\mVec{h}, \mVec{k}), \hspace{0.4em} \mVec{k})$. Immediately, this means\\ if $F^\prime(\mVec{a}, \mVec{b})(\mVec{h}, \mVec{k}) = \mVec{0}$, we must have that $\mVec{k} = \mVec{0}$. Also note that\\ $f^\prime(\mVec{a}, \mVec{b})(\mVec{h}, \mVec{k}) = \mMat{A}(\mVec{h}, \mVec{k}) = \mMat{A}_x\mVec{h} + \mMat{A}_y\mVec{k}$. Since $\mMat{A}_x$ is invertible\\ and $\mVec{k} = 0$, we know that $\mMat{A}_x\mVec{h} + \mMat{A}_y\mVec{k} = \mVec{0} \Longrightarrow \mVec{h} = \mVec{0}$.\retTwo

      Therefore, we have shown that:
      
      {\centering $F^\prime(\mVec{a}, \mVec{b})(\mVec{h}, \mVec{k}) = \mVec{0} \Longrightarrow (\mVec{h}, \mVec{k}) = (\mVec{0}, \mVec{0})$.\retTwo\par}

      Or in other words, $F^\prime(\mVec{a}, \mVec{b})$ is invertible.\retTwo

      Now by the inverse function theorem let $U$ and $V$ be open subsets of\\ $\mathbb{R}^{n + m}$ such that $(\mVec{a}, \mVec{b}) \in U$,\hspace{0.4em} $F(\mVec{a}, \mVec{b}) = (\mVec{0}, \mVec{b}) \in V$, and $F$ is\\ invertible going from $U$ to $V$. Then set $W = \{\mVec{y} \in \mathbb{R}^m \mid (\mVec{0}, \mVec{y}) \in V\}$.\retTwo

      Clearly, $W$ is open since $V$ is open. Also, $\mVec{b} \in W$. Finally, let $\mVec{y} \in W$\\ and consider that $(\mVec{0}, \mVec{y}) \in V$. Therefore there is a unique $(\mVec{x}, \mVec{y}) \in U$\\ such that $F(\mVec{x}, \mVec{y}) = f((\mVec{x}, \mVec{y}), \mVec{y}) = (\mVec{0}, \mVec{y})$. Obviously, this $\mVec{x}$ is also\\ the unique vector satisfying that $(\mVec{x}, \mVec{y}) \in U$ and $f(\mVec{x}, \mVec{y}) = \mVec{0}$.\\ [9pt]

      \ul{Proof of part B}:\retTwo

      Now let $g: W \longrightarrow \mathbb{R}^n$ such that $g(\mVec{y})$ satisfies that $f(g(\mVec{y}), \mVec{y}) = \mVec{0}$\\ and $(g(\mVec{y}), \mVec{y}) \in V$ for all $\mVec{y} \in U$. Also define $G: V \longrightarrow U$ as the\\ function inverting $F$ on $U$.

      \newpage

      By the inverse function theorem, $G \in \mathscr{C}^1(V, \mathbb{R}^{n + m})$. Also, for all $y \in \mVec{W}$\\ we have that $(g(\mVec{y}), \mVec{y}) = G(F(g(\mVec{y}), \mVec{y})) = G(\mVec{0}, \mVec{y})$. Thus, by\\ defining $\Phi: W \longrightarrow V$ such that $\Phi(\mVec{y}) = (g(\mVec{y}), \mVec{y})$, we know that\\ $\Phi \in \mathscr{C}^1(W, \mathbb{R}^{n + m})$. And in turn, this means that $g \in \mathscr{C}^1(W, \mathbb{R}^n)$.\retTwo

      Finally, consider that $g(\mVec{b}) = \mVec{a}$ trivially. At the same time, note that\\ $\Phi^\prime(\mVec{y})\mVec{k} = (g^\prime(\mVec{y})\mVec{k}, \mVec{k})$ for all $\mVec{y} \in W$ and $\mVec{k} \in \mathbb{R}^m$. Thus, because\\ $f(\Phi(\mVec{y})) = \mVec{0}$ for all $\mVec{y} \in W$, we know that $f^\prime(\Phi(\mVec{y}))\Phi^\prime(\mVec{y}) = \mMat{0}$ by\\ chain rule. And when $\mVec{y} = \mVec{b}$, this becomes $\mMat{A}\Phi^\prime(\mVec{b}) = \mMat{0}$. So for all\\ $\mVec{k} \in \mathbb{R}^m$, we have that:
      
      {\centering $\mMat{A}_xg^\prime(\mVec{b})\mVec{k} + \mMat{A}_y\mVec{k} = \mMat{A}(g^\prime(\mVec{b})\mVec{k}, \mVec{k}) = \mMat{A}\Phi^\prime(\mVec{b})\mVec{k} = \mVec{0}$.\retTwo\par}

      It follows that $\mMat{A}_xg^\prime(\mVec{b}) + \mMat{A}_y = \mMat{0}$. Or in other words, $g^\prime(\mVec{b}) = -\mMat{A}_x^{-1}\mMat{A}_y$. $\blacksquare$\retTwo\retTwo
   \end{myIndent}}

   {\begin{myIndent}\teachComment
      \begin{myClosureOne}{4.9}
         Note that it is really easy to check if $\mMat{A}_x$ is invertible.\retTwo First, let $e_1, \ldots, e_n$ and $u_1, \ldots, u_m$ be the standard bases of $\mathbb{R}^n$ and $\mathbb{R}^m$\\ respectively. Next, fix $\{(e_1, \mVec{0}), \ldots, (e_n, \mVec{0}), (\mVec{0}, u_1), \ldots, (\mVec{0}, u_m)\}$ as\\ our basis for $\mathbb{R}^{n + m}$ and $\{e_1, \ldots, e_n\}$ as our basis for $\mathbb{R}^n$. Then we have\\ that:
   
         {\center $ [\mMat{A}] = 
         \left[\begin{matrix}
            & & & & \\ & & [\mMat{A}_x] & & \\ & & & &
         \end{matrix}\middle|\begin{matrix}
            & & \\ &[\mMat{A}_y]& \\ & &
         \end{matrix}\right]$ \retTwo\par}
   
         So finally, check if $[\mMat{A}_x]$ is invertible or not.\\ [-4pt]
      \end{myClosureOne}\retTwo\retTwo
   \end{myIndent}}
\end{myIndent}}

\mySepTwo

Let $E \subseteq \mathbb{R}^n$ be open and suppose that $f: E \longrightarrow \mathbb{R}$ has the partial derivatives\\ $D_1f, \ldots, D_nf$. Then note that each partial derivative is itself a function from\\ $\mathbb{R}^n$ to $\mathbb{R}$. So, if each $D_jf$ is itself differentiable, then we define the \udefine{second-\\order partial derivatives} of $f$ as $D_{i,j} = D_i(D_jf)$.\retTwo

Additionally, if $D_{i,j}f$ is continuous in $E$ for all $1 \leq i,j \leq n$, then we say that\\ $f \in \mathscr{C}^2(E)$ {\myComment(which is an abbreviation of the notation: $f \in \mathscr{C}^2(E, \mathbb{R})$)}.\retTwo

{\begin{myTindent}\begin{myIndent}\teachComment
   Note that we're only limiting ourselves to non-vector-valued\\ functions here for the sake of simplicity. Also, hopefully you can\\ guess from this definition how an $n$th-order partial derivative is\\ defined, and what it means for a function to be a $\mathscr{C}^n$-mapping.
\end{myIndent}\end{myTindent}}

\newpage

{\begin{myIndent}\hTwo
   \uuline{Theorem}: Let $f: E \longrightarrow \mathbb{R}$ where $E \subseteq \mathbb{R}^n$ is open. If $f \in \mathscr{C}^2(E)$, then\\ $(D_{i,j}f)(\mVec{x}) = (D_{j,i}f)(\mVec{x})$ for all $1 \leq i,j \leq n$ and $\mVec{x} \in E$.\\ [-8pt]

   \begin{myIndent}\hThree
      Proof:\\
      Let $e_1, \ldots, e_n$ be the standard basis of $\mathbb{R}^n$ and let $Q \subset E$ be a closed\\ rectangle with opposite vertices at $\mVec{x}$ and $\mVec{x} + he_i + ke_j$ where\\ $h > 0$ and $k > 0$. Then define:
      
      {\center $\Delta(f, Q) = f(\mVec{x} + he_i + ke_j) - f(\mVec{x} + he_j) - f(\mVec{x} + ke_j) + f(\mVec{x})$ \retTwo\par} 

      Next, let $u(s) = f(\mVec{x} + se_i + ke_j) - f(\mVec{x} + se_i)$. Importantly, $u$ is real\\ function that is differentiable on $[0, h]$. So:

      \begin{center}
         \begin{tabular}{l}
            $\Delta(f, Q) = u(h) - u(0)$ \\ [3pt]
            $\phantom{\Delta(f, Q)} = hu^\prime(s_1)$ \hspace{0.3em} for some $s_1 \in (0, h)$ \hspace{0.5em} {\hFour (by the mean value theorem)} \\ [3pt]
            $\phantom{\Delta(f, Q)} = h((D_if)(\mVec{x} + s_1e_i + ke_j) - (D_if)(\mVec{x} + s_1e_i))$
         \end{tabular}\retTwo
      \end{center}

      Similarly, define $v(t) = (D_if)(\mVec{x} + s_1e_i + te_j) - (D_if)(\mVec{x} + s_1e_i)$. Then\\ $(D_if)(\mVec{x} + s_1e_i + ke_j) - (D_if)(\mVec{x} + s_1e_i) = v(k) - v(0)$ and $v$ is a real\\ function that is differentiable on $[0, k]$. So:

      \begin{center}
         \begin{tabular}{l}
            $\Delta(f, Q) = h(v(k) - v(0))$ \\ [3pt]
            $\phantom{\Delta(f, Q)} = h(kv^\prime(t_1))$  \hspace{0.3em} for some $t_1 \in (0, k)$ \hspace{0.5em} {\hFour (by the mean value theorem)}\\ [3pt]
            $\phantom{\Delta(f, Q)} = hk((D_{j,i}f)(\mVec{x} + s_1e_i + t_1e_j))$.
         \end{tabular}\retTwo
      \end{center}

      Now importantly,  we can use the exact same reasoning to say that\\ $\Delta(f, Q) = hk((D_{i,j}f)(\mVec{x} + s_2e_i + t_2e_j))$ for some $s_2 \in (0, h)$\\ and $t_2 \in (0, k)$.\retTwo

      So finally, consider that because $D_{j,i}f$ and $D_{i,j}f$ are continuous, we\\ know that for all $\varepsilon > 0$, there exists $\delta_1, \delta_2 > 0$ such that:
      \begin{itemize}
         \item {\fontsize{12}{14} $\sqrt{s_1^2 + t_1^2} < \delta_1 \Longrightarrow \left|(D_{j,i}f)(\mVec{x} + s_1e_i + t_1e_j) - (D_{j,i}f)(\mVec{x})\right| < \sfrac{\varepsilon}{2}$}
         \item {\fontsize{12}{14} $\sqrt{s_2^2 + t_2^2} < \delta_2 \Longrightarrow \left|(D_{i,j}f)(\mVec{x} + s_2e_i + t_2e_j) - (D_{i,j}f)(\mVec{x})\right| < \sfrac{\varepsilon}{2}$}
      \end{itemize}\retTwo

      Thus, set $\delta = \min(\delta_1, \delta_2)$ and pick any $h, k > 0$ such that $h^2 < \frac{\delta^2}{2}$ and\\ $k^2 < \frac{\delta^2}{2}$. Then as $0 < s_1, s_2 < h$ and $0 < t_1, t_2 < k$, we know that\\ both $\sqrt{s_1^2 + t_1^2} < \delta_1$ and $\sqrt{s_2^2 + t_2^2} < \delta_2$. So:\\ 

      \begin{center}{\fontsize{11}{13}\selectfont}
         \begin{tabular}{l}
            $|(D_{j,i}f)(\mVec{x}) - (D_{i,j}f)(\mVec{x})|$\\ [6pt]
            $\phantom{aaa} \leq \left|\frac{\Delta(f, Q)}{hk} - (D_{j,i}f)(\mVec{x})\right| + \left|\frac{\Delta(f, Q)}{hk} - (D_{i,j}f)(\mVec{x})\right|$\\ [6pt]
            $\phantom{aaa} = \left|(D_{j,i}f)(\mVec{x} + s_1e_i + t_1e_j) - (D_{j,i}f)(\mVec{x})\right|$\\ [2pt]
            $\phantom{aaaaaaaaaaaaaaaaa} + \left|(D_{i,j}f)(\mVec{x} + s_2e_i + t_2e_j) - (D_{i,j}f)(\mVec{x})\right|$\\
            $\phantom{aaa} < \sfrac{\varepsilon}{2} + \sfrac{\varepsilon}{2} = \varepsilon$
         \end{tabular}\retTwo
      \end{center}

      And since $\varepsilon$ was arbitary, we conclude that $(D_{j,i}f)(\mVec{x}) = (D_{i,j}f)(\mVec{x})$.

   \end{myIndent}
\end{myIndent}}




\end{document}

